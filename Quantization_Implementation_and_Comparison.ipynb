{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fD24jJxq7t3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f9d434b-0d77-49a0-8203-cad54f4df0b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n",
            "Cloning into 'Daredevil-8B'...\n",
            "remote: Enumerating objects: 92, done.\u001b[K\n",
            "remote: Counting objects: 100% (89/89), done.\u001b[K\n",
            "remote: Compressing objects: 100% (89/89), done.\u001b[K\n",
            "remote: Total 92 (delta 41), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (92/92), 2.26 MiB | 1.42 MiB/s, done.\n",
            "Filtering content: 100% (4/4), 2.95 GiB | 27.53 MiB/s, done.\n",
            "Encountered 3 file(s) that may not have been copied correctly on Windows:\n",
            "\tmodel-00003-of-00004.safetensors\n",
            "\tmodel-00001-of-00004.safetensors\n",
            "\tmodel-00002-of-00004.safetensors\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n"
          ]
        }
      ],
      "source": [
        "# @title # ‚ö° AutoQuant\n",
        "\n",
        "# @markdown > üó£Ô∏è [Large Language Model Course](https://github.com/mlabonne/llm-course)\n",
        "\n",
        "# @markdown ‚ù§Ô∏è Created by [@maximelabonne](https://twitter.com/maximelabonne).\n",
        "\n",
        "# @markdown **Usage:** Download the model by **running this cell** and then run the cells corresponding to your quantization methods of interest.\n",
        "\n",
        "# @markdown To quantize a 7B or 8B model, GGUF only needs a T4 GPU, while the other methods require an L4 or A100 GPU.\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ## ü§ó Download model (required)\n",
        "# @markdown `HF_TOKEN` corresponds to the name of the secret that stores your [Hugging Face access token](https://huggingface.co/settings/tokens) in Colab.\n",
        "\n",
        "MODEL_ID = \"mlabonne/Daredevil-8B\" # @param {type:\"string\"}\n",
        "USERNAME = \"jcorenday\" # @param {type:\"string\"}\n",
        "HF_TOKEN = \"HF_TOKEN\" # @param {type:\"string\"}\n",
        "\n",
        "MODEL_NAME = MODEL_ID.split('/')[-1]\n",
        "\n",
        "# Download model\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/{MODEL_ID}\n",
        "!pip install -q huggingface_hub\n",
        "\n",
        "from huggingface_hub import create_repo, HfApi, ModelCard\n",
        "from google.colab import userdata, runtime\n",
        "\n",
        "# Defined in the secrets tab in Google Colab\n",
        "hf_token = userdata.get(HF_TOKEN)\n",
        "api = HfApi()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"mlabonne/Daredevil-8B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475,
          "referenced_widgets": [
            "f66187b44ddb42a5a1fad3146c3f6255",
            "d75ac4d7862a408b8f2980cce8e47436",
            "ff5fe59fe9b64175907204407891580f",
            "54ca6f77540d435cb3e7f118c5b185e9",
            "5cced1e3e2804a61b041441bfa7e4547",
            "892492a70bbf482c910bd3dddf1be4c9",
            "94d0820a35e242738f47defdf739d07f",
            "3e18818553e74f83b8b1ac9e9e9565c6",
            "ae0c76b28463407b8e707318a8d825e8",
            "b18f8160818640a8b7d360270eaaaa87",
            "33f5fcf78a014d2290e89cd07cebe113",
            "28efaf5a401c4606b30c437456d5f019",
            "6894289abfb5403fb0ac83612b2ad499",
            "c831dac9f0a9472584ee379265ddb69d",
            "051ff396d2304091b91627e158c30ca5",
            "44567038202a4f8098231b6377ff0f44",
            "065a684f0456409195c6acde7bde4363",
            "ad2cc9af12d84d03b7d82607d8e4bbca",
            "6c836fd2ac4440d9adbc5f1e4ae5c136",
            "fd7dabb983aa47cf92d7df00741befa1",
            "9c4b9eeb1e4145b3949608ca3b35db5e",
            "6a14d90e4e8048a580ce74f1aff99654",
            "cd1c9720d9d1414586a2e5a038385430",
            "9487b9b30e6746fdaa516f343cda3e95",
            "1d1f3611b7a946788e69b7318f6c29c1",
            "5134c16e4d43433f987a13efe7d4e7b9",
            "0f2aab6c99ca4696a59995f058b8622b",
            "66cddb44f9e9481382734e6cb019fa72",
            "3fca8d896cb5464a8aa96346f290fae3",
            "c9f8941cf88449ab8061dae64143cc20",
            "af7bab9696af41c69a50fe9ea7bcff45",
            "6a42beabe0274544bd580e0ead386c65",
            "2be49d11d3a94a6faa8d8a662e3fdf1d",
            "c2d6c7e9167044bd84aef59d602fe6ac",
            "e86e364e66f24102b18a345f02bb0f2d",
            "30a806bb1129444a8742bd6d69891899",
            "d95ff3f8c428407584188b0c2d63787d",
            "9f534b44470549efa1d5de2bc6dd0989",
            "dd5a2da6c35f4ac4843b03bcae8afedd",
            "38f90f23c3034f5b8595368d3c24359e",
            "0a38db6145784b659df1115dc78417b2",
            "d6760b2b05594cb49ff5e40ac5982434",
            "c1ccdbbf728e4a1aad7e964124041107",
            "fb84fb7dac284521bfb36c401bed74af",
            "52d815c0305f4a108d7013d0d5a3ce9e",
            "1a45ad46f4b54a0cb138e125beac37a2",
            "068cb915f608429396dc4f512422643b",
            "4dc4a42e2fd14d299bcf4f2f9cd33c51",
            "79e35a6316ba4644b9ea6ef0e4aae200",
            "ad628a6218ec4b63bd22c27bd2d682de",
            "f7b1267fea1a4c90a4e16346d7fdeee1",
            "5b433f4bf3144c41b677167b9f81e7c7",
            "5b67f1e7d1c3468682f9160d83865332",
            "de670ef4715b4cbc95fa95c565e1fd22",
            "6df0afaa41ad4786a5fe6ae84e826b1c",
            "281689f67d11411dbe66f3ca1ee9a411",
            "eca6455c587a4d16b453ef9892eed6b9",
            "5562114265314e6f9d2c21aa06f20325",
            "e6a2db0fa47943bdae1b0d7af6e48a5a",
            "9436957b363c4b7692244f428db294d1",
            "8545be8e225b488e9087ba838b054071",
            "5cd3872ebdf3471e8bb73c3156c8ceb0",
            "8411e3d71dc542959ba65299213f0a41",
            "8e922a75880c477a838273ce05fd8edd",
            "4e3b86b91caf4a679a0b13bd884390c0",
            "36fb12c2d1e34fa4893fcdd92c3c9adb",
            "d1471994fea64dda8f4f6475b92bfb2a",
            "9408f71c66f24c7dab48ab9a5054e97f",
            "9280806eddb041d1ab823c1821415d54",
            "36a71ef54de44256bee9e555b19eceb2",
            "ee6049828d07469aa22ebbe10e5dffce",
            "f07365a5c5d247f1966c371c30bf8fcc",
            "eb7b908df66a4196ad551a82aab21768",
            "3208efaf8db94c33ab25780d3fae9561",
            "9e947d99469c4d08907314983b8f0f5d",
            "d6c45755c06d457fb3c661cf7d3560d3",
            "4b0a0aebe7db496d862b60a6cb122811",
            "9ba6c83db9f84c359c83830843802512",
            "5afc52df76d94b3db38c56d63d20edf7",
            "4a566bddd4464b5385963a1c3b41f133",
            "8cee6e7994034fb393eba3ba413b1f2e",
            "1201bdd42e4a4200bf46c7ca7407b1cf",
            "dca0d065f53e41a390897e276c695829",
            "c51c74a39f8a4764b3849afebf8bb37f",
            "82ccc54d925e4ccc9ee464a30fd7dcba",
            "d57fd40796694db6a851bbbed08e12d1",
            "6fde1bc3aca94244babd8e2441b2bc78",
            "c5d0dd9daea947bcbbbf7076ee9ffc75",
            "7ee3b7fbd3824cc3b96f90f0dab927fd",
            "cb6285b059ec41449b2c77a174e8a2a3",
            "7451f1a611764f0f9863cc9bc60d033a",
            "d6e0c415d31545d0be4fb1b37868d088",
            "7f816fd2b5ac4f15b430b025a798fdab",
            "b09905a1f9a94ad3b1f396d31f18c8d4",
            "7cc764f2659741ad931c5e1b3a51f48f",
            "03d559863eaa44bf9af0b251e84202e8",
            "6b9069bf679d492fb7bdbb55e6cc6b72",
            "80df318602ba4dfc92e399f6d4111b36",
            "fbb489a4c599459cb8a9e12cb8e60756",
            "794ab4f8c64d462682d921304c0a2d44",
            "c51d07609c0f45919c67d61ae7470a1d",
            "e2589f1cbefc433695e1360047bd1d81",
            "c033d88262b8419792b28499d9c45900",
            "da718d4018554a739007f82f428b09aa",
            "b1d6deee2c4545f596429f5d8bc8a3e6",
            "686cde5ce9644ef796206e9930a4971b",
            "264cee4589c742c98e00088cf741edc6",
            "76d1f5dee9c24c4d899d299c0ac5d88a",
            "f313e2cd13324234bd9523607250c2b7",
            "2943794bf29e4a1eb269a38e5cabfa76",
            "6833ff2efccf4cfab7f9d88a22ca78c6",
            "59735e02b0be4bc6a3acaabce337e701",
            "a30a9d3cad934e0d9bc4ec5c507a6dd3",
            "e6090dfb51ef4416ac893d1c17c25c87",
            "28686b7f86184038a1fdf7f1b9ea5d8a",
            "d20fd51b42774cd1ad83ac756ff001c5",
            "77f33fcff9934991b5e5a77ecb0b8370",
            "bf1a78a726f34d32ad96520c3ad7e9f8",
            "3602bca21bfb4bdcb06e654bdf6df861",
            "e86dbe0039e94bb88b2f81437084c127",
            "ad187da78eeb437fabb1334a0bdcd7ec",
            "8613e6d4502e48959e0fec7010c30635",
            "f971475c6d12498abf0fe8527bca107c",
            "536851cb1f2145c4b4b0c2d114273bb9",
            "28216418edd6434ba6387163ab7506c9",
            "6279b894605e41358ed0a3d7614ee146",
            "276a3b6078894315861b4a94d64bafb8",
            "2a05ff4f56524b2db810bf0dceda1654",
            "33a89b2f28c442b8963e23724eda9dac",
            "d5c50c90a7294d619cc3fa25e626b079",
            "445a58de87c545309385b8776bb40d36",
            "61f987539055462c933ba21dcc08b677"
          ]
        },
        "id": "lHiyAFkx1Mk0",
        "outputId": "4b60fad7-ead3-4ea4-e09a-48e700d374bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f66187b44ddb42a5a1fad3146c3f6255"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "28efaf5a401c4606b30c437456d5f019"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd1c9720d9d1414586a2e5a038385430"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/714 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2d6c7e9167044bd84aef59d602fe6ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52d815c0305f4a108d7013d0d5a3ce9e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "281689f67d11411dbe66f3ca1ee9a411"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1471994fea64dda8f4f6475b92bfb2a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ba6c83db9f84c359c83830843802512"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ee3b7fbd3824cc3b96f90f0dab927fd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "794ab4f8c64d462682d921304c0a2d44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6833ff2efccf4cfab7f9d88a22ca78c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/186 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8613e6d4502e48959e0fec7010c30635"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, in the land of the rising sun, there was a small village nestled in the mountains. The villagers lived simple lives, relying on the land for their sustenance and survival. They were a tight-knit community, bound\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time as time\n",
        "\n",
        "# Example input text\n",
        "input_text = \"I am a Filipino and\"\n",
        "\n",
        "# Capture start time for tokenization\n",
        "tokenize_start_time = time.time()\n",
        "# Tokenize the input\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "tokenize_end_time = time.time()\n",
        "\n",
        "# Capture start time for generation\n",
        "generate_start_time = time.time()\n",
        "# Generate predictions\n",
        "outputs = model.generate(**inputs, max_length=50)\n",
        "generate_end_time = time.time()\n",
        "\n",
        "# Capture start time for decoding\n",
        "decode_start_time = time.time()\n",
        "# Decode the output\n",
        "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "decode_end_time = time.time()\n",
        "\n",
        "# Print the output\n",
        "print(\"Output:\", output_text)\n",
        "\n",
        "# Calculate and print the metrics in milliseconds\n",
        "tokenize_time_ms = (tokenize_end_time - tokenize_start_time) * 1000\n",
        "generate_time_ms = (generate_end_time - generate_start_time) * 1000\n",
        "decode_time_ms = (decode_end_time - decode_start_time) * 1000\n",
        "total_time_ms = tokenize_time_ms + generate_time_ms + decode_time_ms\n",
        "\n",
        "print(f\"Tokenize time: {tokenize_time_ms:.2f} ms\")\n",
        "print(f\"Generate time: {generate_time_ms:.2f} ms\")\n",
        "print(f\"Decode time: {decode_time_ms:.2f} ms\")\n",
        "print(f\"Total time: {total_time_ms:.2f} ms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkEsMChXINzP",
        "outputId": "e83042f6-12fa-4558-807a-d052a72423fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: I am a Filipino and I am proud of my heritage. I believe that the Philippines is a beautiful country with a rich culture and history. I am proud of our national heroes, our traditions, and our people.\n",
            "I am also proud of the\n",
            "Tokenize time: 1.07 ms\n",
            "Generate time: 23601.08 ms\n",
            "Decode time: 0.31 ms\n",
            "Total time: 23602.46 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NL0yGhbe3EFk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ea4db376965b483183b2b3a3c97ccb6f",
            "0be21e876be14ab88a8dd5b789e413e1",
            "c3487547c2ba4d1986ecfa86144989d6",
            "665220344bd047ff83b60aa8e04534d0",
            "66423fc52ebf40758f7948d2abef3a49",
            "537309b50ca341828919a3587d8e6fb0",
            "49df527ab0464941b0df2129a181d549",
            "b05c1666c74f4e61b0e2aceb8c53fc52",
            "9546446318e54236ba0034f7386a8a00",
            "be45ac263c594b7c91fa358336165a59",
            "08fc66ed78ed47efb38601b26bf61ff6"
          ]
        },
        "outputId": "b5cd2f15-be40-41a4-e5c0-013c30c2be06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 29390, done.\u001b[K\n",
            "remote: Counting objects: 100% (8595/8595), done.\u001b[K\n",
            "remote: Compressing objects: 100% (667/667), done.\u001b[K\n",
            "remote: Total 29390 (delta 8288), reused 7978 (delta 7928), pack-reused 20795\u001b[K\n",
            "Receiving objects: 100% (29390/29390), 50.98 MiB | 23.39 MiB/s, done.\n",
            "Resolving deltas: 100% (21127/21127), done.\n",
            "I ccache not found. Consider installing it for faster compilation.\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion \n",
            "I CXXFLAGS:  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE \n",
            "I NVCCFLAGS: -std=c++11 -O3 \n",
            "I LDFLAGS:    \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       c++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c ggml/src/sgemm.cpp -o ggml/src/sgemm.o\n",
            "cc  -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion    -c ggml/src/ggml.c -o ggml/src/ggml.o\n",
            "cc  -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion    -c ggml/src/ggml-alloc.c -o ggml/src/ggml-alloc.o\n",
            "cc  -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion    -c ggml/src/ggml-backend.c -o ggml/src/ggml-backend.o\n",
            "cc -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion     -c ggml/src/ggml-quants.c -o ggml/src/ggml-quants.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c src/llama.cpp -o src/llama.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c src/unicode.cpp -o src/unicode.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c src/unicode-data.cpp -o src/unicode-data.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c common/common.cpp -o common/common.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c common/console.cpp -o common/console.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c common/ngram-cache.cpp -o common/ngram-cache.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c common/sampling.cpp -o common/sampling.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c common/train.cpp -o common/train.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c common/grammar-parser.cpp -o common/grammar-parser.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c common/build-info.cpp -o common/build-info.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c common/json-schema-to-grammar.cpp -o common/json-schema-to-grammar.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/baby-llama/baby-llama.cpp -o examples/baby-llama/baby-llama.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/baby-llama/baby-llama.o -o llama-baby-llama  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/batched/batched.cpp -o examples/batched/batched.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/batched/batched.o -o llama-batched  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/batched-bench/batched-bench.cpp -o examples/batched-bench/batched-bench.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/batched-bench/batched-bench.o -o llama-batched-bench  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/llama-bench/llama-bench.cpp -o examples/llama-bench/llama-bench.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/llama-bench/llama-bench.o -o llama-bench  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/benchmark/benchmark-matmult.cpp -o examples/benchmark/benchmark-matmult.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o common/build-info.o examples/benchmark/benchmark-matmult.o -o llama-benchmark-matmult  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/main/main.cpp -o examples/main/main.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/main/main.o -o llama-cli  \n",
            "\n",
            "====  Run ./llama-cli -h for help.  ====\n",
            "\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp -o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o -o llama-convert-llama2c-to-ggml  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/embedding/embedding.cpp -o examples/embedding/embedding.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/embedding/embedding.o -o llama-embedding  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/eval-callback/eval-callback.cpp -o examples/eval-callback/eval-callback.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/eval-callback/eval-callback.o -o llama-eval-callback  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/export-lora/export-lora.cpp -o examples/export-lora/export-lora.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o examples/export-lora/export-lora.o -o llama-export-lora  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/finetune/finetune.cpp -o examples/finetune/finetune.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/finetune/finetune.o -o llama-finetune  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/gbnf-validator/gbnf-validator.cpp -o examples/gbnf-validator/gbnf-validator.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/gbnf-validator/gbnf-validator.o -o llama-gbnf-validator  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/gguf/gguf.cpp -o examples/gguf/gguf.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o examples/gguf/gguf.o -o llama-gguf  \n",
            "cc -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion  -Iexamples/gguf-hash/deps -c examples/gguf-hash/deps/sha1/sha1.c -o examples/gguf-hash/deps/sha1/sha1.o\n",
            "cc -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion  -Iexamples/gguf-hash/deps -c examples/gguf-hash/deps/xxhash/xxhash.c -o examples/gguf-hash/deps/xxhash/xxhash.o\n",
            "cc -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion  -Iexamples/gguf-hash/deps -c examples/gguf-hash/deps/sha256/sha256.c -o examples/gguf-hash/deps/sha256/sha256.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -Iexamples/gguf-hash/deps -c examples/gguf-hash/gguf-hash.cpp -o examples/gguf-hash/gguf-hash.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  examples/gguf-hash/deps/sha1/sha1.o examples/gguf-hash/deps/xxhash/xxhash.o examples/gguf-hash/deps/sha256/sha256.o ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/gguf-hash/gguf-hash.o -o llama-gguf-hash  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/gguf-split/gguf-split.cpp -o examples/gguf-split/gguf-split.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/gguf-split/gguf-split.o -o llama-gguf-split  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/gritlm/gritlm.cpp -o examples/gritlm/gritlm.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/gritlm/gritlm.o -o llama-gritlm  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/imatrix/imatrix.cpp -o examples/imatrix/imatrix.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/imatrix/imatrix.o -o llama-imatrix  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/infill/infill.cpp -o examples/infill/infill.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/infill/infill.o -o llama-infill  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/llava/llava-cli.cpp -o examples/llava/llava-cli.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/llava/clip.cpp  -o examples/llava/clip.o -Wno-cast-qual\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/llava/llava.cpp -o examples/llava/llava.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/llava/llava-cli.o examples/llava/clip.o examples/llava/llava.o -o llama-llava-cli  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/lookahead/lookahead.cpp -o examples/lookahead/lookahead.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/lookahead/lookahead.o -o llama-lookahead  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/lookup/lookup.cpp -o examples/lookup/lookup.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/lookup/lookup.o -o llama-lookup  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/lookup/lookup-create.cpp -o examples/lookup/lookup-create.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/lookup/lookup-create.o -o llama-lookup-create  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/lookup/lookup-merge.cpp -o examples/lookup/lookup-merge.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/lookup/lookup-merge.o -o llama-lookup-merge  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/lookup/lookup-stats.cpp -o examples/lookup/lookup-stats.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/lookup/lookup-stats.o -o llama-lookup-stats  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/parallel/parallel.cpp -o examples/parallel/parallel.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/parallel/parallel.o -o llama-parallel  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/passkey/passkey.cpp -o examples/passkey/passkey.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/passkey/passkey.o -o llama-passkey  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/perplexity/perplexity.cpp -o examples/perplexity/perplexity.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/perplexity/perplexity.o -o llama-perplexity  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c pocs/vdot/q8dot.cpp -o pocs/vdot/q8dot.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/ggml.o ggml/src/sgemm.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o pocs/vdot/q8dot.o -o llama-q8dot  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/quantize/quantize.cpp -o examples/quantize/quantize.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/quantize/quantize.o -o llama-quantize  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/quantize-stats/quantize-stats.cpp -o examples/quantize-stats/quantize-stats.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/quantize-stats/quantize-stats.o -o llama-quantize-stats  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/retrieval/retrieval.cpp -o examples/retrieval/retrieval.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/retrieval/retrieval.o -o llama-retrieval  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/save-load-state/save-load-state.cpp -o examples/save-load-state/save-load-state.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/save-load-state/save-load-state.o -o llama-save-load-state  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/server/server.cpp -o examples/server/server.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o -Iexamples/server examples/server/server.o -o llama-server   \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/simple/simple.cpp -o examples/simple/simple.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/simple/simple.o -o llama-simple  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/speculative/speculative.cpp -o examples/speculative/speculative.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/speculative/speculative.o -o llama-speculative  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/tokenize/tokenize.cpp -o examples/tokenize/tokenize.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/tokenize/tokenize.o -o llama-tokenize  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/train-text-from-scratch/train-text-from-scratch.cpp -o examples/train-text-from-scratch/train-text-from-scratch.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/train-text-from-scratch/train-text-from-scratch.o -o llama-train-text-from-scratch  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c pocs/vdot/vdot.cpp -o pocs/vdot/vdot.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/ggml.o ggml/src/sgemm.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o pocs/vdot/vdot.o -o llama-vdot  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/cvector-generator/cvector-generator.cpp -o examples/cvector-generator/cvector-generator.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/cvector-generator/cvector-generator.o -o llama-cvector-generator  \n",
            "cc -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu\n",
            "Collecting numpy~=1.26.4 (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 1))\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece~=0.2.0 (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 2))\n",
            "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.40.1 in /usr/local/lib/python3.10/dist-packages (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.41.2)\n",
            "Collecting gguf>=0.1.0 (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 4))\n",
            "  Downloading gguf-0.6.0-py3-none-any.whl (23 kB)\n",
            "Collecting protobuf<5.0.0,>=4.21.0 (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 5))\n",
            "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch~=2.2.1 (from -r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3))\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torch-2.2.2%2Bcpu-cp310-cp310-linux_x86_64.whl (186.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m186.8/186.8 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.40.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.40.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.23.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.40.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.40.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.40.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.40.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.40.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.40.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.40.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.40.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.40.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.40.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.40.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.3.0)\n",
            "Installing collected packages: sentencepiece, protobuf, numpy, torch, gguf\n",
            "  Attempting uninstall: sentencepiece\n",
            "    Found existing installation: sentencepiece 0.1.99\n",
            "    Uninstalling sentencepiece-0.1.99:\n",
            "      Successfully uninstalled sentencepiece-0.1.99\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.0+cu121\n",
            "    Uninstalling torch-2.3.0+cu121:\n",
            "      Successfully uninstalled torch-2.3.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.3 which is incompatible.\n",
            "torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 2.2.2+cpu which is incompatible.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.2.2+cpu which is incompatible.\n",
            "torchvision 0.18.0+cu121 requires torch==2.3.0, but you have torch 2.2.2+cpu which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gguf-0.6.0 numpy-1.26.4 protobuf-4.25.3 sentencepiece-0.2.0 torch-2.2.2+cpu\n",
            "INFO:hf-to-gguf:Loading model: Daredevil-8B\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 8192\n",
            "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 14336\n",
            "INFO:hf-to-gguf:gguf: head count = 32\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
            "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
            "INFO:hf-to-gguf:gguf: file type = 1\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "INFO:gguf.vocab:Adding 280147 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type bos to 128000\n",
            "INFO:gguf.vocab:Setting special token type eos to 128001\n",
            "INFO:gguf.vocab:Setting chat_template to {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
            "\n",
            "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "' }}{% endif %}\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> F16, shape = {4096, 128256}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.bfloat16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.bfloat16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.bfloat16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:output.weight,               torch.bfloat16 --> F16, shape = {4096, 128256}\n",
            "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.bfloat16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:Daredevil-8B/daredevil-8b.fp16.bin: n_tensors = 291, total_size = 16.1G\n",
            "Writing: 100% 16.1G/16.1G [03:17<00:00, 81.3Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to Daredevil-8B/daredevil-8b.fp16.bin\n",
            "main: build = 3358 (a59f8fdc)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing 'Daredevil-8B/daredevil-8b.fp16.bin' to 'Daredevil-8B/daredevil-8b.Q4_K_M.gguf' as Q4_K_M\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from Daredevil-8B/daredevil-8b.fp16.bin (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = Daredevil-8B\n",
            "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"ƒ† ƒ†\", \"ƒ† ƒ†ƒ†ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
            "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type  f16:  226 tensors\n",
            "[   1/ 291]                    token_embd.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q4_K .. size =  1002.00 MiB ->   281.81 MiB\n",
            "[   2/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   3/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[   4/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[   5/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[   6/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   7/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[   8/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[   9/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  10/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  11/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  12/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  13/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  14/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  15/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  16/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  17/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  18/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  19/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  20/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  21/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  22/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  23/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  24/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  25/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  26/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  27/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  28/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  29/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  30/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  31/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  32/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  33/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  34/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  35/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  36/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  37/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  38/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  39/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  40/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  41/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  42/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  43/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  44/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  45/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  46/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  47/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  48/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  49/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  50/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  51/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  52/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  53/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  54/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  55/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  56/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  57/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  58/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  59/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  60/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  61/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  62/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  63/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  64/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  65/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  66/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  67/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  68/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  69/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  70/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  71/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  72/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  73/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  74/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  75/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  76/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  77/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  78/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  79/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  80/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  81/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  82/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  83/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  84/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  85/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  86/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  87/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  88/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  89/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  90/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  91/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  92/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  93/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  94/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  95/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  96/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  97/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  98/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  99/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 100/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 101/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 102/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 103/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 104/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 105/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 106/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 107/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 108/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 109/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 110/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 111/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 112/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 113/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 114/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 115/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 116/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 117/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 118/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 119/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 120/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 121/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 122/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 123/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 124/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 125/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 126/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 127/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 128/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 129/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 130/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 131/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 132/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 133/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 134/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 135/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 136/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 137/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 138/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 139/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 140/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 141/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 142/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 143/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 144/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 145/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 146/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 147/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 148/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 149/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 150/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 151/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 152/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 153/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 154/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 155/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 156/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 157/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 158/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 159/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 160/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 161/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 162/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 163/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 164/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 165/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 166/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 167/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 168/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 169/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 170/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 171/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 172/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 173/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 174/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 175/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 176/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 177/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 178/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 179/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 180/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 181/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 182/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 183/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 184/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 185/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 186/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 187/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 188/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 189/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 191/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 192/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 193/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 194/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 195/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 196/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 197/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 198/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 199/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 200/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 201/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 202/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 203/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 204/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 205/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 206/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 207/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 208/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 209/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 210/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 211/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 212/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 213/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 214/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 215/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 216/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 217/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 218/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 219/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 220/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 221/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 222/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 223/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 224/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 225/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 226/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 227/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 228/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 229/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 230/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 231/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 232/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 233/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 234/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 235/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 236/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 237/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 238/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 239/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 240/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 241/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 242/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 243/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 244/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 245/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 246/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 247/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 248/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 249/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 250/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 251/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 252/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 253/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 254/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 255/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 256/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 257/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 258/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 259/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 260/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 261/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 262/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 263/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 264/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 265/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 266/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 267/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 268/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 269/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 270/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 271/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 272/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 273/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 274/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 275/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 276/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 277/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 278/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 279/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 280/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 281/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 282/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 283/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 285/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 286/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 287/ 291]                        output.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q6_K .. size =  1002.00 MiB ->   410.98 MiB\n",
            "[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 289/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 290/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 291/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "llama_model_quantize_internal: model size  = 15317.02 MB\n",
            "llama_model_quantize_internal: quant size  =  4685.30 MB\n",
            "\n",
            "main: quantize time = 868664.71 ms\n",
            "main:    total time = 868664.72 ms\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/8.94k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea4db376965b483183b2b3a3c97ccb6f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "HfHubHTTPError",
          "evalue": " (Request ID: Root=1-668e0306-6241bcf4358347d0783f8d68;8b504961-9b2d-47d8-93d7-92a86930c55c)\n\n403 Forbidden: You don't have the rights to create a model under the namespace \"jcorenday\".\nCannot access content at: https://huggingface.co/api/repos/create.\nIf you are trying to create or update content,make sure you have a token with the `write` role.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/api/repos/create",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-400508e250e5>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Upload model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m create_repo(\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mrepo_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{USERNAME}/{MODEL_NAME}-GGUF\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mrepo_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mcreate_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   3267\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mRepoUrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.endpoint}/{repo_type}/{repo_id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3268\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3269\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3270\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3271\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mcreate_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   3254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3255\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3256\u001b[0;31m             \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3257\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3258\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexist_ok\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m409\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0;34m\"make sure you have a token with the `write` role.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             )\n\u001b[0;32m--> 367\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;31m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m:  (Request ID: Root=1-668e0306-6241bcf4358347d0783f8d68;8b504961-9b2d-47d8-93d7-92a86930c55c)\n\n403 Forbidden: You don't have the rights to create a model under the namespace \"jcorenday\".\nCannot access content at: https://huggingface.co/api/repos/create.\nIf you are trying to create or update content,make sure you have a token with the `write` role."
          ]
        }
      ],
      "source": [
        "# @title ## üß© GGUF\n",
        "\n",
        "# @markdown Recommended methods: `q2_k`, `q3_k_m`, `q4_k_m`, `q5_k_m`, `q6_k`, `q8_0`\n",
        "\n",
        "# @markdown Learn more about GGUF and quantization methods in [this article](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html).\n",
        "\n",
        "QUANTIZATION_FORMAT = \"q4_k_m\" # @param {type:\"string\"}\n",
        "QUANTIZATION_METHODS = QUANTIZATION_FORMAT.replace(\" \", \"\").split(\",\")\n",
        "\n",
        "# Install llama.cpp\n",
        "!git clone https://github.com/ggerganov/llama.cpp && cd llama.cpp && make\n",
        "!pip install -r llama.cpp/requirements.txt\n",
        "\n",
        "# Convert to fp16\n",
        "fp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin\"\n",
        "!python llama.cpp/convert_hf_to_gguf.py {MODEL_NAME} --outtype f16 --outfile {fp16}\n",
        "\n",
        "# Quantize the model for each method in the QUANTIZATION_METHODS list\n",
        "for method in QUANTIZATION_METHODS:\n",
        "    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n",
        "    !./llama.cpp/llama-quantize {fp16} {qtype} {method}\n",
        "\n",
        "# Create model card\n",
        "card = ModelCard.load(MODEL_ID)\n",
        "card.data.tags.append(\"autoquant\")\n",
        "card.data.tags.append(\"gguf\")\n",
        "card.save(f'{MODEL_NAME}/README.md')\n",
        "\n",
        "# Upload model\n",
        "create_repo(\n",
        "    repo_id = f\"{USERNAME}/{MODEL_NAME}-GGUF\",\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True,\n",
        "    token=hf_token\n",
        ")\n",
        "api.upload_folder(\n",
        "    folder_path=MODEL_NAME,\n",
        "    repo_id=f\"{USERNAME}/{MODEL_NAME}-GGUF\",\n",
        "    allow_patterns=[\"*.gguf\",\"$.md\"],\n",
        "    token=hf_token\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get(HF_TOKEN)\n",
        "api = HfApi()"
      ],
      "metadata": {
        "id": "ioseTIYuwa4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload model\n",
        "create_repo(\n",
        "    repo_id = f\"{USERNAME}/{MODEL_NAME}-GGUF\",\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True,\n",
        "    token=hf_token\n",
        ")\n",
        "api.upload_folder(\n",
        "    folder_path=MODEL_NAME,\n",
        "    repo_id=f\"{USERNAME}/{MODEL_NAME}-GGUF\",\n",
        "    allow_patterns=[\"*.gguf\",\"$.md\"],\n",
        "    token=hf_token\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "b10dbca46f684e419005350d279d4755",
            "82cbdb187d0f4428af843a8a1ec0b1b6",
            "540c9770091b43c0a4f942c3cf20a251",
            "41765c0e89de46e5b760fdfeaa6fbfb3",
            "68ba4046ac2b4a0da45f1a3dcd337571",
            "77f37f1d2a12400d84729af455b76517",
            "6e9914283d6d4add8e1b1ce89ed696c0",
            "a3163d333044479d939a2a0d7c289714",
            "b796398cc71b4a4fb64a3a2ef1f4c057",
            "17d984410676455fa3cb58a9bdab48a1",
            "d5e837a9d6d648c683f1c50613258ad4"
          ]
        },
        "id": "dTN0Kd9dvmoB",
        "outputId": "3b809c82-1c1e-4cb2-8e99-52bcd7391301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "daredevil-8b.Q4_K_M.gguf:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b10dbca46f684e419005350d279d4755"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/jcorenday/Daredevil-8B-GGUF/commit/47dc2330bada4fd6a9e6eb533285a246f20aed42', commit_message='Upload folder using huggingface_hub', commit_description='', oid='47dc2330bada4fd6a9e6eb533285a246f20aed42', pr_url=None, pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "model_path = \"/content/Daredevil-8B/daredevil-8b.Q4_K_M.gguf\"\n",
        "input_text = \"I am a Filipino and\"\n",
        "\n",
        "# Assuming llama.cpp provides a command-line tool for inference\n",
        "result = subprocess.run(\n",
        "    [\"/content/llama.cpp/llama-simple\", \"-m\", model_path, \"-p\", input_text,\n",
        "     \"--n_predict\", \"50\"],\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "print(result.stdout)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3O0AAMP1J8b",
        "outputId": "e6ba9013-4cd7-4381-a31a-495bfb769eea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byFA50wf9qpT",
        "outputId": "38633867-7def-455f-9666-c6b8997bdcd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['/content/llama.cpp/llama-simple', '-m', '/content/Daredevil-8B/daredevil-8b.Q4_K_M.gguf', '-p', 'I am a Filipino and', '--n_predict', '50'], returncode=0, stdout='', stderr='llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /content/Daredevil-8B/daredevil-8b.Q4_K_M.gguf (version GGUF V3 (latest))\\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\\nllama_model_loader: - kv   0:                       general.architecture str              = llama\\nllama_model_loader: - kv   1:                               general.name str              = Daredevil-8B\\nllama_model_loader: - kv   2:                          llama.block_count u32              = 32\\nllama_model_loader: - kv   3:                       llama.context_length u32              = 8192\\nllama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\\nllama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\\nllama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\\nllama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\\nllama_model_loader: - kv  10:                          general.file_type u32              = 15\\nllama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\\nllama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\\nllama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\\nllama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\\nllama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\\\"\", \"#\", \"$\", \"%\", \"&\", \"\\'\", ...\\nllama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\\nllama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"ƒ† ƒ†\", \"ƒ† ƒ†ƒ†ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"...\\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\\nllama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\\nllama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\\nllama_model_loader: - kv  21:               general.quantization_version u32              = 2\\nllama_model_loader: - type  f32:   65 tensors\\nllama_model_loader: - type q4_K:  193 tensors\\nllama_model_loader: - type q6_K:   33 tensors\\nllm_load_vocab: special tokens cache size = 256\\nllm_load_vocab: token to piece cache size = 0.8000 MB\\nllm_load_print_meta: format           = GGUF V3 (latest)\\nllm_load_print_meta: arch             = llama\\nllm_load_print_meta: vocab type       = BPE\\nllm_load_print_meta: n_vocab          = 128256\\nllm_load_print_meta: n_merges         = 280147\\nllm_load_print_meta: vocab_only       = 0\\nllm_load_print_meta: n_ctx_train      = 8192\\nllm_load_print_meta: n_embd           = 4096\\nllm_load_print_meta: n_layer          = 32\\nllm_load_print_meta: n_head           = 32\\nllm_load_print_meta: n_head_kv        = 8\\nllm_load_print_meta: n_rot            = 128\\nllm_load_print_meta: n_swa            = 0\\nllm_load_print_meta: n_embd_head_k    = 128\\nllm_load_print_meta: n_embd_head_v    = 128\\nllm_load_print_meta: n_gqa            = 4\\nllm_load_print_meta: n_embd_k_gqa     = 1024\\nllm_load_print_meta: n_embd_v_gqa     = 1024\\nllm_load_print_meta: f_norm_eps       = 0.0e+00\\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\\nllm_load_print_meta: f_logit_scale    = 0.0e+00\\nllm_load_print_meta: n_ff             = 14336\\nllm_load_print_meta: n_expert         = 0\\nllm_load_print_meta: n_expert_used    = 0\\nllm_load_print_meta: causal attn      = 1\\nllm_load_print_meta: pooling type     = 0\\nllm_load_print_meta: rope type        = 0\\nllm_load_print_meta: rope scaling     = linear\\nllm_load_print_meta: freq_base_train  = 500000.0\\nllm_load_print_meta: freq_scale_train = 1\\nllm_load_print_meta: n_ctx_orig_yarn  = 8192\\nllm_load_print_meta: rope_finetuned   = unknown\\nllm_load_print_meta: ssm_d_conv       = 0\\nllm_load_print_meta: ssm_d_inner      = 0\\nllm_load_print_meta: ssm_d_state      = 0\\nllm_load_print_meta: ssm_dt_rank      = 0\\nllm_load_print_meta: model type       = 8B\\nllm_load_print_meta: model ftype      = Q4_K - Medium\\nllm_load_print_meta: model params     = 8.03 B\\nllm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \\nllm_load_print_meta: general.name     = Daredevil-8B\\nllm_load_print_meta: BOS token        = 128000 \\'<|begin_of_text|>\\'\\nllm_load_print_meta: EOS token        = 128001 \\'<|end_of_text|>\\'\\nllm_load_print_meta: LF token         = 128 \\'√Ñ\\'\\nllm_load_print_meta: EOT token        = 128009 \\'<|eot_id|>\\'\\nllm_load_print_meta: max token length = 256\\nllm_load_tensors: ggml ctx size =    0.14 MiB\\nllm_load_tensors:        CPU buffer size =  4685.30 MiB\\n........................................................................................\\nllama_new_context_with_model: n_ctx      = 8192\\nllama_new_context_with_model: n_batch    = 2048\\nllama_new_context_with_model: n_ubatch   = 512\\nllama_new_context_with_model: flash_attn = 0\\nllama_new_context_with_model: freq_base  = 500000.0\\nllama_new_context_with_model: freq_scale = 1\\nllama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\\nllama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\\nllama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\\nllama_new_context_with_model:        CPU compute buffer size =   560.01 MiB\\nllama_new_context_with_model: graph nodes  = 1030\\nllama_new_context_with_model: graph splits = 1\\n\\nmain: n_predict = 50, n_ctx = 8192, n_kv_req = 50\\n\\n<|begin_of_text|>I am a Filipino and I am proud to be one. I am proud of our rich culture, our beautiful language, our delicious food, and our warm hospitality. I am proud of our history, our heroes, and our struggles. I am\\n\\nmain: decoded 44 tokens in 28.92 s, speed: 1.52 t/s\\n\\nllama_print_timings:        load time =    3356.56 ms\\nllama_print_timings:      sample time =       8.59 ms /    45 runs   (    0.19 ms per token,  5238.65 tokens per second)\\nllama_print_timings: prompt eval time =    2478.02 ms /     6 tokens (  413.00 ms per token,     2.42 tokens per second)\\nllama_print_timings:        eval time =   28863.81 ms /    44 runs   (  656.00 ms per token,     1.52 tokens per second)\\nllama_print_timings:       total time =   32274.32 ms /    50 tokens\\n\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Check GPU availability\n",
        "gpu_available = torch.cuda.is_available()\n",
        "\n",
        "if gpu_available:\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"GPU: {gpu_name}\")\n",
        "else:\n",
        "    print(\"No GPU available, please enable GPU in the runtime settings.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsLq-Vw5Czv8",
        "outputId": "9a437fc4-58ab-42d2-aa4f-de81e094dac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir7sRmfpKWdH",
        "outputId": "ea340aba-2cfb-435a-92d9-6668e21ca2f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.82)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OE_R3AXG5Y-F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123,
          "referenced_widgets": [
            "21fdba3514104fa6b76cf83f1c205a75",
            "47b4b8ea49ac48b9b50b696f60129260",
            "448a2eaa96f04d43a9dc98d6f5bfc5a6",
            "6a5db9e257694d288d29fbe42697869b",
            "cc906292ba7046d9a6ac9098097e4ecd",
            "66710857160a408588970a8c8f50a075",
            "c10e4f3ece5b4424b955f722bc5eceae",
            "ec22984f67c54d31818a7941f2a61d24",
            "c1c8f11269b341f48ad78550698a440e",
            "c51ce038d34443d293c070fecded3a4e",
            "f92c20f0e24c472cab7d93ec50fdf21a"
          ]
        },
        "outputId": "f7bbfb05-c748-49ab-de9a-f6fd4b663d29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21fdba3514104fa6b76cf83f1c205a75"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "# @title ## üß† GPTQ\n",
        "\n",
        "# @markdown Learn more about the GPTQ algorithm in [this article](https://mlabonne.github.io/blog/posts/4_bit_Quantization_with_GPTQ.html).\n",
        "\n",
        "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import random\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "BITS = 4 # @param {type:\"integer\"}\n",
        "GROUP_SIZE = 128 # @param {type:\"integer\"}\n",
        "DAMP_PERCENT = 0.01 # @param {type:\"number\"}\n",
        "\n",
        "# Load quantize config, model and tokenizer\n",
        "quantize_config = BaseQuantizeConfig(\n",
        "    bits=BITS,\n",
        "    group_size=GROUP_SIZE,\n",
        "    damp_percent=DAMP_PERCENT,\n",
        "    desc_act=False,\n",
        ")\n",
        "model = AutoGPTQForCausalLM.from_pretrained(MODEL_ID, quantize_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "\n",
        "# Load data and tokenize examples\n",
        "n_samples = 1024\n",
        "data = load_dataset(\"allenai/c4\", data_files=\"en/c4-train.00001-of-01024.json.gz\", split=f\"train[:{n_samples*5}]\")\n",
        "tokenized_data = tokenizer(\"\\n\\n\".join(data['text']), return_tensors='pt')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data.input_ids.shape[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6ja5i6jUuRJ",
        "outputId": "448b7b5a-7e22-48f6-eb06-af8fcaa7ea43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2341884"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.model_max_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwF10KxWU7FH",
        "outputId": "b3b9ae50-9711-409b-989f-3b6b6aaec06f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000000000000000019884624838656"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data.input_ids.shape[1] - tokenizer.model_max_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDbdUnv8upBU",
        "outputId": "abe7dace-47d5-4049-8f60-55c73b5ff3c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1000000000000000019884622496772"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Format tokenized examples\n",
        "examples_ids = []\n",
        "for _ in range(n_samples):\n",
        "    i = random.randint(0, tokenizer.model_max_length - tokenized_data.input_ids.shape[1] - 1)\n",
        "    j = i + tokenizer.model_max_length\n",
        "    input_ids = tokenized_data.input_ids[:, i:j]\n",
        "    attention_mask = torch.ones_like(input_ids)\n",
        "    examples_ids.append({'input_ids': input_ids, 'attention_mask': attention_mask})\n"
      ],
      "metadata": {
        "id": "22tLHl2-UGRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Quantize with GPTQ\n",
        "model.quantize(\n",
        "    examples_ids,\n",
        "    batch_size=128,\n",
        "    use_triton=True,\n",
        ")\n",
        "\n",
        "# Save model and tokenizer\n",
        "save_folder = MODEL_ID + \"-GPTQ\"\n",
        "model.save_quantized(save_folder, use_safetensors=True)\n",
        "tokenizer.save_pretrained(save_folder)\n",
        "\n",
        "# Create model card\n",
        "card = ModelCard.load(MODEL_ID)\n",
        "card.data.tags.append(\"autoquant\")\n",
        "card.data.tags.append(\"gptq\")\n",
        "card.save(f'{save_folder}/README.md')\n",
        "\n",
        "# Upload model\n",
        "create_repo(\n",
        "    repo_id = f\"{USERNAME}/{MODEL_NAME}-GPTQ\",\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True,\n",
        "    token=hf_token\n",
        ")\n",
        "api.upload_folder(\n",
        "    folder_path=save_folder,\n",
        "    repo_id=f\"{USERNAME}/{MODEL_NAME}-GPTQ\",\n",
        "    token=hf_token\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9434636914074d43a43331c65af84648",
            "e8bc2a40745d43b8a5592658153106df",
            "eb6dff63847246479fc5bf30f4de48c1",
            "bb5575e8670148319ea76862c0cd64aa",
            "396460d9c43041ac8774ffff141989f7",
            "5f46060e7f0a4114a287fe188718492b",
            "4f2c9f648bf142128a4032496a72d1f8",
            "d4280dbcde904c6caef00bda720bbbdd",
            "f1fb92f5013b4e1fa2cd1f60700312ce",
            "d1586fef5dcb45efa902878b326e22dc",
            "7a10c955985e4ce0b4e8ef08e29858b8"
          ]
        },
        "id": "QcogeUtvVQ0p",
        "outputId": "e49fdf62-2257-4301-bbba-454ec8838595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO - Start quantizing layer 1/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 1/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 1/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 1/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 1/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 1/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 1/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 1/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 1/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 1/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 1/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 1/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 1/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 1/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 1/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 1/32...\n",
            "INFO - Start quantizing layer 2/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 2/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 2/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 2/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 2/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 2/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 2/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 2/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 2/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 2/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 2/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 2/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 2/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 2/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 2/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 2/32...\n",
            "INFO - Start quantizing layer 3/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 3/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 3/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 3/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 3/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 3/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 3/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 3/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 3/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 3/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 3/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 3/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 3/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 3/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 3/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 3/32...\n",
            "INFO - Start quantizing layer 4/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 4/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 4/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 4/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 4/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 4/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 4/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 4/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 4/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 4/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 4/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 4/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 4/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 4/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 4/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 4/32...\n",
            "INFO - Start quantizing layer 5/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 5/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 5/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 5/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 5/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 5/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 5/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 5/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 5/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 5/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 5/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 5/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 5/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 5/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 5/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 5/32...\n",
            "INFO - Start quantizing layer 6/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 6/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 6/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 6/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 6/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 6/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 6/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 6/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 6/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 6/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 6/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 6/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 6/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 6/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 6/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 6/32...\n",
            "INFO - Start quantizing layer 7/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 7/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 7/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 7/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 7/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 7/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 7/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 7/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 7/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 7/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 7/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 7/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 7/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 7/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 7/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 7/32...\n",
            "INFO - Start quantizing layer 8/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 8/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 8/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 8/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 8/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 8/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 8/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 8/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 8/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 8/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 8/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 8/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 8/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 8/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 8/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 8/32...\n",
            "INFO - Start quantizing layer 9/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 9/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 9/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 9/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 9/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 9/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 9/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 9/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 9/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 9/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 9/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 9/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 9/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 9/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 9/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 9/32...\n",
            "INFO - Start quantizing layer 10/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 10/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 10/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 10/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 10/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 10/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 10/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 10/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 10/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 10/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 10/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 10/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 10/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 10/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 10/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 10/32...\n",
            "INFO - Start quantizing layer 11/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 11/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 11/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 11/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 11/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 11/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 11/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 11/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 11/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 11/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 11/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 11/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 11/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 11/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 11/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 11/32...\n",
            "INFO - Start quantizing layer 12/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 12/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 12/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 12/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 12/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 12/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 12/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 12/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 12/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 12/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 12/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 12/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 12/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 12/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 12/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 12/32...\n",
            "INFO - Start quantizing layer 13/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 13/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 13/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 13/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 13/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 13/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 13/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 13/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 13/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 13/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 13/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 13/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 13/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 13/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 13/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 13/32...\n",
            "INFO - Start quantizing layer 14/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 14/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 14/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 14/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 14/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 14/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 14/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 14/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 14/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 14/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 14/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 14/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 14/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 14/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 14/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 14/32...\n",
            "INFO - Start quantizing layer 15/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 15/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 15/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 15/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 15/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 15/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 15/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 15/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 15/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 15/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 15/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 15/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 15/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 15/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 15/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 15/32...\n",
            "INFO - Start quantizing layer 16/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 16/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 16/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 16/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 16/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 16/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 16/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 16/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 16/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 16/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 16/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 16/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 16/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 16/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 16/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 16/32...\n",
            "INFO - Start quantizing layer 17/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 17/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 17/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 17/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 17/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 17/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 17/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 17/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 17/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 17/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 17/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 17/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 17/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 17/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 17/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 17/32...\n",
            "INFO - Start quantizing layer 18/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 18/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 18/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 18/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 18/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 18/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 18/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 18/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 18/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 18/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 18/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 18/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 18/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 18/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 18/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 18/32...\n",
            "INFO - Start quantizing layer 19/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 19/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 19/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 19/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 19/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 19/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 19/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 19/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 19/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 19/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 19/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 19/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 19/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 19/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 19/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 19/32...\n",
            "INFO - Start quantizing layer 20/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 20/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 20/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 20/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 20/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 20/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 20/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 20/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 20/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 20/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 20/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 20/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 20/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 20/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 20/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 20/32...\n",
            "INFO - Start quantizing layer 21/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 21/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 21/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 21/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 21/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 21/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 21/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 21/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 21/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 21/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 21/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 21/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 21/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 21/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 21/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 21/32...\n",
            "INFO - Start quantizing layer 22/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 22/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 22/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 22/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 22/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 22/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 22/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 22/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 22/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 22/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 22/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 22/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 22/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 22/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 22/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 22/32...\n",
            "INFO - Start quantizing layer 23/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 23/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 23/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 23/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 23/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 23/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 23/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 23/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 23/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 23/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 23/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 23/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 23/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 23/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 23/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 23/32...\n",
            "INFO - Start quantizing layer 24/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 24/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 24/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 24/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 24/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 24/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 24/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 24/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 24/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 24/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 24/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 24/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 24/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 24/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 24/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 24/32...\n",
            "INFO - Start quantizing layer 25/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 25/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 25/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 25/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 25/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 25/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 25/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 25/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 25/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 25/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 25/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 25/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 25/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 25/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 25/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 25/32...\n",
            "INFO - Start quantizing layer 26/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 26/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 26/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 26/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 26/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 26/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 26/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 26/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 26/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 26/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 26/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 26/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 26/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 26/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 26/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 26/32...\n",
            "INFO - Start quantizing layer 27/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 27/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 27/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 27/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 27/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 27/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 27/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 27/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 27/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 27/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 27/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 27/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 27/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 27/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 27/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 27/32...\n",
            "INFO - Start quantizing layer 28/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 28/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 28/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 28/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 28/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 28/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 28/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 28/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 28/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 28/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 28/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 28/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 28/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 28/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 28/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 28/32...\n",
            "INFO - Start quantizing layer 29/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 29/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 29/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 29/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 29/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 29/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 29/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 29/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 29/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 29/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 29/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 29/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 29/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 29/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 29/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 29/32...\n",
            "INFO - Start quantizing layer 30/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 30/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 30/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 30/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 30/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 30/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 30/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 30/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 30/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 30/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 30/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 30/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 30/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 30/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 30/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 30/32...\n",
            "INFO - Start quantizing layer 31/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 31/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 31/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 31/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 31/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 31/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 31/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 31/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 31/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 31/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 31/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 31/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 31/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 31/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 31/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 31/32...\n",
            "INFO - Start quantizing layer 32/32\n",
            "INFO:auto_gptq.modeling._base:Start quantizing layer 32/32\n",
            "INFO - Quantizing self_attn.k_proj in layer 32/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.k_proj in layer 32/32...\n",
            "INFO - Quantizing self_attn.v_proj in layer 32/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.v_proj in layer 32/32...\n",
            "INFO - Quantizing self_attn.q_proj in layer 32/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.q_proj in layer 32/32...\n",
            "INFO - Quantizing self_attn.o_proj in layer 32/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing self_attn.o_proj in layer 32/32...\n",
            "INFO - Quantizing mlp.up_proj in layer 32/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.up_proj in layer 32/32...\n",
            "INFO - Quantizing mlp.gate_proj in layer 32/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.gate_proj in layer 32/32...\n",
            "INFO - Quantizing mlp.down_proj in layer 32/32...\n",
            "INFO:auto_gptq.modeling._base:Quantizing mlp.down_proj in layer 32/32...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "gptq_model-4bit-128g.safetensors:   0%|          | 0.00/5.74G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9434636914074d43a43331c65af84648"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 32min 35s, sys: 1min 11s, total: 33min 46s\n",
            "Wall time: 19min 24s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/jcorenday/Daredevil-8B-GPTQ/commit/ae12ad49df545f4bc0b85757c6f4914d10d2f370', commit_message='Upload folder using huggingface_hub', commit_description='', oid='ae12ad49df545f4bc0b85757c6f4914d10d2f370', pr_url=None, pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Reload model and tokenizer\n",
        "model = AutoGPTQForCausalLM.from_quantized(\n",
        "    save_folder,\n",
        "    device=device,\n",
        "    use_triton=True,\n",
        "    use_safetensors=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWJhjmh-dnR9",
        "outputId": "6974927a-a2fe-4ecf-970d-31f93716d5bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
            "WARNING:auto_gptq.modeling._base:Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
            "WARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
            "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
            "2. You are using pytorch without CUDA support.\n",
            "3. CUDA and nvcc are not installed in your device.\n",
            "WARNING:auto_gptq.modeling._base:CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
            "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
            "2. You are using pytorch without CUDA support.\n",
            "3. CUDA and nvcc are not installed in your device.\n",
            "WARNING - ignoring unknown parameter in quantize_config.json: quant_method.\n",
            "WARNING:auto_gptq.modeling._base:ignoring unknown parameter in quantize_config.json: quant_method.\n",
            "INFO - The layer lm_head is not quantized.\n",
            "INFO:auto_gptq.modeling._base:The layer lm_head is not quantized.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the generator pipeline\n",
        "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Perform the text generation\n",
        "result = generator(\"I am a Filipino and\", do_sample=True, max_length=50)[0]['generated_text']\n",
        "\n",
        "# Stop the timer\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the runtime\n",
        "runtime = end_time - start_time\n",
        "\n",
        "# Print the result and the runtime\n",
        "print(f\"Generated Text: {result}\")\n",
        "print(f\"Runtime: {runtime:.4f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QM6MmvKceWe2",
        "outputId": "3af99fa3-e53e-4d0d-9a94-085f93832774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model 'LlamaGPTQForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: I am a Filipino and/or ulus FiorEqualityCompareruttlehevoadycastle herselfËû∫essaomer GioholdiramÊóèËá™Ê≤ªlite Franklingow ÔøΩreyavouritesjonktopin√°–ª—É–≥–∏93ymebrig ourselves cinsFormatExceptionoenardy#adonomyeping Bryÿ´€åÿ±_TScasecmpwickATRIX„É¨„Çπ Trib\n",
            "Runtime: 5.7416 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYqXRPUWnbi2",
        "outputId": "e5b91f00-ec87-40db-85d5-285b0cb96b72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZC9Nsr9u5WhN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9c7e429-8add-46c0-ba84-af48428071eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n",
            "Cloning into 'Daredevil-8B'...\n",
            "remote: Enumerating objects: 92, done.\u001b[K\n",
            "remote: Counting objects: 100% (89/89), done.\u001b[K\n",
            "remote: Compressing objects: 100% (89/89), done.\u001b[K\n",
            "remote: Total 92 (delta 41), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (92/92), 2.26 MiB | 6.17 MiB/s, done.\n",
            "Filtering content: 100% (4/4), 2.95 GiB | 13.56 MiB/s, done.\n",
            "Encountered 3 file(s) that may not have been copied correctly on Windows:\n",
            "\tmodel-00002-of-00004.safetensors\n",
            "\tmodel-00003-of-00004.safetensors\n",
            "\tmodel-00001-of-00004.safetensors\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n",
            "rm: cannot remove 'base_mode/*.bin': No such file or directory\n",
            "--2024-07-10 18:49:37--  https://huggingface.co/datasets/wikitext/resolve/9a9e482b5987f9d25b3a9b2883fc6cc9fd8071b3/wikitext-103-v1/wikitext-test.parquet\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.55, 18.164.174.17, 18.164.174.118, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.55|:443... connected.\n",
            "HTTP request sent, awaiting response... 307 Temporary Redirect\n",
            "Location: /datasets/Salesforce/wikitext/resolve/9a9e482b5987f9d25b3a9b2883fc6cc9fd8071b3/wikitext-103-v1/wikitext-test.parquet [following]\n",
            "--2024-07-10 18:49:37--  https://huggingface.co/datasets/Salesforce/wikitext/resolve/9a9e482b5987f9d25b3a9b2883fc6cc9fd8071b3/wikitext-103-v1/wikitext-test.parquet\n",
            "Reusing existing connection to huggingface.co:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 721735 (705K)\n",
            "Saving to: ‚Äòwikitext-test.parquet‚Äô\n",
            "\n",
            "wikitext-test.parqu 100%[===================>] 704.82K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-07-10 18:49:37 (17.2 MB/s) - ‚Äòwikitext-test.parquet‚Äô saved [721735/721735]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title # ü¶ô ExLlamaV2\n",
        "\n",
        "# @markdown Learn more about ExLlamaV2 in [this article](https://mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run%C2%A0LLMs.html).\n",
        "\n",
        "MODEL_NAME = \"Daredevil-8B\"\n",
        "BPW = 4.0 # @param {type:\"number\"}\n",
        "\n",
        "# Install ExLlamaV2\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/mlabonne/{MODEL_NAME}\n",
        "!mv {MODEL_NAME} base_model\n",
        "!rm base_mode/*.bin\n",
        "\n",
        "# Download dataset\n",
        "!wget https://huggingface.co/datasets/wikitext/resolve/9a9e482b5987f9d25b3a9b2883fc6cc9fd8071b3/wikitext-103-v1/wikitext-test.parquet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantize model\n",
        "!mkdir quant\n",
        "!python exllamav2/convert.py \\\n",
        "    -i base_model \\\n",
        "    -o quant \\\n",
        "    -c wikitext-test.parquet \\\n",
        "    -b {BPW}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ir4nxQffpj1U",
        "outputId": "edd9fd8f-3005-4cb2-e5f7-da14a3411028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            " -- model.layers.2.self_attn.v_proj                    1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.2.self_attn.v_proj                    1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.2.self_attn.v_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.2.self_attn.v_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.2.self_attn.v_proj                    1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.2.self_attn.v_proj                    1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.2.self_attn.v_proj                    1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.2.self_attn.v_proj                    1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.2.self_attn.v_proj                    1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.2.self_attn.v_proj                    1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.2.self_attn.o_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.2.self_attn.o_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.2.self_attn.o_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.2.self_attn.o_proj                    1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.2.self_attn.o_proj                    1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.2.self_attn.o_proj                    1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.2.self_attn.o_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.2.self_attn.o_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.2.self_attn.o_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.2.self_attn.o_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.2.self_attn.o_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.2.self_attn.o_proj                    1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.2.self_attn.o_proj                    1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.2.self_attn.o_proj                    1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.98080723\n",
            " -- 2.1987 bpw  accuracy: 0.98209001\n",
            " -- 2.2831 bpw  accuracy: 0.98463958\n",
            " -- 2.6768 bpw  accuracy: 0.98738944\n",
            " -- 3.1689 bpw  accuracy: 0.98961051\n",
            " -- 3.1705 bpw  accuracy: 0.98945444\n",
            " -- 4.0439 bpw  accuracy: 0.99300397\n",
            " -- 4.0471 bpw  accuracy: 0.99321618\n",
            " -- 4.0816 bpw  accuracy: 0.99348628\n",
            " -- 4.1381 bpw  accuracy: 0.99376577\n",
            " -- 4.1705 bpw  accuracy: 0.99371748\n",
            " -- 4.1902 bpw  accuracy: 0.99392052\n",
            " -- 4.2737 bpw  accuracy: 0.99519440\n",
            " -- 4.3295 bpw  accuracy: 0.99567484\n",
            " -- 5.2564 bpw  accuracy: 0.99732562\n",
            " -- 5.3295 bpw  accuracy: 0.99781285\n",
            " -- 6.0439 bpw  accuracy: 0.99765799\n",
            " -- 6.3381 bpw  accuracy: 0.99904341\n",
            " -- 8.0439 bpw  accuracy: 0.99940203\n",
            "--------------------------------------------\n",
            "| Measured: model.layers.2 (Attention)     |\n",
            "| Duration: 11.22 seconds                  |\n",
            "| Completed step: 5/67                     |\n",
            "| Avg time / step (rolling): 20.25 seconds |\n",
            "| Estimated remaining time: 20min 55sec    |\n",
            "| Last checkpoint layer: None              |\n",
            "--------------------------------------------\n",
            " -- Layer: model.layers.2 (MLP)\n",
            " -- model.layers.2.mlp.gate_proj                       0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.2.mlp.gate_proj                       0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.2.mlp.gate_proj                       0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.2.mlp.gate_proj                       0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.2.mlp.gate_proj                       1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.2.mlp.gate_proj                       1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.2.mlp.gate_proj                       0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.2.mlp.gate_proj                       0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.2.mlp.gate_proj                       0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.2.mlp.gate_proj                       0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.2.mlp.gate_proj                       1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.2.mlp.gate_proj                       0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.2.mlp.gate_proj                       1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.2.mlp.up_proj                         0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.2.mlp.up_proj                         0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.2.mlp.up_proj                         0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.2.mlp.up_proj                         0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.2.mlp.up_proj                         0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.2.mlp.up_proj                         1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.2.mlp.up_proj                         0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.2.mlp.up_proj                         0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.2.mlp.up_proj                         0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.2.mlp.up_proj                         0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.2.mlp.up_proj                         1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.2.mlp.up_proj                         0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.2.mlp.up_proj                         1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.2.mlp.down_proj                       0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.2.mlp.down_proj                       0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.2.mlp.down_proj                       0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.2.mlp.down_proj                       0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.2.mlp.down_proj                       0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.2.mlp.down_proj                       0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.2.mlp.down_proj                       0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.2.mlp.down_proj                       0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.2.mlp.down_proj                       0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.2.mlp.down_proj                       0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.2.mlp.down_proj                       0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.2.mlp.down_proj                       0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.2.mlp.down_proj                       0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.2.mlp.down_proj                       1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.96722421\n",
            " -- 2.3230 bpw  accuracy: 0.96809983\n",
            " -- 2.5958 bpw  accuracy: 0.97265660\n",
            " -- 2.9120 bpw  accuracy: 0.97405866\n",
            " -- 3.2833 bpw  accuracy: 0.98356107\n",
            " -- 3.3655 bpw  accuracy: 0.98484620\n",
            " -- 3.6186 bpw  accuracy: 0.98678508\n",
            " -- 4.1368 bpw  accuracy: 0.99147916\n",
            " -- 4.1977 bpw  accuracy: 0.99222843\n",
            " -- 4.2662 bpw  accuracy: 0.99164253\n",
            " -- 4.3484 bpw  accuracy: 0.99266611\n",
            " -- 5.2491 bpw  accuracy: 0.99574640\n",
            " -- 5.3313 bpw  accuracy: 0.99634628\n",
            " -- 6.0713 bpw  accuracy: 0.99769318\n",
            " -- 6.3032 bpw  accuracy: 0.99783492\n",
            " -- 6.8687 bpw  accuracy: 0.99823628\n",
            " -- 8.0354 bpw  accuracy: 0.99937564\n",
            "--------------------------------------------\n",
            "| Measured: model.layers.2 (MLP)           |\n",
            "| Duration: 31.53 seconds                  |\n",
            "| Completed step: 6/67                     |\n",
            "| Avg time / step (rolling): 22.13 seconds |\n",
            "| Estimated remaining time: 22min 29sec    |\n",
            "| Last checkpoint layer: None              |\n",
            "--------------------------------------------\n",
            " -- Layer: model.layers.3 (Attention)\n",
            " -- model.layers.3.self_attn.q_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.3.self_attn.q_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.3.self_attn.q_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.3.self_attn.q_proj                    1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.3.self_attn.q_proj                    1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.3.self_attn.q_proj                    1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.3.self_attn.q_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.3.self_attn.q_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.3.self_attn.q_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.3.self_attn.q_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.3.self_attn.q_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.3.self_attn.q_proj                    1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.3.self_attn.q_proj                    1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.3.self_attn.q_proj                    1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.3.self_attn.k_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.3.self_attn.k_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.3.self_attn.k_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.3.self_attn.k_proj                    1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.3.self_attn.k_proj                    1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.3.self_attn.k_proj                    1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.3.self_attn.k_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.3.self_attn.k_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.3.self_attn.k_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.3.self_attn.k_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.3.self_attn.k_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.3.self_attn.k_proj                    1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.3.self_attn.k_proj                    1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.3.self_attn.k_proj                    1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.3.self_attn.v_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.3.self_attn.v_proj                    0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.3.self_attn.v_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.3.self_attn.v_proj                    0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.3.self_attn.v_proj                    1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.3.self_attn.v_proj                    1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.3.self_attn.v_proj                    1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.3.self_attn.v_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.3.self_attn.v_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.3.self_attn.v_proj                    1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.3.self_attn.v_proj                    1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.3.self_attn.v_proj                    1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.3.self_attn.v_proj                    1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.3.self_attn.v_proj                    1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.3.self_attn.v_proj                    1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.3.self_attn.o_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.3.self_attn.o_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.3.self_attn.o_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.3.self_attn.o_proj                    1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.3.self_attn.o_proj                    1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.3.self_attn.o_proj                    1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.3.self_attn.o_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.3.self_attn.o_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.3.self_attn.o_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.3.self_attn.o_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.3.self_attn.o_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.3.self_attn.o_proj                    1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.3.self_attn.o_proj                    1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.3.self_attn.o_proj                    1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.95625625\n",
            " -- 2.1987 bpw  accuracy: 0.96170760\n",
            " -- 2.2831 bpw  accuracy: 0.96958410\n",
            " -- 2.6768 bpw  accuracy: 0.97281040\n",
            " -- 3.1689 bpw  accuracy: 0.98026132\n",
            " -- 3.1705 bpw  accuracy: 0.97958321\n",
            " -- 4.0439 bpw  accuracy: 0.98957158\n",
            " -- 4.0471 bpw  accuracy: 0.98950158\n",
            " -- 4.0816 bpw  accuracy: 0.99077267\n",
            " -- 4.1381 bpw  accuracy: 0.99093672\n",
            " -- 4.1705 bpw  accuracy: 0.99002243\n",
            " -- 4.1902 bpw  accuracy: 0.99073554\n",
            " -- 4.2737 bpw  accuracy: 0.99343202\n",
            " -- 4.3295 bpw  accuracy: 0.99391945\n",
            " -- 5.2564 bpw  accuracy: 0.99637102\n",
            " -- 5.3295 bpw  accuracy: 0.99697151\n",
            " -- 6.0439 bpw  accuracy: 0.99696637\n",
            " -- 6.3381 bpw  accuracy: 0.99875362\n",
            " -- 8.0439 bpw  accuracy: 0.99923785\n",
            "--------------------------------------------\n",
            "| Measured: model.layers.3 (Attention)     |\n",
            "| Duration: 11.37 seconds                  |\n",
            "| Completed step: 7/67                     |\n",
            "| Avg time / step (rolling): 20.59 seconds |\n",
            "| Estimated remaining time: 20min 35sec    |\n",
            "| Last checkpoint layer: None              |\n",
            "--------------------------------------------\n",
            " -- Layer: model.layers.3 (MLP)\n",
            " -- model.layers.3.mlp.gate_proj                       0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.3.mlp.gate_proj                       0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.3.mlp.gate_proj                       0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.3.mlp.gate_proj                       0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.3.mlp.gate_proj                       1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.3.mlp.gate_proj                       1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.3.mlp.gate_proj                       0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.3.mlp.gate_proj                       0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.3.mlp.gate_proj                       0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.3.mlp.gate_proj                       0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.3.mlp.gate_proj                       1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.3.mlp.gate_proj                       0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.3.mlp.gate_proj                       1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.3.mlp.up_proj                         0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.3.mlp.up_proj                         0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.3.mlp.up_proj                         0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.3.mlp.up_proj                         0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.3.mlp.up_proj                         0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.3.mlp.up_proj                         1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.3.mlp.up_proj                         0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.3.mlp.up_proj                         0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.3.mlp.up_proj                         0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.3.mlp.up_proj                         0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.3.mlp.up_proj                         1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.3.mlp.up_proj                         0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.3.mlp.up_proj                         1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.3.mlp.down_proj                       0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.3.mlp.down_proj                       0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.3.mlp.down_proj                       0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.3.mlp.down_proj                       0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.3.mlp.down_proj                       0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.3.mlp.down_proj                       0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.3.mlp.down_proj                       0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.3.mlp.down_proj                       0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.3.mlp.down_proj                       0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.3.mlp.down_proj                       0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.3.mlp.down_proj                       0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.3.mlp.down_proj                       0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.3.mlp.down_proj                       0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.3.mlp.down_proj                       1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.95433348\n",
            " -- 2.3230 bpw  accuracy: 0.95559280\n",
            " -- 2.5958 bpw  accuracy: 0.96204342\n",
            " -- 2.9120 bpw  accuracy: 0.96393641\n",
            " -- 3.2833 bpw  accuracy: 0.97703063\n",
            " -- 3.3655 bpw  accuracy: 0.97883203\n",
            " -- 3.6186 bpw  accuracy: 0.98149241\n",
            " -- 4.1368 bpw  accuracy: 0.98817852\n",
            " -- 4.1977 bpw  accuracy: 0.98915897\n",
            " -- 4.2662 bpw  accuracy: 0.98833129\n",
            " -- 4.3484 bpw  accuracy: 0.98974794\n",
            " -- 5.2491 bpw  accuracy: 0.99406732\n",
            " -- 5.3313 bpw  accuracy: 0.99489553\n",
            " -- 6.0713 bpw  accuracy: 0.99679263\n",
            " -- 6.3032 bpw  accuracy: 0.99697656\n",
            " -- 6.8687 bpw  accuracy: 0.99752123\n",
            " -- 8.0354 bpw  accuracy: 0.99914875\n",
            "--------------------------------------------\n",
            "| Measured: model.layers.3 (MLP)           |\n",
            "| Duration: 31.46 seconds                  |\n",
            "| Completed step: 8/67                     |\n",
            "| Avg time / step (rolling): 21.95 seconds |\n",
            "| Estimated remaining time: 21min 34sec    |\n",
            "| Last checkpoint layer: None              |\n",
            "--------------------------------------------\n",
            " -- Layer: model.layers.4 (Attention)\n",
            " -- model.layers.4.self_attn.q_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.4.self_attn.q_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.4.self_attn.q_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.4.self_attn.q_proj                    1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.4.self_attn.q_proj                    1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.4.self_attn.q_proj                    1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.4.self_attn.q_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.4.self_attn.q_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.4.self_attn.q_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.4.self_attn.q_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.4.self_attn.q_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.4.self_attn.q_proj                    1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.4.self_attn.q_proj                    1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.4.self_attn.q_proj                    1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.4.self_attn.k_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.4.self_attn.k_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.4.self_attn.k_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.4.self_attn.k_proj                    1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.4.self_attn.k_proj                    1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.4.self_attn.k_proj                    1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.4.self_attn.k_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.4.self_attn.k_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.4.self_attn.k_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.4.self_attn.k_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.4.self_attn.k_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.4.self_attn.k_proj                    1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.4.self_attn.k_proj                    1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.4.self_attn.k_proj                    1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.4.self_attn.v_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.4.self_attn.v_proj                    0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.4.self_attn.v_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.4.self_attn.v_proj                    0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.4.self_attn.v_proj                    1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.4.self_attn.v_proj                    1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.4.self_attn.v_proj                    1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.4.self_attn.v_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.4.self_attn.v_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.4.self_attn.v_proj                    1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.4.self_attn.v_proj                    1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.4.self_attn.v_proj                    1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.4.self_attn.v_proj                    1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.4.self_attn.v_proj                    1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.4.self_attn.v_proj                    1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.4.self_attn.o_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.4.self_attn.o_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.4.self_attn.o_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.4.self_attn.o_proj                    1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.4.self_attn.o_proj                    1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.4.self_attn.o_proj                    1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.4.self_attn.o_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.4.self_attn.o_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.4.self_attn.o_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.4.self_attn.o_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.4.self_attn.o_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.4.self_attn.o_proj                    1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.4.self_attn.o_proj                    1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.4.self_attn.o_proj                    1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.95219317\n",
            " -- 2.1987 bpw  accuracy: 0.95427046\n",
            " -- 2.2831 bpw  accuracy: 0.96446398\n",
            " -- 2.6768 bpw  accuracy: 0.96981758\n",
            " -- 3.1689 bpw  accuracy: 0.97648836\n",
            " -- 3.1705 bpw  accuracy: 0.97578864\n",
            " -- 4.0439 bpw  accuracy: 0.98698082\n",
            " -- 4.0471 bpw  accuracy: 0.98743777\n",
            " -- 4.0816 bpw  accuracy: 0.98815520\n",
            " -- 4.1381 bpw  accuracy: 0.98856114\n",
            " -- 4.1705 bpw  accuracy: 0.98813909\n",
            " -- 4.1902 bpw  accuracy: 0.98880029\n",
            " -- 4.2737 bpw  accuracy: 0.99152529\n",
            " -- 4.3295 bpw  accuracy: 0.99232722\n",
            " -- 5.2564 bpw  accuracy: 0.99522413\n",
            " -- 5.3295 bpw  accuracy: 0.99611877\n",
            " -- 6.0439 bpw  accuracy: 0.99605629\n",
            " -- 6.3381 bpw  accuracy: 0.99825691\n",
            " -- 8.0439 bpw  accuracy: 0.99895654\n",
            "--------------------------------------------\n",
            "| Measured: model.layers.4 (Attention)     |\n",
            "| Duration: 11.37 seconds                  |\n",
            "| Completed step: 9/67                     |\n",
            "| Avg time / step (rolling): 20.77 seconds |\n",
            "| Estimated remaining time: 20min 4sec     |\n",
            "| Last checkpoint layer: None              |\n",
            "--------------------------------------------\n",
            " -- Saving checkpoint...\n",
            " -- Layer: model.layers.4 (MLP)\n",
            " -- model.layers.4.mlp.gate_proj                       0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.4.mlp.gate_proj                       0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.4.mlp.gate_proj                       0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.4.mlp.gate_proj                       0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.4.mlp.gate_proj                       1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.4.mlp.gate_proj                       1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.4.mlp.gate_proj                       0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.4.mlp.gate_proj                       0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.4.mlp.gate_proj                       0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.4.mlp.gate_proj                       0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.4.mlp.gate_proj                       1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.4.mlp.gate_proj                       0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.4.mlp.gate_proj                       1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.4.mlp.up_proj                         0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.4.mlp.up_proj                         0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.4.mlp.up_proj                         0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.4.mlp.up_proj                         0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.4.mlp.up_proj                         0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.4.mlp.up_proj                         1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.4.mlp.up_proj                         0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.4.mlp.up_proj                         0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.4.mlp.up_proj                         0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.4.mlp.up_proj                         0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.4.mlp.up_proj                         1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.4.mlp.up_proj                         0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.4.mlp.up_proj                         1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.4.mlp.down_proj                       0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.4.mlp.down_proj                       0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.4.mlp.down_proj                       0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.4.mlp.down_proj                       0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.4.mlp.down_proj                       0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.4.mlp.down_proj                       0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.4.mlp.down_proj                       0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.4.mlp.down_proj                       0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.4.mlp.down_proj                       0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.4.mlp.down_proj                       0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.4.mlp.down_proj                       0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.4.mlp.down_proj                       0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.4.mlp.down_proj                       0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.4.mlp.down_proj                       1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.94122190\n",
            " -- 2.3230 bpw  accuracy: 0.94295569\n",
            " -- 2.5958 bpw  accuracy: 0.95205396\n",
            " -- 2.9120 bpw  accuracy: 0.95461975\n",
            " -- 3.2833 bpw  accuracy: 0.97063692\n",
            " -- 3.3655 bpw  accuracy: 0.97302019\n",
            " -- 3.6186 bpw  accuracy: 0.97660738\n",
            " -- 4.1368 bpw  accuracy: 0.98486790\n",
            " -- 4.1977 bpw  accuracy: 0.98613314\n",
            " -- 4.2662 bpw  accuracy: 0.98506203\n",
            " -- 4.3484 bpw  accuracy: 0.98691952\n",
            " -- 5.2491 bpw  accuracy: 0.99239538\n",
            " -- 5.3313 bpw  accuracy: 0.99347964\n",
            " -- 6.0713 bpw  accuracy: 0.99587419\n",
            " -- 6.3032 bpw  accuracy: 0.99611759\n",
            " -- 6.8687 bpw  accuracy: 0.99688294\n",
            " -- 8.0354 bpw  accuracy: 0.99889702\n",
            "-----------------------------------------------------\n",
            "| Measured: model.layers.4 (MLP)                    |\n",
            "| Duration: 31.71 seconds                           |\n",
            "| Completed step: 10/67                             |\n",
            "| Avg time / step (rolling): 21.87 seconds          |\n",
            "| Estimated remaining time: 20min 46sec             |\n",
            "| Last checkpoint layer: model.layers.4 (Attention) |\n",
            "-----------------------------------------------------\n",
            " -- Layer: model.layers.5 (Attention)\n",
            " -- model.layers.5.self_attn.q_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.5.self_attn.q_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.5.self_attn.q_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.5.self_attn.q_proj                    1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.5.self_attn.q_proj                    1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.5.self_attn.q_proj                    1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.5.self_attn.q_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.5.self_attn.q_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.5.self_attn.q_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.5.self_attn.q_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.5.self_attn.q_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.5.self_attn.q_proj                    1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.5.self_attn.q_proj                    1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.5.self_attn.q_proj                    1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.5.self_attn.k_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.5.self_attn.k_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.5.self_attn.k_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.5.self_attn.k_proj                    1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.5.self_attn.k_proj                    1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.5.self_attn.k_proj                    1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.5.self_attn.k_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.5.self_attn.k_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.5.self_attn.k_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.5.self_attn.k_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.5.self_attn.k_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.5.self_attn.k_proj                    1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.5.self_attn.k_proj                    1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.5.self_attn.k_proj                    1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.5.self_attn.v_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.5.self_attn.v_proj                    0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.5.self_attn.v_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.5.self_attn.v_proj                    0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.5.self_attn.v_proj                    1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.5.self_attn.v_proj                    1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.5.self_attn.v_proj                    1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.5.self_attn.v_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.5.self_attn.v_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.5.self_attn.v_proj                    1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.5.self_attn.v_proj                    1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.5.self_attn.v_proj                    1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.5.self_attn.v_proj                    1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.5.self_attn.v_proj                    1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.5.self_attn.v_proj                    1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.5.self_attn.o_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.5.self_attn.o_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.5.self_attn.o_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.5.self_attn.o_proj                    1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.5.self_attn.o_proj                    1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.5.self_attn.o_proj                    1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.5.self_attn.o_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.5.self_attn.o_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.5.self_attn.o_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.5.self_attn.o_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.5.self_attn.o_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.5.self_attn.o_proj                    1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.5.self_attn.o_proj                    1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.5.self_attn.o_proj                    1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.95278079\n",
            " -- 2.1987 bpw  accuracy: 0.95512853\n",
            " -- 2.2831 bpw  accuracy: 0.96281354\n",
            " -- 2.6768 bpw  accuracy: 0.96897444\n",
            " -- 3.1689 bpw  accuracy: 0.97738278\n",
            " -- 3.1705 bpw  accuracy: 0.97780121\n",
            " -- 4.0439 bpw  accuracy: 0.98689629\n",
            " -- 4.0471 bpw  accuracy: 0.98736016\n",
            " -- 4.0816 bpw  accuracy: 0.98835803\n",
            " -- 4.1381 bpw  accuracy: 0.98882098\n",
            " -- 4.1705 bpw  accuracy: 0.98867004\n",
            " -- 4.1902 bpw  accuracy: 0.98922272\n",
            " -- 4.2737 bpw  accuracy: 0.99108789\n",
            " -- 4.3295 bpw  accuracy: 0.99206111\n",
            " -- 5.2564 bpw  accuracy: 0.99531582\n",
            " -- 5.3295 bpw  accuracy: 0.99601397\n",
            " -- 6.0439 bpw  accuracy: 0.99649868\n",
            " -- 6.3381 bpw  accuracy: 0.99802205\n",
            " -- 8.0439 bpw  accuracy: 0.99912060\n",
            "-----------------------------------------------------\n",
            "| Measured: model.layers.5 (Attention)              |\n",
            "| Duration: 11.19 seconds                           |\n",
            "| Completed step: 11/67                             |\n",
            "| Avg time / step (rolling): 21.69 seconds          |\n",
            "| Estimated remaining time: 20min 14sec             |\n",
            "| Last checkpoint layer: model.layers.4 (Attention) |\n",
            "-----------------------------------------------------\n",
            " -- Layer: model.layers.5 (MLP)\n",
            " -- model.layers.5.mlp.gate_proj                       0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.5.mlp.gate_proj                       0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.5.mlp.gate_proj                       0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.5.mlp.gate_proj                       0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.5.mlp.gate_proj                       1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.5.mlp.gate_proj                       1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.5.mlp.gate_proj                       0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.5.mlp.gate_proj                       0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.5.mlp.gate_proj                       0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.5.mlp.gate_proj                       0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.5.mlp.gate_proj                       1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.5.mlp.gate_proj                       0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.5.mlp.gate_proj                       1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.5.mlp.up_proj                         0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.5.mlp.up_proj                         0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.5.mlp.up_proj                         0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.5.mlp.up_proj                         0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.5.mlp.up_proj                         0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.5.mlp.up_proj                         1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.5.mlp.up_proj                         0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.5.mlp.up_proj                         0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.5.mlp.up_proj                         0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.5.mlp.up_proj                         0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.5.mlp.up_proj                         1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.5.mlp.up_proj                         0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.5.mlp.up_proj                         1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.5.mlp.down_proj                       0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.5.mlp.down_proj                       0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.5.mlp.down_proj                       0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.5.mlp.down_proj                       0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.5.mlp.down_proj                       0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.5.mlp.down_proj                       0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.5.mlp.down_proj                       0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.5.mlp.down_proj                       0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.5.mlp.down_proj                       0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.5.mlp.down_proj                       0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.5.mlp.down_proj                       0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.5.mlp.down_proj                       0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.5.mlp.down_proj                       0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.5.mlp.down_proj                       1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.92999384\n",
            " -- 2.3230 bpw  accuracy: 0.93210711\n",
            " -- 2.5958 bpw  accuracy: 0.94288108\n",
            " -- 2.9120 bpw  accuracy: 0.94591819\n",
            " -- 3.2833 bpw  accuracy: 0.96511707\n",
            " -- 3.3655 bpw  accuracy: 0.96788794\n",
            " -- 3.6186 bpw  accuracy: 0.97216491\n",
            " -- 4.1368 bpw  accuracy: 0.98200612\n",
            " -- 4.1977 bpw  accuracy: 0.98347332\n",
            " -- 4.2662 bpw  accuracy: 0.98224398\n",
            " -- 4.3484 bpw  accuracy: 0.98443235\n",
            " -- 5.2491 bpw  accuracy: 0.99095891\n",
            " -- 5.3313 bpw  accuracy: 0.99224189\n",
            " -- 6.0713 bpw  accuracy: 0.99508456\n",
            " -- 6.3032 bpw  accuracy: 0.99537892\n",
            " -- 6.8687 bpw  accuracy: 0.99627741\n",
            " -- 8.0354 bpw  accuracy: 0.99868519\n",
            "-----------------------------------------------------\n",
            "| Measured: model.layers.5 (MLP)                    |\n",
            "| Duration: 31.53 seconds                           |\n",
            "| Completed step: 12/67                             |\n",
            "| Avg time / step (rolling): 21.54 seconds          |\n",
            "| Estimated remaining time: 19min 44sec             |\n",
            "| Last checkpoint layer: model.layers.4 (Attention) |\n",
            "-----------------------------------------------------\n",
            " -- Layer: model.layers.6 (Attention)\n",
            " -- model.layers.6.self_attn.q_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.6.self_attn.q_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.6.self_attn.q_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.6.self_attn.q_proj                    1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.6.self_attn.q_proj                    1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.6.self_attn.q_proj                    1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.6.self_attn.q_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.6.self_attn.q_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.6.self_attn.q_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.6.self_attn.q_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.6.self_attn.q_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.6.self_attn.q_proj                    1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.6.self_attn.q_proj                    1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.6.self_attn.q_proj                    1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.6.self_attn.k_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.6.self_attn.k_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.6.self_attn.k_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.6.self_attn.k_proj                    1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.6.self_attn.k_proj                    1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.6.self_attn.k_proj                    1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.6.self_attn.k_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.6.self_attn.k_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.6.self_attn.k_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.6.self_attn.k_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.6.self_attn.k_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.6.self_attn.k_proj                    1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.6.self_attn.k_proj                    1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.6.self_attn.k_proj                    1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.6.self_attn.v_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.6.self_attn.v_proj                    0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.6.self_attn.v_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.6.self_attn.v_proj                    0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.6.self_attn.v_proj                    1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.6.self_attn.v_proj                    1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.6.self_attn.v_proj                    1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.6.self_attn.v_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.6.self_attn.v_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.6.self_attn.v_proj                    1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.6.self_attn.v_proj                    1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.6.self_attn.v_proj                    1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.6.self_attn.v_proj                    1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.6.self_attn.v_proj                    1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.6.self_attn.v_proj                    1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.6.self_attn.o_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.6.self_attn.o_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.6.self_attn.o_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.6.self_attn.o_proj                    1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.6.self_attn.o_proj                    1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.6.self_attn.o_proj                    1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.6.self_attn.o_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.6.self_attn.o_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.6.self_attn.o_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.6.self_attn.o_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.6.self_attn.o_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.6.self_attn.o_proj                    1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.6.self_attn.o_proj                    1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.6.self_attn.o_proj                    1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.94706478\n",
            " -- 2.1987 bpw  accuracy: 0.94743562\n",
            " -- 2.2831 bpw  accuracy: 0.95549333\n",
            " -- 2.6768 bpw  accuracy: 0.96566537\n",
            " -- 3.1689 bpw  accuracy: 0.97254444\n",
            " -- 3.1705 bpw  accuracy: 0.97307992\n",
            " -- 4.0439 bpw  accuracy: 0.98545416\n",
            " -- 4.0471 bpw  accuracy: 0.98546592\n",
            " -- 4.0816 bpw  accuracy: 0.98690729\n",
            " -- 4.1381 bpw  accuracy: 0.98751267\n",
            " -- 4.1705 bpw  accuracy: 0.98687375\n",
            " -- 4.1902 bpw  accuracy: 0.98779800\n",
            " -- 4.2737 bpw  accuracy: 0.98959489\n",
            " -- 4.3295 bpw  accuracy: 0.99045532\n",
            " -- 5.2564 bpw  accuracy: 0.99430143\n",
            " -- 5.3295 bpw  accuracy: 0.99520874\n",
            " -- 6.0439 bpw  accuracy: 0.99613157\n",
            " -- 6.3381 bpw  accuracy: 0.99758258\n",
            " -- 8.0439 bpw  accuracy: 0.99898571\n",
            "-----------------------------------------------------\n",
            "| Measured: model.layers.6 (Attention)              |\n",
            "| Duration: 11.29 seconds                           |\n",
            "| Completed step: 13/67                             |\n",
            "| Avg time / step (rolling): 21.53 seconds          |\n",
            "| Estimated remaining time: 19min 22sec             |\n",
            "| Last checkpoint layer: model.layers.4 (Attention) |\n",
            "-----------------------------------------------------\n",
            " -- Layer: model.layers.6 (MLP)\n",
            " -- model.layers.6.mlp.gate_proj                       0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.6.mlp.gate_proj                       0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.6.mlp.gate_proj                       0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.6.mlp.gate_proj                       0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.6.mlp.gate_proj                       1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.6.mlp.gate_proj                       1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.6.mlp.gate_proj                       0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.6.mlp.gate_proj                       0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.6.mlp.gate_proj                       0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.6.mlp.gate_proj                       0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.6.mlp.gate_proj                       1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.6.mlp.gate_proj                       0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.6.mlp.gate_proj                       1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.6.mlp.up_proj                         0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.6.mlp.up_proj                         0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.6.mlp.up_proj                         0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.6.mlp.up_proj                         0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.6.mlp.up_proj                         0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.6.mlp.up_proj                         1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.6.mlp.up_proj                         0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.6.mlp.up_proj                         0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.6.mlp.up_proj                         0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.6.mlp.up_proj                         0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.6.mlp.up_proj                         1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.6.mlp.up_proj                         0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.6.mlp.up_proj                         1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.6.mlp.down_proj                       0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.6.mlp.down_proj                       0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.6.mlp.down_proj                       0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.6.mlp.down_proj                       0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.6.mlp.down_proj                       0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.6.mlp.down_proj                       0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.6.mlp.down_proj                       0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.6.mlp.down_proj                       0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.6.mlp.down_proj                       0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.6.mlp.down_proj                       0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.6.mlp.down_proj                       0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.6.mlp.down_proj                       0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.6.mlp.down_proj                       0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.6.mlp.down_proj                       1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.92101092\n",
            " -- 2.3230 bpw  accuracy: 0.92325196\n",
            " -- 2.5958 bpw  accuracy: 0.93585581\n",
            " -- 2.9120 bpw  accuracy: 0.93936318\n",
            " -- 3.2833 bpw  accuracy: 0.96052404\n",
            " -- 3.3655 bpw  accuracy: 0.96365540\n",
            " -- 3.6186 bpw  accuracy: 0.96866167\n",
            " -- 4.1368 bpw  accuracy: 0.97969414\n",
            " -- 4.1977 bpw  accuracy: 0.98134129\n",
            " -- 4.2662 bpw  accuracy: 0.97988009\n",
            " -- 4.3484 bpw  accuracy: 0.98235278\n",
            " -- 5.2491 bpw  accuracy: 0.98974124\n",
            " -- 5.3313 bpw  accuracy: 0.99119867\n",
            " -- 6.0713 bpw  accuracy: 0.99445348\n",
            " -- 6.3032 bpw  accuracy: 0.99475679\n",
            " -- 6.8687 bpw  accuracy: 0.99582309\n",
            " -- 8.0354 bpw  accuracy: 0.99852741\n",
            "-----------------------------------------------------\n",
            "| Measured: model.layers.6 (MLP)                    |\n",
            "| Duration: 31.53 seconds                           |\n",
            "| Completed step: 14/67                             |\n",
            "| Avg time / step (rolling): 21.42 seconds          |\n",
            "| Estimated remaining time: 18min 55sec             |\n",
            "| Last checkpoint layer: model.layers.4 (Attention) |\n",
            "-----------------------------------------------------\n",
            " -- Layer: model.layers.7 (Attention)\n",
            " -- model.layers.7.self_attn.q_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.7.self_attn.q_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.7.self_attn.q_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.7.self_attn.q_proj                    1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.7.self_attn.q_proj                    1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.7.self_attn.q_proj                    1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.7.self_attn.q_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.7.self_attn.q_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.7.self_attn.q_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.7.self_attn.q_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.7.self_attn.q_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.7.self_attn.q_proj                    1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.7.self_attn.q_proj                    1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.7.self_attn.q_proj                    1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.7.self_attn.k_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.7.self_attn.k_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.7.self_attn.k_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.7.self_attn.k_proj                    1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.7.self_attn.k_proj                    1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.7.self_attn.k_proj                    1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.7.self_attn.k_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.7.self_attn.k_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.7.self_attn.k_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.7.self_attn.k_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.7.self_attn.k_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.7.self_attn.k_proj                    1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.7.self_attn.k_proj                    1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.7.self_attn.k_proj                    1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.7.self_attn.v_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.7.self_attn.v_proj                    0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.7.self_attn.v_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.7.self_attn.v_proj                    0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.7.self_attn.v_proj                    1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.7.self_attn.v_proj                    1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.7.self_attn.v_proj                    1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.7.self_attn.v_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.7.self_attn.v_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.7.self_attn.v_proj                    1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.7.self_attn.v_proj                    1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.7.self_attn.v_proj                    1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.7.self_attn.v_proj                    1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.7.self_attn.v_proj                    1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.7.self_attn.v_proj                    1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.7.self_attn.o_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.7.self_attn.o_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.7.self_attn.o_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.7.self_attn.o_proj                    1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.7.self_attn.o_proj                    1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.7.self_attn.o_proj                    1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.7.self_attn.o_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.7.self_attn.o_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.7.self_attn.o_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.7.self_attn.o_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.7.self_attn.o_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.7.self_attn.o_proj                    1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.7.self_attn.o_proj                    1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.7.self_attn.o_proj                    1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.94088483\n",
            " -- 2.1987 bpw  accuracy: 0.94351500\n",
            " -- 2.2831 bpw  accuracy: 0.95105974\n",
            " -- 2.6768 bpw  accuracy: 0.96285213\n",
            " -- 3.1689 bpw  accuracy: 0.97037772\n",
            " -- 3.1705 bpw  accuracy: 0.97091379\n",
            " -- 4.0439 bpw  accuracy: 0.98404009\n",
            " -- 4.0471 bpw  accuracy: 0.98449196\n",
            " -- 4.0816 bpw  accuracy: 0.98549221\n",
            " -- 4.1381 bpw  accuracy: 0.98635581\n",
            " -- 4.1705 bpw  accuracy: 0.98520698\n",
            " -- 4.1902 bpw  accuracy: 0.98635811\n",
            " -- 4.2737 bpw  accuracy: 0.98803019\n",
            " -- 4.3295 bpw  accuracy: 0.98888233\n",
            " -- 5.2564 bpw  accuracy: 0.99355321\n",
            " -- 5.3295 bpw  accuracy: 0.99452824\n",
            " -- 6.0439 bpw  accuracy: 0.99583730\n",
            " -- 6.3381 bpw  accuracy: 0.99718879\n",
            " -- 8.0439 bpw  accuracy: 0.99888833\n",
            "-----------------------------------------------------\n",
            "| Measured: model.layers.7 (Attention)              |\n",
            "| Duration: 11.40 seconds                           |\n",
            "| Completed step: 15/67                             |\n",
            "| Avg time / step (rolling): 21.44 seconds          |\n",
            "| Estimated remaining time: 18min 34sec             |\n",
            "| Last checkpoint layer: model.layers.4 (Attention) |\n",
            "-----------------------------------------------------\n",
            " -- Layer: model.layers.7 (MLP)\n",
            " -- model.layers.7.mlp.gate_proj                       0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.7.mlp.gate_proj                       0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.7.mlp.gate_proj                       0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.7.mlp.gate_proj                       0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.7.mlp.gate_proj                       1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.7.mlp.gate_proj                       1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.7.mlp.gate_proj                       0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.7.mlp.gate_proj                       0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.7.mlp.gate_proj                       0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.7.mlp.gate_proj                       0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.7.mlp.gate_proj                       1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.7.mlp.gate_proj                       0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.7.mlp.gate_proj                       1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.7.mlp.up_proj                         0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.7.mlp.up_proj                         0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.7.mlp.up_proj                         0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.7.mlp.up_proj                         0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.7.mlp.up_proj                         0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.7.mlp.up_proj                         1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.7.mlp.up_proj                         0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.7.mlp.up_proj                         0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.7.mlp.up_proj                         0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.7.mlp.up_proj                         0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.7.mlp.up_proj                         1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.7.mlp.up_proj                         0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.7.mlp.up_proj                         1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.7.mlp.down_proj                       0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.7.mlp.down_proj                       0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.7.mlp.down_proj                       0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.7.mlp.down_proj                       0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.7.mlp.down_proj                       0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.7.mlp.down_proj                       0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.7.mlp.down_proj                       0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.7.mlp.down_proj                       0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.7.mlp.down_proj                       0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.7.mlp.down_proj                       0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.7.mlp.down_proj                       0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.7.mlp.down_proj                       0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.7.mlp.down_proj                       0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.7.mlp.down_proj                       1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.91538800\n",
            " -- 2.3230 bpw  accuracy: 0.91770028\n",
            " -- 2.5958 bpw  accuracy: 0.93156581\n",
            " -- 2.9120 bpw  accuracy: 0.93542700\n",
            " -- 3.2833 bpw  accuracy: 0.95742524\n",
            " -- 3.3655 bpw  accuracy: 0.96091332\n",
            " -- 3.6186 bpw  accuracy: 0.96642818\n",
            " -- 4.1368 bpw  accuracy: 0.97805451\n",
            " -- 4.1977 bpw  accuracy: 0.97989929\n",
            " -- 4.2662 bpw  accuracy: 0.97825496\n",
            " -- 4.3484 bpw  accuracy: 0.98097129\n",
            " -- 5.2491 bpw  accuracy: 0.98890000\n",
            " -- 5.3313 bpw  accuracy: 0.99050226\n",
            " -- 6.0713 bpw  accuracy: 0.99399297\n",
            " -- 6.3032 bpw  accuracy: 0.99432416\n",
            " -- 6.8687 bpw  accuracy: 0.99553247\n",
            " -- 8.0354 bpw  accuracy: 0.99841397\n",
            "-----------------------------------------------------\n",
            "| Measured: model.layers.7 (MLP)                    |\n",
            "| Duration: 31.55 seconds                           |\n",
            "| Completed step: 16/67                             |\n",
            "| Avg time / step (rolling): 21.44 seconds          |\n",
            "| Estimated remaining time: 18min 13sec             |\n",
            "| Last checkpoint layer: model.layers.4 (Attention) |\n",
            "-----------------------------------------------------\n",
            " -- Layer: model.layers.8 (Attention)\n",
            " -- model.layers.8.self_attn.q_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.8.self_attn.q_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.8.self_attn.q_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.8.self_attn.q_proj                    1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.8.self_attn.q_proj                    1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.8.self_attn.q_proj                    1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.8.self_attn.q_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.8.self_attn.q_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.8.self_attn.q_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.8.self_attn.q_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.8.self_attn.q_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.8.self_attn.q_proj                    1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.8.self_attn.q_proj                    1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.8.self_attn.q_proj                    1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.8.self_attn.k_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.8.self_attn.k_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.8.self_attn.k_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.8.self_attn.k_proj                    1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.8.self_attn.k_proj                    1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.8.self_attn.k_proj                    1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.8.self_attn.k_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.8.self_attn.k_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.8.self_attn.k_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.8.self_attn.k_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.8.self_attn.k_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.8.self_attn.k_proj                    1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.8.self_attn.k_proj                    1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.8.self_attn.k_proj                    1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.8.self_attn.v_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.8.self_attn.v_proj                    0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.8.self_attn.v_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.8.self_attn.v_proj                    0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.8.self_attn.v_proj                    1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.8.self_attn.v_proj                    1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.8.self_attn.v_proj                    1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.8.self_attn.v_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.8.self_attn.v_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.8.self_attn.v_proj                    1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.8.self_attn.v_proj                    1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.8.self_attn.v_proj                    1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.8.self_attn.v_proj                    1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.8.self_attn.v_proj                    1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.8.self_attn.v_proj                    1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.8.self_attn.o_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.8.self_attn.o_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.8.self_attn.o_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.8.self_attn.o_proj                    1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.8.self_attn.o_proj                    1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.8.self_attn.o_proj                    1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.8.self_attn.o_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.8.self_attn.o_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.8.self_attn.o_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.8.self_attn.o_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.8.self_attn.o_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.8.self_attn.o_proj                    1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.8.self_attn.o_proj                    1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.8.self_attn.o_proj                    1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.93431227\n",
            " -- 2.1987 bpw  accuracy: 0.93461525\n",
            " -- 2.2831 bpw  accuracy: 0.94329898\n",
            " -- 2.6768 bpw  accuracy: 0.95666480\n",
            " -- 3.1689 bpw  accuracy: 0.96672679\n",
            " -- 3.1705 bpw  accuracy: 0.96655341\n",
            " -- 4.0439 bpw  accuracy: 0.98154632\n",
            " -- 4.0471 bpw  accuracy: 0.98207025\n",
            " -- 4.0816 bpw  accuracy: 0.98320900\n",
            " -- 4.1381 bpw  accuracy: 0.98441708\n",
            " -- 4.1705 bpw  accuracy: 0.98337738\n",
            " -- 4.1902 bpw  accuracy: 0.98436730\n",
            " -- 4.2737 bpw  accuracy: 0.98624801\n",
            " -- 4.3295 bpw  accuracy: 0.98740933\n",
            " -- 5.2564 bpw  accuracy: 0.99256466\n",
            " -- 5.3295 bpw  accuracy: 0.99370778\n",
            " -- 6.0439 bpw  accuracy: 0.99514915\n",
            " -- 6.3381 bpw  accuracy: 0.99673639\n",
            " -- 8.0439 bpw  accuracy: 0.99874970\n",
            "-----------------------------------------------------\n",
            "| Measured: model.layers.8 (Attention)              |\n",
            "| Duration: 11.40 seconds                           |\n",
            "| Completed step: 17/67                             |\n",
            "| Avg time / step (rolling): 21.45 seconds          |\n",
            "| Estimated remaining time: 17min 52sec             |\n",
            "| Last checkpoint layer: model.layers.4 (Attention) |\n",
            "-----------------------------------------------------\n",
            " -- Layer: model.layers.8 (MLP)\n",
            " -- model.layers.8.mlp.gate_proj                       0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.8.mlp.gate_proj                       0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.8.mlp.gate_proj                       0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.8.mlp.gate_proj                       0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.8.mlp.gate_proj                       1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.8.mlp.gate_proj                       1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.8.mlp.gate_proj                       0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.8.mlp.gate_proj                       0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.8.mlp.gate_proj                       0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.8.mlp.gate_proj                       0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.8.mlp.gate_proj                       1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.8.mlp.gate_proj                       0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.8.mlp.gate_proj                       1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.8.mlp.up_proj                         0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.8.mlp.up_proj                         0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.8.mlp.up_proj                         0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.8.mlp.up_proj                         0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.8.mlp.up_proj                         0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.8.mlp.up_proj                         1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.8.mlp.up_proj                         0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.8.mlp.up_proj                         0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.8.mlp.up_proj                         0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.8.mlp.up_proj                         0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.8.mlp.up_proj                         1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.8.mlp.up_proj                         0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.8.mlp.up_proj                         1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.8.mlp.down_proj                       0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.8.mlp.down_proj                       0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.8.mlp.down_proj                       0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.8.mlp.down_proj                       0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.8.mlp.down_proj                       0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.8.mlp.down_proj                       0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.8.mlp.down_proj                       0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.8.mlp.down_proj                       0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.8.mlp.down_proj                       0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.8.mlp.down_proj                       0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.8.mlp.down_proj                       0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.8.mlp.down_proj                       0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.8.mlp.down_proj                       0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.8.mlp.down_proj                       1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.91269438\n",
            " -- 2.3230 bpw  accuracy: 0.91506013\n",
            " -- 2.5958 bpw  accuracy: 0.92888828\n",
            " -- 2.9120 bpw  accuracy: 0.93275113\n",
            " -- 3.2833 bpw  accuracy: 0.95589270\n",
            " -- 3.3655 bpw  accuracy: 0.95953925\n",
            " -- 3.6186 bpw  accuracy: 0.96510021\n",
            " -- 4.1368 bpw  accuracy: 0.97727029\n",
            " -- 4.1977 bpw  accuracy: 0.97915269\n",
            " -- 4.2662 bpw  accuracy: 0.97739245\n",
            " -- 4.3484 bpw  accuracy: 0.98026804\n",
            " -- 5.2491 bpw  accuracy: 0.98841898\n",
            " -- 5.3313 bpw  accuracy: 0.99014302\n",
            " -- 6.0713 bpw  accuracy: 0.99372654\n",
            " -- 6.3032 bpw  accuracy: 0.99405848\n",
            " -- 6.8687 bpw  accuracy: 0.99525831\n",
            " -- 8.0354 bpw  accuracy: 0.99833703\n",
            "-----------------------------------------------------\n",
            "| Measured: model.layers.8 (MLP)                    |\n",
            "| Duration: 31.60 seconds                           |\n",
            "| Completed step: 18/67                             |\n",
            "| Avg time / step (rolling): 21.46 seconds          |\n",
            "| Estimated remaining time: 17min 31sec             |\n",
            "| Last checkpoint layer: model.layers.4 (Attention) |\n",
            "-----------------------------------------------------\n",
            " -- Saving checkpoint...\n",
            " -- Layer: model.layers.9 (Attention)\n",
            " -- model.layers.9.self_attn.q_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.9.self_attn.q_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.9.self_attn.q_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.9.self_attn.q_proj                    1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.9.self_attn.q_proj                    1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.9.self_attn.q_proj                    1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.9.self_attn.q_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.9.self_attn.q_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.9.self_attn.q_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.9.self_attn.q_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.9.self_attn.q_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.9.self_attn.q_proj                    1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.9.self_attn.q_proj                    1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.9.self_attn.q_proj                    1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.9.self_attn.k_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.9.self_attn.k_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.9.self_attn.k_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.9.self_attn.k_proj                    1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.9.self_attn.k_proj                    1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.9.self_attn.k_proj                    1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.9.self_attn.k_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.9.self_attn.k_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.9.self_attn.k_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.9.self_attn.k_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.9.self_attn.k_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.9.self_attn.k_proj                    1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.9.self_attn.k_proj                    1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.9.self_attn.k_proj                    1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.9.self_attn.v_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.9.self_attn.v_proj                    0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.9.self_attn.v_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.9.self_attn.v_proj                    0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.9.self_attn.v_proj                    1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.9.self_attn.v_proj                    1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.9.self_attn.v_proj                    1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.9.self_attn.v_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.9.self_attn.v_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.9.self_attn.v_proj                    1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.9.self_attn.v_proj                    1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.9.self_attn.v_proj                    1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.9.self_attn.v_proj                    1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.9.self_attn.v_proj                    1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.9.self_attn.v_proj                    1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.9.self_attn.o_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.9.self_attn.o_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.9.self_attn.o_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.9.self_attn.o_proj                    1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.9.self_attn.o_proj                    1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.9.self_attn.o_proj                    1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.9.self_attn.o_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.9.self_attn.o_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.9.self_attn.o_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.9.self_attn.o_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.9.self_attn.o_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.9.self_attn.o_proj                    1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.9.self_attn.o_proj                    1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.9.self_attn.o_proj                    1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.92784348\n",
            " -- 2.1987 bpw  accuracy: 0.92879576\n",
            " -- 2.2831 bpw  accuracy: 0.93973438\n",
            " -- 2.6768 bpw  accuracy: 0.95277936\n",
            " -- 3.1689 bpw  accuracy: 0.96252969\n",
            " -- 3.1705 bpw  accuracy: 0.96292977\n",
            " -- 4.0439 bpw  accuracy: 0.97945479\n",
            " -- 4.0471 bpw  accuracy: 0.97989581\n",
            " -- 4.0816 bpw  accuracy: 0.98130098\n",
            " -- 4.1381 bpw  accuracy: 0.98224561\n",
            " -- 4.1705 bpw  accuracy: 0.98148095\n",
            " -- 4.1902 bpw  accuracy: 0.98248411\n",
            " -- 4.2737 bpw  accuracy: 0.98509027\n",
            " -- 4.3295 bpw  accuracy: 0.98642000\n",
            " -- 5.2564 bpw  accuracy: 0.99162589\n",
            " -- 5.3295 bpw  accuracy: 0.99315495\n",
            " -- 6.0439 bpw  accuracy: 0.99441598\n",
            " -- 6.3381 bpw  accuracy: 0.99656262\n",
            " -- 8.0439 bpw  accuracy: 0.99853794\n",
            "-----------------------------------------------\n",
            "| Measured: model.layers.9 (Attention)        |\n",
            "| Duration: 11.42 seconds                     |\n",
            "| Completed step: 19/67                       |\n",
            "| Avg time / step (rolling): 21.46 seconds    |\n",
            "| Estimated remaining time: 17min 10sec       |\n",
            "| Last checkpoint layer: model.layers.8 (MLP) |\n",
            "-----------------------------------------------\n",
            " -- Layer: model.layers.9 (MLP)\n",
            " -- model.layers.9.mlp.gate_proj                       0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.9.mlp.gate_proj                       0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.9.mlp.gate_proj                       0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.9.mlp.gate_proj                       0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.9.mlp.gate_proj                       1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.9.mlp.gate_proj                       1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.9.mlp.gate_proj                       0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.9.mlp.gate_proj                       0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.9.mlp.gate_proj                       0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.9.mlp.gate_proj                       0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.9.mlp.gate_proj                       1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.9.mlp.gate_proj                       0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.9.mlp.gate_proj                       1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.9.mlp.up_proj                         0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.9.mlp.up_proj                         0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.9.mlp.up_proj                         0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.9.mlp.up_proj                         0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.9.mlp.up_proj                         0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.9.mlp.up_proj                         1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.9.mlp.up_proj                         0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.9.mlp.up_proj                         0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.9.mlp.up_proj                         0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.9.mlp.up_proj                         0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.9.mlp.up_proj                         1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.9.mlp.up_proj                         0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.9.mlp.up_proj                         1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.9.mlp.down_proj                       0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.9.mlp.down_proj                       0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.9.mlp.down_proj                       0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.9.mlp.down_proj                       0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.9.mlp.down_proj                       0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.9.mlp.down_proj                       0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.9.mlp.down_proj                       0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.9.mlp.down_proj                       0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.9.mlp.down_proj                       0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.9.mlp.down_proj                       0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.9.mlp.down_proj                       0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.9.mlp.down_proj                       0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.9.mlp.down_proj                       0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.9.mlp.down_proj                       1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.91066384\n",
            " -- 2.3230 bpw  accuracy: 0.91313238\n",
            " -- 2.5958 bpw  accuracy: 0.92718720\n",
            " -- 2.9120 bpw  accuracy: 0.93111043\n",
            " -- 3.2833 bpw  accuracy: 0.95497565\n",
            " -- 3.3655 bpw  accuracy: 0.95866618\n",
            " -- 3.6186 bpw  accuracy: 0.96429581\n",
            " -- 4.1368 bpw  accuracy: 0.97678643\n",
            " -- 4.1977 bpw  accuracy: 0.97868750\n",
            " -- 4.2662 bpw  accuracy: 0.97698383\n",
            " -- 4.3484 bpw  accuracy: 0.97985245\n",
            " -- 5.2491 bpw  accuracy: 0.98823316\n",
            " -- 5.3313 bpw  accuracy: 0.98993579\n",
            " -- 6.0713 bpw  accuracy: 0.99362492\n",
            " -- 6.3032 bpw  accuracy: 0.99397510\n",
            " -- 6.8687 bpw  accuracy: 0.99519907\n",
            " -- 8.0354 bpw  accuracy: 0.99830450\n",
            "-----------------------------------------------\n",
            "| Measured: model.layers.9 (MLP)              |\n",
            "| Duration: 31.42 seconds                     |\n",
            "| Completed step: 20/67                       |\n",
            "| Avg time / step (rolling): 21.43 seconds    |\n",
            "| Estimated remaining time: 16min 47sec       |\n",
            "| Last checkpoint layer: model.layers.8 (MLP) |\n",
            "-----------------------------------------------\n",
            " -- Layer: model.layers.10 (Attention)\n",
            " -- model.layers.10.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.10.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.10.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.10.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.10.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.10.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.10.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.10.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.10.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.10.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.10.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.10.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.10.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.10.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.10.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.10.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.10.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.10.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.10.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.10.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.10.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.10.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.10.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.10.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.10.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.10.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.10.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.10.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.10.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.10.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.10.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.10.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.10.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.10.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.10.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.10.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.10.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.10.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.10.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.10.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.10.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.10.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.10.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.10.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.10.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.10.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.10.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.10.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.10.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.10.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.10.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.10.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.10.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.10.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.10.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.10.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.10.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.93084162\n",
            " -- 2.1987 bpw  accuracy: 0.93274751\n",
            " -- 2.2831 bpw  accuracy: 0.94037225\n",
            " -- 2.6768 bpw  accuracy: 0.95484854\n",
            " -- 3.1689 bpw  accuracy: 0.96454178\n",
            " -- 3.1705 bpw  accuracy: 0.96492089\n",
            " -- 4.0439 bpw  accuracy: 0.98002987\n",
            " -- 4.0471 bpw  accuracy: 0.98076265\n",
            " -- 4.0816 bpw  accuracy: 0.98204817\n",
            " -- 4.1381 bpw  accuracy: 0.98315209\n",
            " -- 4.1705 bpw  accuracy: 0.98235689\n",
            " -- 4.1902 bpw  accuracy: 0.98351971\n",
            " -- 4.2737 bpw  accuracy: 0.98537937\n",
            " -- 4.3295 bpw  accuracy: 0.98639170\n",
            " -- 5.2564 bpw  accuracy: 0.99201000\n",
            " -- 5.3295 bpw  accuracy: 0.99323388\n",
            " -- 6.0439 bpw  accuracy: 0.99484803\n",
            " -- 6.3381 bpw  accuracy: 0.99641165\n",
            " -- 8.0439 bpw  accuracy: 0.99861231\n",
            "-----------------------------------------------\n",
            "| Measured: model.layers.10 (Attention)       |\n",
            "| Duration: 11.26 seconds                     |\n",
            "| Completed step: 21/67                       |\n",
            "| Avg time / step (rolling): 21.44 seconds    |\n",
            "| Estimated remaining time: 16min 26sec       |\n",
            "| Last checkpoint layer: model.layers.8 (MLP) |\n",
            "-----------------------------------------------\n",
            " -- Layer: model.layers.10 (MLP)\n",
            " -- model.layers.10.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.10.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.10.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.10.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.10.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.10.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.10.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.10.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.10.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.10.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.10.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.10.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.10.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.10.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.10.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.10.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.10.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.10.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.10.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.10.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.10.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.10.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.10.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.10.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.10.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.10.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.10.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.10.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.10.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.10.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.10.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.10.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.10.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.10.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.10.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.10.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.10.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.10.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.10.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.10.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.90849285\n",
            " -- 2.3230 bpw  accuracy: 0.91103502\n",
            " -- 2.5958 bpw  accuracy: 0.92548325\n",
            " -- 2.9120 bpw  accuracy: 0.92953862\n",
            " -- 3.2833 bpw  accuracy: 0.95393056\n",
            " -- 3.3655 bpw  accuracy: 0.95764039\n",
            " -- 3.6186 bpw  accuracy: 0.96349862\n",
            " -- 4.1368 bpw  accuracy: 0.97632005\n",
            " -- 4.1977 bpw  accuracy: 0.97822617\n",
            " -- 4.2662 bpw  accuracy: 0.97645912\n",
            " -- 4.3484 bpw  accuracy: 0.97937922\n",
            " -- 5.2491 bpw  accuracy: 0.98798121\n",
            " -- 5.3313 bpw  accuracy: 0.98970504\n",
            " -- 6.0713 bpw  accuracy: 0.99350858\n",
            " -- 6.3032 bpw  accuracy: 0.99384363\n",
            " -- 6.8687 bpw  accuracy: 0.99509135\n",
            " -- 8.0354 bpw  accuracy: 0.99827643\n",
            "-----------------------------------------------\n",
            "| Measured: model.layers.10 (MLP)             |\n",
            "| Duration: 31.42 seconds                     |\n",
            "| Completed step: 22/67                       |\n",
            "| Avg time / step (rolling): 21.43 seconds    |\n",
            "| Estimated remaining time: 16min 4sec        |\n",
            "| Last checkpoint layer: model.layers.8 (MLP) |\n",
            "-----------------------------------------------\n",
            " -- Layer: model.layers.11 (Attention)\n",
            " -- model.layers.11.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.11.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.11.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.11.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.11.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.11.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.11.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.11.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.11.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.11.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.11.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.11.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.11.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.11.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.11.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.11.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.11.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.11.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.11.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.11.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.11.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.11.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.11.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.11.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.11.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.11.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.11.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.11.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.11.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.11.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.11.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.11.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.11.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.11.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.11.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.11.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.11.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.11.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.11.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.11.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.11.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.11.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.11.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.11.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.11.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.11.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.11.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.11.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.11.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.11.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.11.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.11.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.11.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.11.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.11.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.11.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.11.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.92910709\n",
            " -- 2.1987 bpw  accuracy: 0.93194419\n",
            " -- 2.2831 bpw  accuracy: 0.93942422\n",
            " -- 2.6768 bpw  accuracy: 0.95261595\n",
            " -- 3.1689 bpw  accuracy: 0.96440568\n",
            " -- 3.1705 bpw  accuracy: 0.96491177\n",
            " -- 4.0439 bpw  accuracy: 0.98055326\n",
            " -- 4.0471 bpw  accuracy: 0.98104859\n",
            " -- 4.0816 bpw  accuracy: 0.98231886\n",
            " -- 4.1381 bpw  accuracy: 0.98338200\n",
            " -- 4.1705 bpw  accuracy: 0.98257477\n",
            " -- 4.1902 bpw  accuracy: 0.98372625\n",
            " -- 4.2737 bpw  accuracy: 0.98546678\n",
            " -- 4.3295 bpw  accuracy: 0.98657820\n",
            " -- 5.2564 bpw  accuracy: 0.99207708\n",
            " -- 5.3295 bpw  accuracy: 0.99328489\n",
            " -- 6.0439 bpw  accuracy: 0.99494933\n",
            " -- 6.3381 bpw  accuracy: 0.99645033\n",
            " -- 8.0439 bpw  accuracy: 0.99866177\n",
            "-----------------------------------------------\n",
            "| Measured: model.layers.11 (Attention)       |\n",
            "| Duration: 11.15 seconds                     |\n",
            "| Completed step: 23/67                       |\n",
            "| Avg time / step (rolling): 21.42 seconds    |\n",
            "| Estimated remaining time: 15min 42sec       |\n",
            "| Last checkpoint layer: model.layers.8 (MLP) |\n",
            "-----------------------------------------------\n",
            " -- Layer: model.layers.11 (MLP)\n",
            " -- model.layers.11.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.11.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.11.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.11.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.11.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.11.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.11.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.11.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.11.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.11.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.11.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.11.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.11.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.11.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.11.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.11.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.11.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.11.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.11.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.11.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.11.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.11.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.11.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.11.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.11.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.11.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.11.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.11.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.11.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.11.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.11.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.11.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.11.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.11.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.11.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.11.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.11.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.11.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.11.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.11.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.90561591\n",
            " -- 2.3230 bpw  accuracy: 0.90828554\n",
            " -- 2.5958 bpw  accuracy: 0.92324370\n",
            " -- 2.9120 bpw  accuracy: 0.92749861\n",
            " -- 3.2833 bpw  accuracy: 0.95225925\n",
            " -- 3.3655 bpw  accuracy: 0.95620919\n",
            " -- 3.6186 bpw  accuracy: 0.96229432\n",
            " -- 4.1368 bpw  accuracy: 0.97532378\n",
            " -- 4.1977 bpw  accuracy: 0.97734066\n",
            " -- 4.2662 bpw  accuracy: 0.97552941\n",
            " -- 4.3484 bpw  accuracy: 0.97859777\n",
            " -- 5.2491 bpw  accuracy: 0.98748571\n",
            " -- 5.3313 bpw  accuracy: 0.98930096\n",
            " -- 6.0713 bpw  accuracy: 0.99320926\n",
            " -- 6.3032 bpw  accuracy: 0.99358594\n",
            " -- 6.8687 bpw  accuracy: 0.99491873\n",
            " -- 8.0354 bpw  accuracy: 0.99820709\n",
            "-----------------------------------------------\n",
            "| Measured: model.layers.11 (MLP)             |\n",
            "| Duration: 31.58 seconds                     |\n",
            "| Completed step: 24/67                       |\n",
            "| Avg time / step (rolling): 21.42 seconds    |\n",
            "| Estimated remaining time: 15min 21sec       |\n",
            "| Last checkpoint layer: model.layers.8 (MLP) |\n",
            "-----------------------------------------------\n",
            " -- Layer: model.layers.12 (Attention)\n",
            " -- model.layers.12.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.12.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.12.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.12.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.12.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.12.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.12.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.12.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.12.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.12.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.12.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.12.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.12.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.12.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.12.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.12.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.12.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.12.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.12.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.12.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.12.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.12.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.12.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.12.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.12.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.12.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.12.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.12.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.12.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.12.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.12.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.12.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.12.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.12.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.12.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.12.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.12.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.12.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.12.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.12.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.12.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.12.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.12.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.12.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.12.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.12.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.12.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.12.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.12.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.12.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.12.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.12.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.12.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.12.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.12.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.12.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.12.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.92480075\n",
            " -- 2.1987 bpw  accuracy: 0.92664126\n",
            " -- 2.2831 bpw  accuracy: 0.93732889\n",
            " -- 2.6768 bpw  accuracy: 0.95405952\n",
            " -- 3.1689 bpw  accuracy: 0.96250723\n",
            " -- 3.1705 bpw  accuracy: 0.96300404\n",
            " -- 4.0439 bpw  accuracy: 0.97906740\n",
            " -- 4.0471 bpw  accuracy: 0.97941478\n",
            " -- 4.0816 bpw  accuracy: 0.98076692\n",
            " -- 4.1381 bpw  accuracy: 0.98174791\n",
            " -- 4.1705 bpw  accuracy: 0.98105906\n",
            " -- 4.1902 bpw  accuracy: 0.98255470\n",
            " -- 4.2737 bpw  accuracy: 0.98455200\n",
            " -- 4.3295 bpw  accuracy: 0.98574420\n",
            " -- 5.2564 bpw  accuracy: 0.99170199\n",
            " -- 5.3295 bpw  accuracy: 0.99285880\n",
            " -- 6.0439 bpw  accuracy: 0.99449640\n",
            " -- 6.3381 bpw  accuracy: 0.99620264\n",
            " -- 8.0439 bpw  accuracy: 0.99854283\n",
            "-----------------------------------------------\n",
            "| Measured: model.layers.12 (Attention)       |\n",
            "| Duration: 11.23 seconds                     |\n",
            "| Completed step: 25/67                       |\n",
            "| Avg time / step (rolling): 21.40 seconds    |\n",
            "| Estimated remaining time: 14min 58sec       |\n",
            "| Last checkpoint layer: model.layers.8 (MLP) |\n",
            "-----------------------------------------------\n",
            " -- Layer: model.layers.12 (MLP)\n",
            " -- model.layers.12.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.12.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.12.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.12.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.12.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.12.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.12.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.12.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.12.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.12.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.12.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.12.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.12.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.12.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.12.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.12.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.12.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.12.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.12.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.12.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.12.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.12.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.12.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.12.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.12.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.12.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.12.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.12.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.12.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.12.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.12.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.12.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.12.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.12.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.12.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.12.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.12.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.12.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.12.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.12.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.90337749\n",
            " -- 2.3230 bpw  accuracy: 0.90614898\n",
            " -- 2.5958 bpw  accuracy: 0.92153312\n",
            " -- 2.9120 bpw  accuracy: 0.92592835\n",
            " -- 3.2833 bpw  accuracy: 0.95109844\n",
            " -- 3.3655 bpw  accuracy: 0.95517263\n",
            " -- 3.6186 bpw  accuracy: 0.96141263\n",
            " -- 4.1368 bpw  accuracy: 0.97456418\n",
            " -- 4.1977 bpw  accuracy: 0.97668502\n",
            " -- 4.2662 bpw  accuracy: 0.97495172\n",
            " -- 4.3484 bpw  accuracy: 0.97810819\n",
            " -- 5.2491 bpw  accuracy: 0.98717605\n",
            " -- 5.3313 bpw  accuracy: 0.98904832\n",
            " -- 6.0713 bpw  accuracy: 0.99297756\n",
            " -- 6.3032 bpw  accuracy: 0.99344473\n",
            " -- 6.8687 bpw  accuracy: 0.99484589\n",
            " -- 8.0354 bpw  accuracy: 0.99813756\n",
            "-----------------------------------------------\n",
            "| Measured: model.layers.12 (MLP)             |\n",
            "| Duration: 31.54 seconds                     |\n",
            "| Completed step: 26/67                       |\n",
            "| Avg time / step (rolling): 21.40 seconds    |\n",
            "| Estimated remaining time: 14min 37sec       |\n",
            "| Last checkpoint layer: model.layers.8 (MLP) |\n",
            "-----------------------------------------------\n",
            " -- Layer: model.layers.13 (Attention)\n",
            " -- model.layers.13.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.13.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.13.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.13.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.13.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.13.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.13.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.13.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.13.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.13.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.13.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.13.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.13.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.13.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.13.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.13.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.13.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.13.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.13.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.13.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.13.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.13.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.13.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.13.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.13.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.13.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.13.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.13.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.13.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.13.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.13.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.13.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.13.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.13.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.13.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.13.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.13.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.13.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.13.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.13.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.13.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.13.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.13.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.13.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.13.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.13.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.13.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.13.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.13.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.13.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.13.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.13.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.13.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.13.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.13.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.13.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.13.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.91994610\n",
            " -- 2.1987 bpw  accuracy: 0.92251152\n",
            " -- 2.2831 bpw  accuracy: 0.93159539\n",
            " -- 2.6768 bpw  accuracy: 0.94741425\n",
            " -- 3.1689 bpw  accuracy: 0.95971736\n",
            " -- 3.1705 bpw  accuracy: 0.96038645\n",
            " -- 4.0439 bpw  accuracy: 0.97784640\n",
            " -- 4.0471 bpw  accuracy: 0.97846402\n",
            " -- 4.0816 bpw  accuracy: 0.97995782\n",
            " -- 4.1381 bpw  accuracy: 0.98098407\n",
            " -- 4.1705 bpw  accuracy: 0.97999895\n",
            " -- 4.1902 bpw  accuracy: 0.98118604\n",
            " -- 4.2737 bpw  accuracy: 0.98341753\n",
            " -- 4.3295 bpw  accuracy: 0.98452526\n",
            " -- 5.2564 bpw  accuracy: 0.99095107\n",
            " -- 5.3295 bpw  accuracy: 0.99228262\n",
            " -- 6.0439 bpw  accuracy: 0.99422442\n",
            " -- 6.3381 bpw  accuracy: 0.99595013\n",
            " -- 8.0439 bpw  accuracy: 0.99846757\n",
            "-----------------------------------------------\n",
            "| Measured: model.layers.13 (Attention)       |\n",
            "| Duration: 11.37 seconds                     |\n",
            "| Completed step: 27/67                       |\n",
            "| Avg time / step (rolling): 21.40 seconds    |\n",
            "| Estimated remaining time: 14min 15sec       |\n",
            "| Last checkpoint layer: model.layers.8 (MLP) |\n",
            "-----------------------------------------------\n",
            " -- Saving checkpoint...\n",
            " -- Layer: model.layers.13 (MLP)\n",
            " -- model.layers.13.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.13.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.13.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.13.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.13.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.13.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.13.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.13.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.13.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.13.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.13.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.13.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.13.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.13.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.13.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.13.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.13.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.13.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.13.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.13.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.13.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.13.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.13.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.13.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.13.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.13.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.13.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.13.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.13.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.13.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.13.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.13.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.13.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.13.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.13.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.13.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.13.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.13.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.13.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.13.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.90162193\n",
            " -- 2.3230 bpw  accuracy: 0.90445747\n",
            " -- 2.5958 bpw  accuracy: 0.91996752\n",
            " -- 2.9120 bpw  accuracy: 0.92437757\n",
            " -- 3.2833 bpw  accuracy: 0.95040521\n",
            " -- 3.3655 bpw  accuracy: 0.95447193\n",
            " -- 3.6186 bpw  accuracy: 0.96070551\n",
            " -- 4.1368 bpw  accuracy: 0.97425877\n",
            " -- 4.1977 bpw  accuracy: 0.97638744\n",
            " -- 4.2662 bpw  accuracy: 0.97466776\n",
            " -- 4.3484 bpw  accuracy: 0.97783467\n",
            " -- 5.2491 bpw  accuracy: 0.98706648\n",
            " -- 5.3313 bpw  accuracy: 0.98893262\n",
            " -- 6.0713 bpw  accuracy: 0.99293190\n",
            " -- 6.3032 bpw  accuracy: 0.99340072\n",
            " -- 6.8687 bpw  accuracy: 0.99474873\n",
            " -- 8.0354 bpw  accuracy: 0.99812217\n",
            "------------------------------------------------------\n",
            "| Measured: model.layers.13 (MLP)                    |\n",
            "| Duration: 31.57 seconds                            |\n",
            "| Completed step: 28/67                              |\n",
            "| Avg time / step (rolling): 21.40 seconds           |\n",
            "| Estimated remaining time: 13min 54sec              |\n",
            "| Last checkpoint layer: model.layers.13 (Attention) |\n",
            "------------------------------------------------------\n",
            " -- Layer: model.layers.14 (Attention)\n",
            " -- model.layers.14.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.14.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.14.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.14.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.14.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.14.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.14.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.14.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.14.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.14.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.14.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.14.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.14.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.14.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.14.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.14.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.14.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.14.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.14.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.14.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.14.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.14.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.14.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.14.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.14.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.14.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.14.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.14.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.14.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.14.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.14.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.14.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.14.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.14.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.14.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.14.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.14.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.14.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.14.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.14.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.14.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.14.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.14.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.14.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.14.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.14.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.14.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.14.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.14.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.14.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.14.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.14.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.14.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.14.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.14.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.14.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.14.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.91996649\n",
            " -- 2.1987 bpw  accuracy: 0.92207050\n",
            " -- 2.2831 bpw  accuracy: 0.92954510\n",
            " -- 2.6768 bpw  accuracy: 0.94548721\n",
            " -- 3.1689 bpw  accuracy: 0.95962142\n",
            " -- 3.1705 bpw  accuracy: 0.96018060\n",
            " -- 4.0439 bpw  accuracy: 0.97706181\n",
            " -- 4.0471 bpw  accuracy: 0.97758801\n",
            " -- 4.0816 bpw  accuracy: 0.97901440\n",
            " -- 4.1381 bpw  accuracy: 0.98042608\n",
            " -- 4.1705 bpw  accuracy: 0.97980555\n",
            " -- 4.1902 bpw  accuracy: 0.98090227\n",
            " -- 4.2737 bpw  accuracy: 0.98260450\n",
            " -- 4.3295 bpw  accuracy: 0.98381312\n",
            " -- 5.2564 bpw  accuracy: 0.99061839\n",
            " -- 5.3295 bpw  accuracy: 0.99205655\n",
            " -- 6.0439 bpw  accuracy: 0.99403018\n",
            " -- 6.3381 bpw  accuracy: 0.99572320\n",
            " -- 8.0439 bpw  accuracy: 0.99839614\n",
            "------------------------------------------------------\n",
            "| Measured: model.layers.14 (Attention)              |\n",
            "| Duration: 11.22 seconds                            |\n",
            "| Completed step: 29/67                              |\n",
            "| Avg time / step (rolling): 21.38 seconds           |\n",
            "| Estimated remaining time: 13min 32sec              |\n",
            "| Last checkpoint layer: model.layers.13 (Attention) |\n",
            "------------------------------------------------------\n",
            " -- Layer: model.layers.14 (MLP)\n",
            " -- model.layers.14.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.14.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.14.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.14.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.14.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.14.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.14.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.14.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.14.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.14.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.14.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.14.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.14.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.14.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.14.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.14.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.14.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.14.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.14.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.14.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.14.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.14.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.14.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.14.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.14.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.14.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.14.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.14.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.14.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.14.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.14.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.14.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.14.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.14.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.14.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.14.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.14.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.14.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.14.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.14.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.89589746\n",
            " -- 2.3230 bpw  accuracy: 0.89898828\n",
            " -- 2.5958 bpw  accuracy: 0.91543493\n",
            " -- 2.9120 bpw  accuracy: 0.92026774\n",
            " -- 3.2833 bpw  accuracy: 0.94748539\n",
            " -- 3.3655 bpw  accuracy: 0.95184470\n",
            " -- 3.6186 bpw  accuracy: 0.95849872\n",
            " -- 4.1368 bpw  accuracy: 0.97261759\n",
            " -- 4.1977 bpw  accuracy: 0.97487775\n",
            " -- 4.2662 bpw  accuracy: 0.97318693\n",
            " -- 4.3484 bpw  accuracy: 0.97654389\n",
            " -- 5.2491 bpw  accuracy: 0.98630423\n",
            " -- 5.3313 bpw  accuracy: 0.98828727\n",
            " -- 6.0713 bpw  accuracy: 0.99246158\n",
            " -- 6.3032 bpw  accuracy: 0.99301745\n",
            " -- 6.8687 bpw  accuracy: 0.99446556\n",
            " -- 8.0354 bpw  accuracy: 0.99799754\n",
            "------------------------------------------------------\n",
            "| Measured: model.layers.14 (MLP)                    |\n",
            "| Duration: 31.54 seconds                            |\n",
            "| Completed step: 30/67                              |\n",
            "| Avg time / step (rolling): 21.39 seconds           |\n",
            "| Estimated remaining time: 13min 11sec              |\n",
            "| Last checkpoint layer: model.layers.13 (Attention) |\n",
            "------------------------------------------------------\n",
            " -- Layer: model.layers.15 (Attention)\n",
            " -- model.layers.15.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.15.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.15.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.15.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.15.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.15.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.15.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.15.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.15.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.15.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.15.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.15.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.15.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.15.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.15.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.15.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.15.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.15.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.15.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.15.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.15.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.15.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.15.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.15.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.15.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.15.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.15.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.15.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.15.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.15.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.15.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.15.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.15.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.15.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.15.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.15.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.15.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.15.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.15.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.15.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.15.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.15.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.15.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.15.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.15.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.15.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.15.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.15.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.15.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.15.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.15.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.15.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.15.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.15.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.15.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.15.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.15.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.91887628\n",
            " -- 2.1987 bpw  accuracy: 0.91911474\n",
            " -- 2.2831 bpw  accuracy: 0.93020404\n",
            " -- 2.6768 bpw  accuracy: 0.94669539\n",
            " -- 3.1689 bpw  accuracy: 0.95789047\n",
            " -- 3.1705 bpw  accuracy: 0.95834875\n",
            " -- 4.0439 bpw  accuracy: 0.97633283\n",
            " -- 4.0471 bpw  accuracy: 0.97747824\n",
            " -- 4.0816 bpw  accuracy: 0.97910011\n",
            " -- 4.1381 bpw  accuracy: 0.98025122\n",
            " -- 4.1705 bpw  accuracy: 0.97913579\n",
            " -- 4.1902 bpw  accuracy: 0.98049610\n",
            " -- 4.2737 bpw  accuracy: 0.98274661\n",
            " -- 4.3295 bpw  accuracy: 0.98420786\n",
            " -- 5.2564 bpw  accuracy: 0.99048791\n",
            " -- 5.3295 bpw  accuracy: 0.99201569\n",
            " -- 6.0439 bpw  accuracy: 0.99384819\n",
            " -- 6.3381 bpw  accuracy: 0.99595172\n",
            " -- 8.0439 bpw  accuracy: 0.99838593\n",
            "------------------------------------------------------\n",
            "| Measured: model.layers.15 (Attention)              |\n",
            "| Duration: 11.12 seconds                            |\n",
            "| Completed step: 31/67                              |\n",
            "| Avg time / step (rolling): 21.37 seconds           |\n",
            "| Estimated remaining time: 12min 49sec              |\n",
            "| Last checkpoint layer: model.layers.13 (Attention) |\n",
            "------------------------------------------------------\n",
            " -- Layer: model.layers.15 (MLP)\n",
            " -- model.layers.15.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.15.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.15.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.15.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.15.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.15.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.15.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.15.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.15.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.15.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.15.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.15.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.15.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.15.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.15.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.15.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.15.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.15.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.15.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.15.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.15.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.15.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.15.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.15.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.15.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.15.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.15.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.15.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.15.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.15.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.15.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.15.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.15.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.15.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.15.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.15.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.15.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.15.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.15.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.15.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.89294648\n",
            " -- 2.3230 bpw  accuracy: 0.89615224\n",
            " -- 2.5958 bpw  accuracy: 0.91316538\n",
            " -- 2.9120 bpw  accuracy: 0.91801679\n",
            " -- 3.2833 bpw  accuracy: 0.94642122\n",
            " -- 3.3655 bpw  accuracy: 0.95072825\n",
            " -- 3.6186 bpw  accuracy: 0.95746025\n",
            " -- 4.1368 bpw  accuracy: 0.97216881\n",
            " -- 4.1977 bpw  accuracy: 0.97438321\n",
            " -- 4.2662 bpw  accuracy: 0.97265895\n",
            " -- 4.3484 bpw  accuracy: 0.97603733\n",
            " -- 5.2491 bpw  accuracy: 0.98606593\n",
            " -- 5.3313 bpw  accuracy: 0.98804928\n",
            " -- 6.0713 bpw  accuracy: 0.99236285\n",
            " -- 6.3032 bpw  accuracy: 0.99288932\n",
            " -- 6.8687 bpw  accuracy: 0.99432611\n",
            " -- 8.0354 bpw  accuracy: 0.99794467\n",
            "------------------------------------------------------\n",
            "| Measured: model.layers.15 (MLP)                    |\n",
            "| Duration: 31.65 seconds                            |\n",
            "| Completed step: 32/67                              |\n",
            "| Avg time / step (rolling): 21.40 seconds           |\n",
            "| Estimated remaining time: 12min 28sec              |\n",
            "| Last checkpoint layer: model.layers.13 (Attention) |\n",
            "------------------------------------------------------\n",
            " -- Layer: model.layers.16 (Attention)\n",
            " -- model.layers.16.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.16.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.16.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.16.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.16.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.16.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.16.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.16.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.16.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.16.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.16.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.16.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.16.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.16.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.16.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.16.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.16.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.16.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.16.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.16.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.16.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.16.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.16.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.16.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.16.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.16.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.16.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.16.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.16.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.16.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.16.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.16.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.16.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.16.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.16.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.16.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.16.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.16.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.16.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.16.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.16.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.16.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.16.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.16.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.16.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.16.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.16.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.16.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.16.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.16.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.16.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.16.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.16.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.16.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.16.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.16.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.16.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.92923552\n",
            " -- 2.1987 bpw  accuracy: 0.93238343\n",
            " -- 2.2831 bpw  accuracy: 0.94204008\n",
            " -- 2.6768 bpw  accuracy: 0.95623208\n",
            " -- 3.1689 bpw  accuracy: 0.96443006\n",
            " -- 3.1705 bpw  accuracy: 0.96513445\n",
            " -- 4.0439 bpw  accuracy: 0.97877325\n",
            " -- 4.0471 bpw  accuracy: 0.97991661\n",
            " -- 4.0816 bpw  accuracy: 0.98149752\n",
            " -- 4.1381 bpw  accuracy: 0.98244348\n",
            " -- 4.1705 bpw  accuracy: 0.98219745\n",
            " -- 4.1902 bpw  accuracy: 0.98318359\n",
            " -- 4.2737 bpw  accuracy: 0.98538749\n",
            " -- 4.3295 bpw  accuracy: 0.98642707\n",
            " -- 5.2564 bpw  accuracy: 0.99201228\n",
            " -- 5.3295 bpw  accuracy: 0.99320153\n",
            " -- 6.0439 bpw  accuracy: 0.99447767\n",
            " -- 6.3381 bpw  accuracy: 0.99636530\n",
            " -- 8.0439 bpw  accuracy: 0.99854191\n",
            "------------------------------------------------------\n",
            "| Measured: model.layers.16 (Attention)              |\n",
            "| Duration: 11.31 seconds                            |\n",
            "| Completed step: 33/67                              |\n",
            "| Avg time / step (rolling): 21.41 seconds           |\n",
            "| Estimated remaining time: 12min 8sec               |\n",
            "| Last checkpoint layer: model.layers.13 (Attention) |\n",
            "------------------------------------------------------\n",
            " -- Layer: model.layers.16 (MLP)\n",
            " -- model.layers.16.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.16.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.16.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.16.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.16.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.16.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.16.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.16.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.16.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.16.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.16.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.16.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.16.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.16.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.16.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.16.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.16.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.16.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.16.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.16.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.16.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.16.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.16.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.16.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.16.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.16.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.16.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.16.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.16.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.16.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.16.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.16.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.16.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.16.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.16.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.16.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.16.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.16.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.16.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.16.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.89604890\n",
            " -- 2.3230 bpw  accuracy: 0.89902119\n",
            " -- 2.5958 bpw  accuracy: 0.91558176\n",
            " -- 2.9120 bpw  accuracy: 0.92038651\n",
            " -- 3.2833 bpw  accuracy: 0.94780222\n",
            " -- 3.3655 bpw  accuracy: 0.95206684\n",
            " -- 3.6186 bpw  accuracy: 0.95870009\n",
            " -- 4.1368 bpw  accuracy: 0.97293684\n",
            " -- 4.1977 bpw  accuracy: 0.97517043\n",
            " -- 4.2662 bpw  accuracy: 0.97336787\n",
            " -- 4.3484 bpw  accuracy: 0.97670107\n",
            " -- 5.2491 bpw  accuracy: 0.98640501\n",
            " -- 5.3313 bpw  accuracy: 0.98837320\n",
            " -- 6.0713 bpw  accuracy: 0.99256783\n",
            " -- 6.3032 bpw  accuracy: 0.99306136\n",
            " -- 6.8687 bpw  accuracy: 0.99446413\n",
            " -- 8.0354 bpw  accuracy: 0.99800144\n",
            "------------------------------------------------------\n",
            "| Measured: model.layers.16 (MLP)                    |\n",
            "| Duration: 31.56 seconds                            |\n",
            "| Completed step: 34/67                              |\n",
            "| Avg time / step (rolling): 21.41 seconds           |\n",
            "| Estimated remaining time: 11min 46sec              |\n",
            "| Last checkpoint layer: model.layers.13 (Attention) |\n",
            "------------------------------------------------------\n",
            " -- Layer: model.layers.17 (Attention)\n",
            " -- model.layers.17.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.17.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.17.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.17.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.17.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.17.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.17.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.17.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.17.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.17.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.17.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.17.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.17.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.17.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.17.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.17.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.17.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.17.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.17.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.17.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.17.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.17.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.17.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.17.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.17.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.17.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.17.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.17.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.17.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.17.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.17.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.17.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.17.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.17.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.17.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.17.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.17.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.17.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.17.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.17.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.17.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.17.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.17.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.17.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.17.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.17.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.17.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.17.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.17.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.17.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.17.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.17.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.17.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.17.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.17.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.17.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.17.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.92889529\n",
            " -- 2.1987 bpw  accuracy: 0.93216293\n",
            " -- 2.2831 bpw  accuracy: 0.94562823\n",
            " -- 2.6768 bpw  accuracy: 0.95739217\n",
            " -- 3.1689 bpw  accuracy: 0.96599029\n",
            " -- 3.1705 bpw  accuracy: 0.96564958\n",
            " -- 4.0439 bpw  accuracy: 0.98077963\n",
            " -- 4.0471 bpw  accuracy: 0.98107864\n",
            " -- 4.0816 bpw  accuracy: 0.98262387\n",
            " -- 4.1381 bpw  accuracy: 0.98342510\n",
            " -- 4.1705 bpw  accuracy: 0.98263062\n",
            " -- 4.1902 bpw  accuracy: 0.98373292\n",
            " -- 4.2737 bpw  accuracy: 0.98642544\n",
            " -- 4.3295 bpw  accuracy: 0.98759280\n",
            " -- 5.2564 bpw  accuracy: 0.99256071\n",
            " -- 5.3295 bpw  accuracy: 0.99379639\n",
            " -- 6.0439 bpw  accuracy: 0.99494069\n",
            " -- 6.3381 bpw  accuracy: 0.99683122\n",
            " -- 8.0439 bpw  accuracy: 0.99863664\n",
            "------------------------------------------------------\n",
            "| Measured: model.layers.17 (Attention)              |\n",
            "| Duration: 11.24 seconds                            |\n",
            "| Completed step: 35/67                              |\n",
            "| Avg time / step (rolling): 21.41 seconds           |\n",
            "| Estimated remaining time: 11min 25sec              |\n",
            "| Last checkpoint layer: model.layers.13 (Attention) |\n",
            "------------------------------------------------------\n",
            " -- Layer: model.layers.17 (MLP)\n",
            " -- model.layers.17.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.17.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.17.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.17.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.17.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.17.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.17.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.17.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.17.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.17.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.17.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.17.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.17.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.17.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.17.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.17.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.17.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.17.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.17.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.17.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.17.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.17.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.17.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.17.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.17.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.17.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.17.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.17.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.17.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.17.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.17.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.17.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.17.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.17.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.17.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.17.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.17.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.17.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.17.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.17.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.89810377\n",
            " -- 2.3230 bpw  accuracy: 0.90102265\n",
            " -- 2.5958 bpw  accuracy: 0.91727484\n",
            " -- 2.9120 bpw  accuracy: 0.92197914\n",
            " -- 3.2833 bpw  accuracy: 0.94898923\n",
            " -- 3.3655 bpw  accuracy: 0.95310385\n",
            " -- 3.6186 bpw  accuracy: 0.95962537\n",
            " -- 4.1368 bpw  accuracy: 0.97347488\n",
            " -- 4.1977 bpw  accuracy: 0.97566188\n",
            " -- 4.2662 bpw  accuracy: 0.97398367\n",
            " -- 4.3484 bpw  accuracy: 0.97721457\n",
            " -- 5.2491 bpw  accuracy: 0.98671974\n",
            " -- 5.3313 bpw  accuracy: 0.98862852\n",
            " -- 6.0713 bpw  accuracy: 0.99272597\n",
            " -- 6.3032 bpw  accuracy: 0.99322702\n",
            " -- 6.8687 bpw  accuracy: 0.99460366\n",
            " -- 8.0354 bpw  accuracy: 0.99803504\n",
            "------------------------------------------------------\n",
            "| Measured: model.layers.17 (MLP)                    |\n",
            "| Duration: 31.45 seconds                            |\n",
            "| Completed step: 36/67                              |\n",
            "| Avg time / step (rolling): 21.40 seconds           |\n",
            "| Estimated remaining time: 11min 3sec               |\n",
            "| Last checkpoint layer: model.layers.13 (Attention) |\n",
            "------------------------------------------------------\n",
            " -- Saving checkpoint...\n",
            " -- Layer: model.layers.18 (Attention)\n",
            " -- model.layers.18.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.18.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.18.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.18.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.18.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.18.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.18.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.18.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.18.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.18.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.18.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.18.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.18.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.18.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.18.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.18.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.18.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.18.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.18.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.18.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.18.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.18.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.18.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.18.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.18.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.18.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.18.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.18.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.18.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.18.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.18.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.18.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.18.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.18.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.18.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.18.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.18.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.18.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.18.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.18.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.18.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.18.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.18.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.18.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.18.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.18.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.18.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.18.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.18.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.18.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.18.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.18.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.18.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.18.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.18.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.18.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.18.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.94362005\n",
            " -- 2.1987 bpw  accuracy: 0.94412075\n",
            " -- 2.2831 bpw  accuracy: 0.95475247\n",
            " -- 2.6768 bpw  accuracy: 0.96336450\n",
            " -- 3.1689 bpw  accuracy: 0.97132821\n",
            " -- 3.1705 bpw  accuracy: 0.97271227\n",
            " -- 4.0439 bpw  accuracy: 0.98252212\n",
            " -- 4.0471 bpw  accuracy: 0.98489573\n",
            " -- 4.0816 bpw  accuracy: 0.98528482\n",
            " -- 4.1381 bpw  accuracy: 0.98585145\n",
            " -- 4.1705 bpw  accuracy: 0.98602431\n",
            " -- 4.1902 bpw  accuracy: 0.98701559\n",
            " -- 4.2737 bpw  accuracy: 0.98893338\n",
            " -- 4.3295 bpw  accuracy: 0.98997384\n",
            " -- 5.2564 bpw  accuracy: 0.99376281\n",
            " -- 5.3295 bpw  accuracy: 0.99493448\n",
            " -- 6.0439 bpw  accuracy: 0.99541730\n",
            " -- 6.3381 bpw  accuracy: 0.99742426\n",
            " -- 8.0439 bpw  accuracy: 0.99881374\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.18 (Attention)        |\n",
            "| Duration: 11.26 seconds                      |\n",
            "| Completed step: 37/67                        |\n",
            "| Avg time / step (rolling): 21.39 seconds     |\n",
            "| Estimated remaining time: 10min 41sec        |\n",
            "| Last checkpoint layer: model.layers.17 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.18 (MLP)\n",
            " -- model.layers.18.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.18.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.18.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.18.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.18.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.18.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.18.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.18.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.18.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.18.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.18.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.18.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.18.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.18.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.18.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.18.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.18.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.18.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.18.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.18.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.18.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.18.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.18.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.18.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.18.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.18.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.18.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.18.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.18.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.18.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.18.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.18.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.18.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.18.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.18.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.18.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.18.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.18.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.18.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.18.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.90333419\n",
            " -- 2.3230 bpw  accuracy: 0.90595301\n",
            " -- 2.5958 bpw  accuracy: 0.92105982\n",
            " -- 2.9120 bpw  accuracy: 0.92544246\n",
            " -- 3.2833 bpw  accuracy: 0.95154686\n",
            " -- 3.3655 bpw  accuracy: 0.95538338\n",
            " -- 3.6186 bpw  accuracy: 0.96151613\n",
            " -- 4.1368 bpw  accuracy: 0.97489651\n",
            " -- 4.1977 bpw  accuracy: 0.97696934\n",
            " -- 4.2662 bpw  accuracy: 0.97530645\n",
            " -- 4.3484 bpw  accuracy: 0.97832392\n",
            " -- 5.2491 bpw  accuracy: 0.98740267\n",
            " -- 5.3313 bpw  accuracy: 0.98919012\n",
            " -- 6.0713 bpw  accuracy: 0.99314146\n",
            " -- 6.3032 bpw  accuracy: 0.99357685\n",
            " -- 6.8687 bpw  accuracy: 0.99486574\n",
            " -- 8.0354 bpw  accuracy: 0.99815319\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.18 (MLP)              |\n",
            "| Duration: 31.48 seconds                      |\n",
            "| Completed step: 38/67                        |\n",
            "| Avg time / step (rolling): 21.38 seconds     |\n",
            "| Estimated remaining time: 10min 20sec        |\n",
            "| Last checkpoint layer: model.layers.17 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.19 (Attention)\n",
            " -- model.layers.19.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.19.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.19.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.19.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.19.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.19.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.19.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.19.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.19.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.19.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.19.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.19.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.19.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.19.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.19.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.19.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.19.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.19.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.19.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.19.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.19.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.19.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.19.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.19.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.19.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.19.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.19.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.19.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.19.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.19.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.19.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.19.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.19.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.19.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.19.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.19.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.19.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.19.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.19.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.19.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.19.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.19.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.19.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.19.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.19.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.19.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.19.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.19.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.19.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.19.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.19.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.19.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.19.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.19.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.19.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.19.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.19.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.94043825\n",
            " -- 2.1987 bpw  accuracy: 0.94065509\n",
            " -- 2.2831 bpw  accuracy: 0.95822816\n",
            " -- 2.6768 bpw  accuracy: 0.96435566\n",
            " -- 3.1689 bpw  accuracy: 0.96915632\n",
            " -- 3.1705 bpw  accuracy: 0.97118984\n",
            " -- 4.0439 bpw  accuracy: 0.98172525\n",
            " -- 4.0471 bpw  accuracy: 0.98294568\n",
            " -- 4.0816 bpw  accuracy: 0.98462417\n",
            " -- 4.1381 bpw  accuracy: 0.98495234\n",
            " -- 4.1705 bpw  accuracy: 0.98473104\n",
            " -- 4.1902 bpw  accuracy: 0.98623300\n",
            " -- 4.2737 bpw  accuracy: 0.98964067\n",
            " -- 4.3295 bpw  accuracy: 0.99022512\n",
            " -- 5.2564 bpw  accuracy: 0.99411642\n",
            " -- 5.3295 bpw  accuracy: 0.99521870\n",
            " -- 6.0439 bpw  accuracy: 0.99532230\n",
            " -- 6.3381 bpw  accuracy: 0.99772359\n",
            " -- 8.0439 bpw  accuracy: 0.99874877\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.19 (Attention)        |\n",
            "| Duration: 11.12 seconds                      |\n",
            "| Completed step: 39/67                        |\n",
            "| Avg time / step (rolling): 21.37 seconds     |\n",
            "| Estimated remaining time: 9min 58sec         |\n",
            "| Last checkpoint layer: model.layers.17 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.19 (MLP)\n",
            " -- model.layers.19.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.19.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.19.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.19.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.19.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.19.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.19.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.19.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.19.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.19.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.19.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.19.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.19.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.19.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.19.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.19.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.19.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.19.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.19.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.19.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.19.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.19.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.19.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.19.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.19.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.19.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.19.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.19.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.19.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.19.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.19.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.19.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.19.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.19.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.19.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.19.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.19.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.19.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.19.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.19.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.90524807\n",
            " -- 2.3230 bpw  accuracy: 0.90781958\n",
            " -- 2.5958 bpw  accuracy: 0.92221823\n",
            " -- 2.9120 bpw  accuracy: 0.92643214\n",
            " -- 3.2833 bpw  accuracy: 0.95244255\n",
            " -- 3.3655 bpw  accuracy: 0.95621434\n",
            " -- 3.6186 bpw  accuracy: 0.96213241\n",
            " -- 4.1368 bpw  accuracy: 0.97539868\n",
            " -- 4.1977 bpw  accuracy: 0.97741549\n",
            " -- 4.2662 bpw  accuracy: 0.97578208\n",
            " -- 4.3484 bpw  accuracy: 0.97877339\n",
            " -- 5.2491 bpw  accuracy: 0.98766220\n",
            " -- 5.3313 bpw  accuracy: 0.98940781\n",
            " -- 6.0713 bpw  accuracy: 0.99326428\n",
            " -- 6.3032 bpw  accuracy: 0.99372308\n",
            " -- 6.8687 bpw  accuracy: 0.99491261\n",
            " -- 8.0354 bpw  accuracy: 0.99818852\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.19 (MLP)              |\n",
            "| Duration: 31.40 seconds                      |\n",
            "| Completed step: 40/67                        |\n",
            "| Avg time / step (rolling): 21.36 seconds     |\n",
            "| Estimated remaining time: 9min 36sec         |\n",
            "| Last checkpoint layer: model.layers.17 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.20 (Attention)\n",
            " -- model.layers.20.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.20.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.20.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.20.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.20.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.20.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.20.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.20.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.20.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.20.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.20.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.20.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.20.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.20.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.20.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.20.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.20.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.20.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.20.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.20.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.20.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.20.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.20.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.20.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.20.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.20.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.20.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.20.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.20.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.20.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.20.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.20.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.20.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.20.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.20.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.20.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.20.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.20.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.20.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.20.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.20.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.20.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.20.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.20.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.20.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.20.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.20.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.20.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.20.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.20.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.20.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.20.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.20.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.20.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.20.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.20.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.20.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.94008072\n",
            " -- 2.1987 bpw  accuracy: 0.94385044\n",
            " -- 2.2831 bpw  accuracy: 0.95926914\n",
            " -- 2.6768 bpw  accuracy: 0.96653859\n",
            " -- 3.1689 bpw  accuracy: 0.96993048\n",
            " -- 3.1705 bpw  accuracy: 0.97296935\n",
            " -- 4.0439 bpw  accuracy: 0.98279354\n",
            " -- 4.0471 bpw  accuracy: 0.98368159\n",
            " -- 4.0816 bpw  accuracy: 0.98526551\n",
            " -- 4.1381 bpw  accuracy: 0.98564959\n",
            " -- 4.1705 bpw  accuracy: 0.98564252\n",
            " -- 4.1902 bpw  accuracy: 0.98674724\n",
            " -- 4.2737 bpw  accuracy: 0.98964029\n",
            " -- 4.3295 bpw  accuracy: 0.99065160\n",
            " -- 5.2564 bpw  accuracy: 0.99443335\n",
            " -- 5.3295 bpw  accuracy: 0.99523933\n",
            " -- 6.0439 bpw  accuracy: 0.99569369\n",
            " -- 6.3381 bpw  accuracy: 0.99762937\n",
            " -- 8.0439 bpw  accuracy: 0.99883261\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.20 (Attention)        |\n",
            "| Duration: 11.20 seconds                      |\n",
            "| Completed step: 41/67                        |\n",
            "| Avg time / step (rolling): 21.37 seconds     |\n",
            "| Estimated remaining time: 9min 15sec         |\n",
            "| Last checkpoint layer: model.layers.17 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.20 (MLP)\n",
            " -- model.layers.20.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.20.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.20.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.20.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.20.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.20.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.20.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.20.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.20.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.20.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.20.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.20.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.20.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.20.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.20.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.20.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.20.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.20.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.20.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.20.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.20.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.20.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.20.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.20.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.20.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.20.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.20.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.20.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.20.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.20.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.20.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.20.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.20.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.20.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.20.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.20.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.20.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.20.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.20.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.20.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.90554244\n",
            " -- 2.3230 bpw  accuracy: 0.90801356\n",
            " -- 2.5958 bpw  accuracy: 0.92222883\n",
            " -- 2.9120 bpw  accuracy: 0.92644545\n",
            " -- 3.2833 bpw  accuracy: 0.95252003\n",
            " -- 3.3655 bpw  accuracy: 0.95622659\n",
            " -- 3.6186 bpw  accuracy: 0.96211782\n",
            " -- 4.1368 bpw  accuracy: 0.97540841\n",
            " -- 4.1977 bpw  accuracy: 0.97744594\n",
            " -- 4.2662 bpw  accuracy: 0.97583487\n",
            " -- 4.3484 bpw  accuracy: 0.97878825\n",
            " -- 5.2491 bpw  accuracy: 0.98770445\n",
            " -- 5.3313 bpw  accuracy: 0.98943451\n",
            " -- 6.0713 bpw  accuracy: 0.99331776\n",
            " -- 6.3032 bpw  accuracy: 0.99374212\n",
            " -- 6.8687 bpw  accuracy: 0.99493144\n",
            " -- 8.0354 bpw  accuracy: 0.99820994\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.20 (MLP)              |\n",
            "| Duration: 32.50 seconds                      |\n",
            "| Completed step: 42/67                        |\n",
            "| Avg time / step (rolling): 21.45 seconds     |\n",
            "| Estimated remaining time: 8min 56sec         |\n",
            "| Last checkpoint layer: model.layers.17 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.21 (Attention)\n",
            " -- model.layers.21.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.21.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.21.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.21.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.21.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.21.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.21.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.21.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.21.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.21.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.21.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.21.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.21.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.21.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.21.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.21.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.21.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.21.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.21.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.21.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.21.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.21.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.21.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.21.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.21.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.21.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.21.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.21.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.21.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.21.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.21.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.21.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.21.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.21.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.21.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.21.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.21.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.21.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.21.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.21.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.21.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.21.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.21.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.21.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.21.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.21.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.21.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.21.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.21.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.21.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.21.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.21.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.21.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.21.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.21.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.21.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.21.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.93891547\n",
            " -- 2.1987 bpw  accuracy: 0.94220700\n",
            " -- 2.2831 bpw  accuracy: 0.95503103\n",
            " -- 2.6768 bpw  accuracy: 0.96275628\n",
            " -- 3.1689 bpw  accuracy: 0.97091591\n",
            " -- 3.1705 bpw  accuracy: 0.97104236\n",
            " -- 4.0439 bpw  accuracy: 0.98288625\n",
            " -- 4.0471 bpw  accuracy: 0.98384500\n",
            " -- 4.0816 bpw  accuracy: 0.98468287\n",
            " -- 4.1381 bpw  accuracy: 0.98545510\n",
            " -- 4.1705 bpw  accuracy: 0.98550257\n",
            " -- 4.1902 bpw  accuracy: 0.98665865\n",
            " -- 4.2737 bpw  accuracy: 0.98918589\n",
            " -- 4.3295 bpw  accuracy: 0.98997149\n",
            " -- 5.2564 bpw  accuracy: 0.99393381\n",
            " -- 5.3295 bpw  accuracy: 0.99502383\n",
            " -- 6.0439 bpw  accuracy: 0.99544389\n",
            " -- 6.3381 bpw  accuracy: 0.99742731\n",
            " -- 8.0439 bpw  accuracy: 0.99878930\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.21 (Attention)        |\n",
            "| Duration: 11.55 seconds                      |\n",
            "| Completed step: 43/67                        |\n",
            "| Avg time / step (rolling): 21.48 seconds     |\n",
            "| Estimated remaining time: 8min 35sec         |\n",
            "| Last checkpoint layer: model.layers.17 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.21 (MLP)\n",
            " -- model.layers.21.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.21.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.21.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.21.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.21.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.21.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.21.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.21.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.21.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.21.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.21.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.21.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.21.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.21.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.21.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.21.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.21.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.21.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.21.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.21.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.21.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.21.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.21.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.21.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.21.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.21.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.21.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.21.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.21.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.21.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.21.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.21.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.21.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.21.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.21.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.21.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.21.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.21.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.21.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.21.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.90896790\n",
            " -- 2.3230 bpw  accuracy: 0.91141610\n",
            " -- 2.5958 bpw  accuracy: 0.92470469\n",
            " -- 2.9120 bpw  accuracy: 0.92864341\n",
            " -- 3.2833 bpw  accuracy: 0.95423617\n",
            " -- 3.3655 bpw  accuracy: 0.95786692\n",
            " -- 3.6186 bpw  accuracy: 0.96334343\n",
            " -- 4.1368 bpw  accuracy: 0.97620006\n",
            " -- 4.1977 bpw  accuracy: 0.97820113\n",
            " -- 4.2662 bpw  accuracy: 0.97669875\n",
            " -- 4.3484 bpw  accuracy: 0.97956180\n",
            " -- 5.2491 bpw  accuracy: 0.98813084\n",
            " -- 5.3313 bpw  accuracy: 0.98980183\n",
            " -- 6.0713 bpw  accuracy: 0.99348814\n",
            " -- 6.3032 bpw  accuracy: 0.99396510\n",
            " -- 6.8687 bpw  accuracy: 0.99506962\n",
            " -- 8.0354 bpw  accuracy: 0.99822976\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.21 (MLP)              |\n",
            "| Duration: 33.08 seconds                      |\n",
            "| Completed step: 44/67                        |\n",
            "| Avg time / step (rolling): 21.63 seconds     |\n",
            "| Estimated remaining time: 8min 17sec         |\n",
            "| Last checkpoint layer: model.layers.17 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.22 (Attention)\n",
            " -- model.layers.22.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.22.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.22.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.22.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.22.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.22.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.22.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.22.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.22.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.22.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.22.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.22.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.22.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.22.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.22.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.22.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.22.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.22.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.22.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.22.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.22.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.22.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.22.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.22.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.22.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.22.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.22.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.22.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.22.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.22.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.22.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.22.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.22.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.22.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.22.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.22.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.22.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.22.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.22.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.22.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.22.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.22.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.22.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.22.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.22.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.22.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.22.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.22.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.22.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.22.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.22.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.22.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.22.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.22.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.22.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.22.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.22.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.94559620\n",
            " -- 2.1987 bpw  accuracy: 0.94660355\n",
            " -- 2.2831 bpw  accuracy: 0.96114398\n",
            " -- 2.6768 bpw  accuracy: 0.96804215\n",
            " -- 3.1689 bpw  accuracy: 0.97383562\n",
            " -- 3.1705 bpw  accuracy: 0.97434662\n",
            " -- 4.0439 bpw  accuracy: 0.98390226\n",
            " -- 4.0471 bpw  accuracy: 0.98541099\n",
            " -- 4.0816 bpw  accuracy: 0.98643687\n",
            " -- 4.1381 bpw  accuracy: 0.98690584\n",
            " -- 4.1705 bpw  accuracy: 0.98699130\n",
            " -- 4.1902 bpw  accuracy: 0.98808010\n",
            " -- 4.2737 bpw  accuracy: 0.99000779\n",
            " -- 4.3295 bpw  accuracy: 0.99061608\n",
            " -- 5.2564 bpw  accuracy: 0.99469424\n",
            " -- 5.3295 bpw  accuracy: 0.99562005\n",
            " -- 6.0439 bpw  accuracy: 0.99588079\n",
            " -- 6.3381 bpw  accuracy: 0.99781009\n",
            " -- 8.0439 bpw  accuracy: 0.99892539\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.22 (Attention)        |\n",
            "| Duration: 11.61 seconds                      |\n",
            "| Completed step: 45/67                        |\n",
            "| Avg time / step (rolling): 21.66 seconds     |\n",
            "| Estimated remaining time: 7min 56sec         |\n",
            "| Last checkpoint layer: model.layers.17 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Saving checkpoint...\n",
            " -- Layer: model.layers.22 (MLP)\n",
            " -- model.layers.22.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.22.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.22.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.22.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.22.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.22.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.22.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.22.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.22.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.22.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.22.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.22.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.22.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.22.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.22.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.22.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.22.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.22.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.22.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.22.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.22.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.22.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.22.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.22.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.22.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.22.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.22.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.22.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.22.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.22.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.22.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.22.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.22.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.22.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.22.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.22.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.22.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.22.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.22.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.22.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.91251618\n",
            " -- 2.3230 bpw  accuracy: 0.91474833\n",
            " -- 2.5958 bpw  accuracy: 0.92713317\n",
            " -- 2.9120 bpw  accuracy: 0.93082406\n",
            " -- 3.2833 bpw  accuracy: 0.95597177\n",
            " -- 3.3655 bpw  accuracy: 0.95939075\n",
            " -- 3.6186 bpw  accuracy: 0.96454042\n",
            " -- 4.1368 bpw  accuracy: 0.97719537\n",
            " -- 4.1977 bpw  accuracy: 0.97911052\n",
            " -- 4.2662 bpw  accuracy: 0.97761156\n",
            " -- 4.3484 bpw  accuracy: 0.98032599\n",
            " -- 5.2491 bpw  accuracy: 0.98861435\n",
            " -- 5.3313 bpw  accuracy: 0.99020259\n",
            " -- 6.0713 bpw  accuracy: 0.99382043\n",
            " -- 6.3032 bpw  accuracy: 0.99419719\n",
            " -- 6.8687 bpw  accuracy: 0.99521853\n",
            " -- 8.0354 bpw  accuracy: 0.99831528\n",
            "------------------------------------------------------\n",
            "| Measured: model.layers.22 (MLP)                    |\n",
            "| Duration: 32.82 seconds                            |\n",
            "| Completed step: 46/67                              |\n",
            "| Avg time / step (rolling): 21.80 seconds           |\n",
            "| Estimated remaining time: 7min 37sec               |\n",
            "| Last checkpoint layer: model.layers.22 (Attention) |\n",
            "------------------------------------------------------\n",
            " -- Layer: model.layers.23 (Attention)\n",
            " -- model.layers.23.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.23.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.23.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.23.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.23.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.23.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.23.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.23.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.23.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.23.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.23.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.23.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.23.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.23.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.23.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.23.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.23.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.23.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.23.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.23.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.23.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.23.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.23.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.23.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.23.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.23.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.23.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.23.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.23.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.23.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.23.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.23.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.23.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.23.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.23.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.23.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.23.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.23.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.23.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.23.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.23.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.23.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.23.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.23.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.23.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.23.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.23.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.23.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.23.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.23.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.23.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.23.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.23.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.23.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.23.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.23.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.23.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.94350832\n",
            " -- 2.1987 bpw  accuracy: 0.94743957\n",
            " -- 2.2831 bpw  accuracy: 0.96365369\n",
            " -- 2.6768 bpw  accuracy: 0.96934651\n",
            " -- 3.1689 bpw  accuracy: 0.97380774\n",
            " -- 3.1705 bpw  accuracy: 0.97427079\n",
            " -- 4.0439 bpw  accuracy: 0.98460368\n",
            " -- 4.0471 bpw  accuracy: 0.98561674\n",
            " -- 4.0816 bpw  accuracy: 0.98704598\n",
            " -- 4.1381 bpw  accuracy: 0.98740184\n",
            " -- 4.1705 bpw  accuracy: 0.98702644\n",
            " -- 4.1902 bpw  accuracy: 0.98722533\n",
            " -- 4.2737 bpw  accuracy: 0.99092999\n",
            " -- 4.3295 bpw  accuracy: 0.99169384\n",
            " -- 5.2564 bpw  accuracy: 0.99495155\n",
            " -- 5.3295 bpw  accuracy: 0.99588160\n",
            " -- 6.0439 bpw  accuracy: 0.99592722\n",
            " -- 6.3381 bpw  accuracy: 0.99804881\n",
            " -- 8.0439 bpw  accuracy: 0.99893775\n",
            "------------------------------------------------------\n",
            "| Measured: model.layers.23 (Attention)              |\n",
            "| Duration: 11.56 seconds                            |\n",
            "| Completed step: 47/67                              |\n",
            "| Avg time / step (rolling): 21.83 seconds           |\n",
            "| Estimated remaining time: 7min 16sec               |\n",
            "| Last checkpoint layer: model.layers.22 (Attention) |\n",
            "------------------------------------------------------\n",
            " -- Layer: model.layers.23 (MLP)\n",
            " -- model.layers.23.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.23.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.23.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.23.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.23.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.23.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.23.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.23.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.23.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.23.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.23.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.23.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.23.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.23.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.23.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.23.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.23.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.23.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.23.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.23.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.23.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.23.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.23.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.23.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.23.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.23.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.23.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.23.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.23.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.23.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.23.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.23.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.23.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.23.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.23.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.23.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.23.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.23.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.23.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.23.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.91492095\n",
            " -- 2.3230 bpw  accuracy: 0.91703334\n",
            " -- 2.5958 bpw  accuracy: 0.92876001\n",
            " -- 2.9120 bpw  accuracy: 0.93226460\n",
            " -- 3.2833 bpw  accuracy: 0.95706935\n",
            " -- 3.3655 bpw  accuracy: 0.96038942\n",
            " -- 3.6186 bpw  accuracy: 0.96531573\n",
            " -- 4.1368 bpw  accuracy: 0.97780038\n",
            " -- 4.1977 bpw  accuracy: 0.97965460\n",
            " -- 4.2662 bpw  accuracy: 0.97816395\n",
            " -- 4.3484 bpw  accuracy: 0.98081349\n",
            " -- 5.2491 bpw  accuracy: 0.98890239\n",
            " -- 5.3313 bpw  accuracy: 0.99044484\n",
            " -- 6.0713 bpw  accuracy: 0.99398197\n",
            " -- 6.3032 bpw  accuracy: 0.99434175\n",
            " -- 6.8687 bpw  accuracy: 0.99530725\n",
            " -- 8.0354 bpw  accuracy: 0.99835360\n",
            "------------------------------------------------------\n",
            "| Measured: model.layers.23 (MLP)                    |\n",
            "| Duration: 32.94 seconds                            |\n",
            "| Completed step: 48/67                              |\n",
            "| Avg time / step (rolling): 21.98 seconds           |\n",
            "| Estimated remaining time: 6min 57sec               |\n",
            "| Last checkpoint layer: model.layers.22 (Attention) |\n",
            "------------------------------------------------------\n",
            " -- Layer: model.layers.24 (Attention)\n",
            " -- model.layers.24.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.24.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.24.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.24.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.24.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.24.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.24.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.24.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.24.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.24.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.24.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.24.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.24.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.24.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.24.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.24.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.24.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.24.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.24.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.24.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.24.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.24.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.24.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.24.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.24.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.24.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.24.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.24.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.24.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.24.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.24.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.24.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.24.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.24.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.24.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.24.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.24.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.24.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.24.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.24.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.24.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.24.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.24.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.24.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.24.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.24.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.24.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.24.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.24.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.24.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.24.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.24.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.24.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.24.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.24.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.24.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.24.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.94613010\n",
            " -- 2.1987 bpw  accuracy: 0.94711100\n",
            " -- 2.2831 bpw  accuracy: 0.96372519\n",
            " -- 2.6768 bpw  accuracy: 0.96937439\n",
            " -- 3.1689 bpw  accuracy: 0.97366796\n",
            " -- 3.1705 bpw  accuracy: 0.97431418\n",
            " -- 4.0439 bpw  accuracy: 0.98413352\n",
            " -- 4.0471 bpw  accuracy: 0.98518307\n",
            " -- 4.0816 bpw  accuracy: 0.98657722\n",
            " -- 4.1381 bpw  accuracy: 0.98701340\n",
            " -- 4.1705 bpw  accuracy: 0.98712385\n",
            " -- 4.1902 bpw  accuracy: 0.98801015\n",
            " -- 4.2737 bpw  accuracy: 0.99044554\n",
            " -- 4.3295 bpw  accuracy: 0.99154698\n",
            " -- 5.2564 bpw  accuracy: 0.99478565\n",
            " -- 5.3295 bpw  accuracy: 0.99571508\n",
            " -- 6.0439 bpw  accuracy: 0.99574457\n",
            " -- 6.3381 bpw  accuracy: 0.99791052\n",
            " -- 8.0439 bpw  accuracy: 0.99889482\n",
            "------------------------------------------------------\n",
            "| Measured: model.layers.24 (Attention)              |\n",
            "| Duration: 11.53 seconds                            |\n",
            "| Completed step: 49/67                              |\n",
            "| Avg time / step (rolling): 22.02 seconds           |\n",
            "| Estimated remaining time: 6min 36sec               |\n",
            "| Last checkpoint layer: model.layers.22 (Attention) |\n",
            "------------------------------------------------------\n",
            " -- Layer: model.layers.24 (MLP)\n",
            " -- model.layers.24.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.24.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.24.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.24.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.24.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.24.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.24.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.24.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.24.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.24.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.24.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.24.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.24.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.24.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.24.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.24.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.24.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.24.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.24.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.24.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.24.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.24.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.24.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.24.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.24.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.24.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.24.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.24.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.24.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.24.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.24.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.24.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.24.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.24.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.24.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.24.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.24.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.24.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.24.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.24.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.91657185\n",
            " -- 2.3230 bpw  accuracy: 0.91864502\n",
            " -- 2.5958 bpw  accuracy: 0.92985990\n",
            " -- 2.9120 bpw  accuracy: 0.93323530\n",
            " -- 3.2833 bpw  accuracy: 0.95777146\n",
            " -- 3.3655 bpw  accuracy: 0.96110214\n",
            " -- 3.6186 bpw  accuracy: 0.96585736\n",
            " -- 4.1368 bpw  accuracy: 0.97818041\n",
            " -- 4.1977 bpw  accuracy: 0.98002018\n",
            " -- 4.2662 bpw  accuracy: 0.97849492\n",
            " -- 4.3484 bpw  accuracy: 0.98114214\n",
            " -- 5.2491 bpw  accuracy: 0.98904744\n",
            " -- 5.3313 bpw  accuracy: 0.99060529\n",
            " -- 6.0713 bpw  accuracy: 0.99406705\n",
            " -- 6.3032 bpw  accuracy: 0.99441305\n",
            " -- 6.8687 bpw  accuracy: 0.99533542\n",
            " -- 8.0354 bpw  accuracy: 0.99838082\n",
            "------------------------------------------------------\n",
            "| Measured: model.layers.24 (MLP)                    |\n",
            "| Duration: 32.79 seconds                            |\n",
            "| Completed step: 50/67                              |\n",
            "| Avg time / step (rolling): 22.16 seconds           |\n",
            "| Estimated remaining time: 6min 16sec               |\n",
            "| Last checkpoint layer: model.layers.22 (Attention) |\n",
            "------------------------------------------------------\n",
            " -- Layer: model.layers.25 (Attention)\n",
            " -- model.layers.25.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.25.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.25.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.25.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.25.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.25.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.25.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.25.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.25.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.25.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.25.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.25.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.25.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.25.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.25.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.25.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.25.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.25.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.25.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.25.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.25.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.25.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.25.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.25.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.25.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.25.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.25.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.25.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.25.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.25.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.25.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.25.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.25.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.25.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.25.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.25.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.25.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.25.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.25.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.25.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.25.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.25.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.25.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.25.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.25.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.25.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.25.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.25.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.25.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.25.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.25.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.25.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.25.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.25.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.25.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.25.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.25.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.94703671\n",
            " -- 2.1987 bpw  accuracy: 0.94903753\n",
            " -- 2.2831 bpw  accuracy: 0.96352154\n",
            " -- 2.6768 bpw  accuracy: 0.96895083\n",
            " -- 3.1689 bpw  accuracy: 0.97402924\n",
            " -- 3.1705 bpw  accuracy: 0.97503982\n",
            " -- 4.0439 bpw  accuracy: 0.98478128\n",
            " -- 4.0471 bpw  accuracy: 0.98583486\n",
            " -- 4.0816 bpw  accuracy: 0.98695449\n",
            " -- 4.1381 bpw  accuracy: 0.98716294\n",
            " -- 4.1705 bpw  accuracy: 0.98734276\n",
            " -- 4.1902 bpw  accuracy: 0.98825277\n",
            " -- 4.2737 bpw  accuracy: 0.99091094\n",
            " -- 4.3295 bpw  accuracy: 0.99162244\n",
            " -- 5.2564 bpw  accuracy: 0.99485902\n",
            " -- 5.3295 bpw  accuracy: 0.99582485\n",
            " -- 6.0439 bpw  accuracy: 0.99580015\n",
            " -- 6.3381 bpw  accuracy: 0.99791224\n",
            " -- 8.0439 bpw  accuracy: 0.99890361\n",
            "------------------------------------------------------\n",
            "| Measured: model.layers.25 (Attention)              |\n",
            "| Duration: 11.43 seconds                            |\n",
            "| Completed step: 51/67                              |\n",
            "| Avg time / step (rolling): 22.18 seconds           |\n",
            "| Estimated remaining time: 5min 54sec               |\n",
            "| Last checkpoint layer: model.layers.22 (Attention) |\n",
            "------------------------------------------------------\n",
            " -- Layer: model.layers.25 (MLP)\n",
            " -- model.layers.25.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.25.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.25.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.25.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.25.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.25.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.25.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.25.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.25.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.25.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.25.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.25.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.25.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.25.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.25.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.25.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.25.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.25.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.25.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.25.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.25.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.25.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.25.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.25.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.25.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.25.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.25.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.25.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.25.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.25.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.25.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.25.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.25.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.25.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.25.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.25.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.25.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.25.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.25.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.25.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.91562339\n",
            " -- 2.3230 bpw  accuracy: 0.91773888\n",
            " -- 2.5958 bpw  accuracy: 0.92891522\n",
            " -- 2.9120 bpw  accuracy: 0.93226922\n",
            " -- 3.2833 bpw  accuracy: 0.95728451\n",
            " -- 3.3655 bpw  accuracy: 0.96060057\n",
            " -- 3.6186 bpw  accuracy: 0.96533976\n",
            " -- 4.1368 bpw  accuracy: 0.97791154\n",
            " -- 4.1977 bpw  accuracy: 0.97977326\n",
            " -- 4.2662 bpw  accuracy: 0.97828817\n",
            " -- 4.3484 bpw  accuracy: 0.98090362\n",
            " -- 5.2491 bpw  accuracy: 0.98896852\n",
            " -- 5.3313 bpw  accuracy: 0.99049083\n",
            " -- 6.0713 bpw  accuracy: 0.99402504\n",
            " -- 6.3032 bpw  accuracy: 0.99437950\n",
            " -- 6.8687 bpw  accuracy: 0.99529801\n",
            " -- 8.0354 bpw  accuracy: 0.99836888\n",
            "------------------------------------------------------\n",
            "| Measured: model.layers.25 (MLP)                    |\n",
            "| Duration: 32.71 seconds                            |\n",
            "| Completed step: 52/67                              |\n",
            "| Avg time / step (rolling): 22.20 seconds           |\n",
            "| Estimated remaining time: 5min 33sec               |\n",
            "| Last checkpoint layer: model.layers.22 (Attention) |\n",
            "------------------------------------------------------\n",
            " -- Layer: model.layers.26 (Attention)\n",
            " -- model.layers.26.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.26.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.26.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.26.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.26.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.26.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.26.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.26.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.26.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.26.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.26.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.26.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.26.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.26.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.26.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.26.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.26.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.26.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.26.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.26.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.26.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.26.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.26.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.26.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.26.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.26.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.26.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.26.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.26.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.26.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.26.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.26.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.26.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.26.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.26.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.26.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.26.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.26.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.26.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.26.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.26.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.26.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.26.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.26.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.26.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.26.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.26.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.26.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.26.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.26.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.26.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.26.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.26.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.26.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.26.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.26.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.26.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.94674800\n",
            " -- 2.1987 bpw  accuracy: 0.95061924\n",
            " -- 2.2831 bpw  accuracy: 0.96216866\n",
            " -- 2.6768 bpw  accuracy: 0.96879940\n",
            " -- 3.1689 bpw  accuracy: 0.97346908\n",
            " -- 3.1705 bpw  accuracy: 0.97502373\n",
            " -- 4.0439 bpw  accuracy: 0.98457659\n",
            " -- 4.0471 bpw  accuracy: 0.98542139\n",
            " -- 4.0816 bpw  accuracy: 0.98670273\n",
            " -- 4.1381 bpw  accuracy: 0.98716105\n",
            " -- 4.1705 bpw  accuracy: 0.98666579\n",
            " -- 4.1902 bpw  accuracy: 0.98802649\n",
            " -- 4.2737 bpw  accuracy: 0.99043331\n",
            " -- 4.3295 bpw  accuracy: 0.99131313\n",
            " -- 5.2564 bpw  accuracy: 0.99476094\n",
            " -- 5.3295 bpw  accuracy: 0.99558877\n",
            " -- 6.0439 bpw  accuracy: 0.99588819\n",
            " -- 6.3381 bpw  accuracy: 0.99774740\n",
            " -- 8.0439 bpw  accuracy: 0.99888948\n",
            "------------------------------------------------------\n",
            "| Measured: model.layers.26 (Attention)              |\n",
            "| Duration: 11.55 seconds                            |\n",
            "| Completed step: 53/67                              |\n",
            "| Avg time / step (rolling): 22.20 seconds           |\n",
            "| Estimated remaining time: 5min 10sec               |\n",
            "| Last checkpoint layer: model.layers.22 (Attention) |\n",
            "------------------------------------------------------\n",
            " -- Layer: model.layers.26 (MLP)\n",
            " -- model.layers.26.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.26.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.26.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.26.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.26.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.26.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.26.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.26.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.26.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.26.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.26.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.26.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.26.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.26.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.26.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.26.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.26.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.26.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.26.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.26.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.26.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.26.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.26.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.26.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.26.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.26.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.26.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.26.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.26.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.26.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.26.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.26.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.26.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.26.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.26.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.26.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.26.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.26.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.26.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.26.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.91583343\n",
            " -- 2.3230 bpw  accuracy: 0.91793470\n",
            " -- 2.5958 bpw  accuracy: 0.92900173\n",
            " -- 2.9120 bpw  accuracy: 0.93232732\n",
            " -- 3.2833 bpw  accuracy: 0.95733818\n",
            " -- 3.3655 bpw  accuracy: 0.96066561\n",
            " -- 3.6186 bpw  accuracy: 0.96536637\n",
            " -- 4.1368 bpw  accuracy: 0.97794226\n",
            " -- 4.1977 bpw  accuracy: 0.97980166\n",
            " -- 4.2662 bpw  accuracy: 0.97829534\n",
            " -- 4.3484 bpw  accuracy: 0.98092586\n",
            " -- 5.2491 bpw  accuracy: 0.98897351\n",
            " -- 5.3313 bpw  accuracy: 0.99050241\n",
            " -- 6.0713 bpw  accuracy: 0.99402146\n",
            " -- 6.3032 bpw  accuracy: 0.99437597\n",
            " -- 6.8687 bpw  accuracy: 0.99529353\n",
            " -- 8.0354 bpw  accuracy: 0.99837019\n",
            "------------------------------------------------------\n",
            "| Measured: model.layers.26 (MLP)                    |\n",
            "| Duration: 32.78 seconds                            |\n",
            "| Completed step: 54/67                              |\n",
            "| Avg time / step (rolling): 22.17 seconds           |\n",
            "| Estimated remaining time: 4min 48sec               |\n",
            "| Last checkpoint layer: model.layers.22 (Attention) |\n",
            "------------------------------------------------------\n",
            " -- Saving checkpoint...\n",
            " -- Layer: model.layers.27 (Attention)\n",
            " -- model.layers.27.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.27.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.27.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.27.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.27.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.27.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.27.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.27.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.27.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.27.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.27.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.27.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.27.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.27.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.27.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.27.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.27.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.27.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.27.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.27.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.27.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.27.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.27.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.27.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.27.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.27.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.27.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.27.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.27.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.27.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.27.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.27.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.27.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.27.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.27.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.27.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.27.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.27.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.27.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.27.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.27.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.27.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.27.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.27.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.27.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.27.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.27.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.27.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.27.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.27.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.27.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.27.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.27.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.27.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.27.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.27.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.27.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.93795811\n",
            " -- 2.1987 bpw  accuracy: 0.94037255\n",
            " -- 2.2831 bpw  accuracy: 0.95687949\n",
            " -- 2.6768 bpw  accuracy: 0.96441086\n",
            " -- 3.1689 bpw  accuracy: 0.97068647\n",
            " -- 3.1705 bpw  accuracy: 0.97144710\n",
            " -- 4.0439 bpw  accuracy: 0.98290058\n",
            " -- 4.0471 bpw  accuracy: 0.98309339\n",
            " -- 4.0816 bpw  accuracy: 0.98477418\n",
            " -- 4.1381 bpw  accuracy: 0.98526963\n",
            " -- 4.1705 bpw  accuracy: 0.98502587\n",
            " -- 4.1902 bpw  accuracy: 0.98672771\n",
            " -- 4.2737 bpw  accuracy: 0.98944031\n",
            " -- 4.3295 bpw  accuracy: 0.99019910\n",
            " -- 5.2564 bpw  accuracy: 0.99403471\n",
            " -- 5.3295 bpw  accuracy: 0.99506409\n",
            " -- 6.0439 bpw  accuracy: 0.99535587\n",
            " -- 6.3381 bpw  accuracy: 0.99749050\n",
            " -- 8.0439 bpw  accuracy: 0.99875455\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.27 (Attention)        |\n",
            "| Duration: 11.62 seconds                      |\n",
            "| Completed step: 55/67                        |\n",
            "| Avg time / step (rolling): 22.17 seconds     |\n",
            "| Estimated remaining time: 4min 26sec         |\n",
            "| Last checkpoint layer: model.layers.26 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.27 (MLP)\n",
            " -- model.layers.27.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.27.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.27.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.27.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.27.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.27.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.27.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.27.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.27.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.27.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.27.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.27.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.27.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.27.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.27.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.27.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.27.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.27.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.27.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.27.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.27.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.27.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.27.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.27.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.27.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.27.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.27.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.27.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.27.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.27.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.27.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.27.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.27.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.27.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.27.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.27.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.27.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.27.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.27.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.27.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.91598529\n",
            " -- 2.3230 bpw  accuracy: 0.91809544\n",
            " -- 2.5958 bpw  accuracy: 0.92895006\n",
            " -- 2.9120 bpw  accuracy: 0.93224909\n",
            " -- 3.2833 bpw  accuracy: 0.95734971\n",
            " -- 3.3655 bpw  accuracy: 0.96071517\n",
            " -- 3.6186 bpw  accuracy: 0.96534735\n",
            " -- 4.1368 bpw  accuracy: 0.97786327\n",
            " -- 4.1977 bpw  accuracy: 0.97976400\n",
            " -- 4.2662 bpw  accuracy: 0.97828293\n",
            " -- 4.3484 bpw  accuracy: 0.98094010\n",
            " -- 5.2491 bpw  accuracy: 0.98896888\n",
            " -- 5.3313 bpw  accuracy: 0.99050572\n",
            " -- 6.0713 bpw  accuracy: 0.99399974\n",
            " -- 6.3032 bpw  accuracy: 0.99438317\n",
            " -- 6.8687 bpw  accuracy: 0.99529367\n",
            " -- 8.0354 bpw  accuracy: 0.99837908\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.27 (MLP)              |\n",
            "| Duration: 32.95 seconds                      |\n",
            "| Completed step: 56/67                        |\n",
            "| Avg time / step (rolling): 22.19 seconds     |\n",
            "| Estimated remaining time: 4min 4sec          |\n",
            "| Last checkpoint layer: model.layers.26 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.28 (Attention)\n",
            " -- model.layers.28.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.28.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.28.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.28.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.28.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.28.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.28.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.28.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.28.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.28.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.28.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.28.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.28.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.28.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.28.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.28.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.28.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.28.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.28.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.28.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.28.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.28.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.28.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.28.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.28.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.28.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.28.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.28.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.28.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.28.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.28.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.28.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.28.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.28.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.28.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.28.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.28.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.28.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.28.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.28.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.28.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.28.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.28.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.28.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.28.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.28.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.28.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.28.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.28.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.28.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.28.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.28.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.28.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.28.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.28.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.28.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.28.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.92809425\n",
            " -- 2.1987 bpw  accuracy: 0.93130559\n",
            " -- 2.2831 bpw  accuracy: 0.95162000\n",
            " -- 2.6768 bpw  accuracy: 0.96183500\n",
            " -- 3.1689 bpw  accuracy: 0.96655401\n",
            " -- 3.1705 bpw  accuracy: 0.96694167\n",
            " -- 4.0439 bpw  accuracy: 0.98134479\n",
            " -- 4.0471 bpw  accuracy: 0.98195715\n",
            " -- 4.0816 bpw  accuracy: 0.98297033\n",
            " -- 4.1381 bpw  accuracy: 0.98360268\n",
            " -- 4.1705 bpw  accuracy: 0.98335986\n",
            " -- 4.1902 bpw  accuracy: 0.98475718\n",
            " -- 4.2737 bpw  accuracy: 0.98811884\n",
            " -- 4.3295 bpw  accuracy: 0.98920792\n",
            " -- 5.2564 bpw  accuracy: 0.99343224\n",
            " -- 5.3295 bpw  accuracy: 0.99438641\n",
            " -- 6.0439 bpw  accuracy: 0.99501176\n",
            " -- 6.3381 bpw  accuracy: 0.99731247\n",
            " -- 8.0439 bpw  accuracy: 0.99866622\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.28 (Attention)        |\n",
            "| Duration: 11.55 seconds                      |\n",
            "| Completed step: 57/67                        |\n",
            "| Avg time / step (rolling): 22.18 seconds     |\n",
            "| Estimated remaining time: 3min 41sec         |\n",
            "| Last checkpoint layer: model.layers.26 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.28 (MLP)\n",
            " -- model.layers.28.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.28.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.28.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.28.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.28.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.28.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.28.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.28.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.28.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.28.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.28.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.28.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.28.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.28.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.28.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.28.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.28.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.28.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.28.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.28.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.28.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.28.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.28.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.28.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.28.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.28.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.28.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.28.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.28.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.28.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.28.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.28.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.28.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.28.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.28.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.28.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.28.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.28.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.28.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.28.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.91184838\n",
            " -- 2.3230 bpw  accuracy: 0.91408184\n",
            " -- 2.5958 bpw  accuracy: 0.92528848\n",
            " -- 2.9120 bpw  accuracy: 0.92882108\n",
            " -- 3.2833 bpw  accuracy: 0.95526566\n",
            " -- 3.3655 bpw  accuracy: 0.95877705\n",
            " -- 3.6186 bpw  accuracy: 0.96356409\n",
            " -- 4.1368 bpw  accuracy: 0.97664007\n",
            " -- 4.1977 bpw  accuracy: 0.97866603\n",
            " -- 4.2662 bpw  accuracy: 0.97724269\n",
            " -- 4.3484 bpw  accuracy: 0.97999242\n",
            " -- 5.2491 bpw  accuracy: 0.98843080\n",
            " -- 5.3313 bpw  accuracy: 0.99003291\n",
            " -- 6.0713 bpw  accuracy: 0.99367268\n",
            " -- 6.3032 bpw  accuracy: 0.99412162\n",
            " -- 6.8687 bpw  accuracy: 0.99507032\n",
            " -- 8.0354 bpw  accuracy: 0.99827864\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.28 (MLP)              |\n",
            "| Duration: 32.93 seconds                      |\n",
            "| Completed step: 58/67                        |\n",
            "| Avg time / step (rolling): 22.18 seconds     |\n",
            "| Estimated remaining time: 3min 19sec         |\n",
            "| Last checkpoint layer: model.layers.26 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.29 (Attention)\n",
            " -- model.layers.29.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.29.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.29.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.29.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.29.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.29.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.29.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.29.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.29.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.29.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.29.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.29.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.29.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.29.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.29.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.29.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.29.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.29.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.29.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.29.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.29.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.29.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.29.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.29.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.29.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.29.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.29.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.29.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.29.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.29.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.29.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.29.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.29.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.29.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.29.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.29.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.29.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.29.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.29.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.29.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.29.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.29.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.29.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.29.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.29.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.29.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.29.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.29.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.29.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.29.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.29.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.29.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.29.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.29.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.29.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.29.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.29.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.93743287\n",
            " -- 2.1987 bpw  accuracy: 0.93952581\n",
            " -- 2.2831 bpw  accuracy: 0.95433759\n",
            " -- 2.6768 bpw  accuracy: 0.96215299\n",
            " -- 3.1689 bpw  accuracy: 0.96858593\n",
            " -- 3.1705 bpw  accuracy: 0.96925599\n",
            " -- 4.0439 bpw  accuracy: 0.98201847\n",
            " -- 4.0471 bpw  accuracy: 0.98233914\n",
            " -- 4.0816 bpw  accuracy: 0.98426376\n",
            " -- 4.1381 bpw  accuracy: 0.98489235\n",
            " -- 4.1705 bpw  accuracy: 0.98469663\n",
            " -- 4.1902 bpw  accuracy: 0.98559646\n",
            " -- 4.2737 bpw  accuracy: 0.98867508\n",
            " -- 4.3295 bpw  accuracy: 0.98946542\n",
            " -- 5.2564 bpw  accuracy: 0.99372756\n",
            " -- 5.3295 bpw  accuracy: 0.99463208\n",
            " -- 6.0439 bpw  accuracy: 0.99532237\n",
            " -- 6.3381 bpw  accuracy: 0.99737353\n",
            " -- 8.0439 bpw  accuracy: 0.99869163\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.29 (Attention)        |\n",
            "| Duration: 11.54 seconds                      |\n",
            "| Completed step: 59/67                        |\n",
            "| Avg time / step (rolling): 22.18 seconds     |\n",
            "| Estimated remaining time: 2min 57sec         |\n",
            "| Last checkpoint layer: model.layers.26 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.29 (MLP)\n",
            " -- model.layers.29.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.29.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.29.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.29.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.29.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.29.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.29.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.29.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.29.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.29.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.29.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.29.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.29.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.29.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.29.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.29.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.29.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.29.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.29.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.29.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.29.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.29.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.29.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.29.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.29.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.29.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.29.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.29.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.29.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.29.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.29.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.29.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.29.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.29.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.29.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.29.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.29.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.29.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.29.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.29.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.90702106\n",
            " -- 2.3230 bpw  accuracy: 0.90948632\n",
            " -- 2.5958 bpw  accuracy: 0.92114358\n",
            " -- 2.9120 bpw  accuracy: 0.92480690\n",
            " -- 3.2833 bpw  accuracy: 0.95224413\n",
            " -- 3.3655 bpw  accuracy: 0.95633677\n",
            " -- 3.6186 bpw  accuracy: 0.96139932\n",
            " -- 4.1368 bpw  accuracy: 0.97507264\n",
            " -- 4.1977 bpw  accuracy: 0.97724465\n",
            " -- 4.2662 bpw  accuracy: 0.97551327\n",
            " -- 4.3484 bpw  accuracy: 0.97867952\n",
            " -- 5.2491 bpw  accuracy: 0.98747328\n",
            " -- 5.3313 bpw  accuracy: 0.98934675\n",
            " -- 6.0713 bpw  accuracy: 0.99306971\n",
            " -- 6.3032 bpw  accuracy: 0.99362639\n",
            " -- 6.8687 bpw  accuracy: 0.99462833\n",
            " -- 8.0354 bpw  accuracy: 0.99812934\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.29 (MLP)              |\n",
            "| Duration: 33.09 seconds                      |\n",
            "| Completed step: 60/67                        |\n",
            "| Avg time / step (rolling): 22.21 seconds     |\n",
            "| Estimated remaining time: 2min 35sec         |\n",
            "| Last checkpoint layer: model.layers.26 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.30 (Attention)\n",
            " -- model.layers.30.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.30.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.30.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.30.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.30.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.30.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.30.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.30.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.30.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.30.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.30.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.30.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.30.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.30.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.30.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.30.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.30.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.30.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.30.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.30.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.30.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.30.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.30.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.30.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.30.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.30.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.30.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.30.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.30.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.30.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.30.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.30.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.30.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.30.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.30.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.30.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.30.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.30.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.30.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.30.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.30.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.30.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.30.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.30.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.30.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.30.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.30.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.30.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.30.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.30.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.30.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.30.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.30.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.30.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.30.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.30.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.30.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.93174469\n",
            " -- 2.1987 bpw  accuracy: 0.93016255\n",
            " -- 2.2831 bpw  accuracy: 0.94967498\n",
            " -- 2.6768 bpw  accuracy: 0.95862677\n",
            " -- 3.1689 bpw  accuracy: 0.96605179\n",
            " -- 3.1705 bpw  accuracy: 0.96689707\n",
            " -- 4.0439 bpw  accuracy: 0.98016313\n",
            " -- 4.0471 bpw  accuracy: 0.98100434\n",
            " -- 4.0816 bpw  accuracy: 0.98268483\n",
            " -- 4.1381 bpw  accuracy: 0.98335214\n",
            " -- 4.1705 bpw  accuracy: 0.98320206\n",
            " -- 4.1902 bpw  accuracy: 0.98362956\n",
            " -- 4.2737 bpw  accuracy: 0.98779307\n",
            " -- 4.3295 bpw  accuracy: 0.98872141\n",
            " -- 5.2564 bpw  accuracy: 0.99331115\n",
            " -- 5.3295 bpw  accuracy: 0.99436144\n",
            " -- 6.0439 bpw  accuracy: 0.99488251\n",
            " -- 6.3381 bpw  accuracy: 0.99717153\n",
            " -- 8.0439 bpw  accuracy: 0.99865818\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.30 (Attention)        |\n",
            "| Duration: 11.60 seconds                      |\n",
            "| Completed step: 61/67                        |\n",
            "| Avg time / step (rolling): 22.23 seconds     |\n",
            "| Estimated remaining time: 2min 13sec         |\n",
            "| Last checkpoint layer: model.layers.26 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.30 (MLP)\n",
            " -- model.layers.30.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.30.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.30.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.30.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.30.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.30.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.30.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.30.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.30.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.30.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.30.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.30.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.30.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.30.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.30.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.30.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.30.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.30.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.30.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.30.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.30.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.30.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.30.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.30.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.30.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.30.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.30.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.30.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.30.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.30.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.30.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.30.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.30.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.30.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.30.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.30.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.30.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.30.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.30.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.30.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.89640599\n",
            " -- 2.3230 bpw  accuracy: 0.89941315\n",
            " -- 2.5958 bpw  accuracy: 0.91088347\n",
            " -- 2.9120 bpw  accuracy: 0.91447106\n",
            " -- 3.2833 bpw  accuracy: 0.94508151\n",
            " -- 3.3655 bpw  accuracy: 0.95132015\n",
            " -- 3.6186 bpw  accuracy: 0.95627121\n",
            " -- 4.1368 bpw  accuracy: 0.97208021\n",
            " -- 4.1977 bpw  accuracy: 0.97443224\n",
            " -- 4.2662 bpw  accuracy: 0.97155793\n",
            " -- 4.3484 bpw  accuracy: 0.97610000\n",
            " -- 5.2491 bpw  accuracy: 0.98538593\n",
            " -- 5.3313 bpw  accuracy: 0.98802554\n",
            " -- 6.0713 bpw  accuracy: 0.99193282\n",
            " -- 6.3032 bpw  accuracy: 0.99252336\n",
            " -- 6.8687 bpw  accuracy: 0.99349836\n",
            " -- 8.0354 bpw  accuracy: 0.99781215\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.30 (MLP)              |\n",
            "| Duration: 33.13 seconds                      |\n",
            "| Completed step: 62/67                        |\n",
            "| Avg time / step (rolling): 22.27 seconds     |\n",
            "| Estimated remaining time: 1min 51sec         |\n",
            "| Last checkpoint layer: model.layers.26 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.31 (Attention)\n",
            " -- model.layers.31.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.31.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.31.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.31.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.31.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.31.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.31.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.31.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.31.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.31.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.31.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.31.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.31.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.31.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.31.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.31.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.31.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.31.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.31.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.31.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.31.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.31.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.31.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.31.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.31.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.31.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.31.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.31.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.31.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.31.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.31.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.31.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.31.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.31.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.31.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.31.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.31.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.31.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.31.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.31.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.31.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.31.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.31.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.31.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.31.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.31.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.31.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.31.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.31.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.31.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.31.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.31.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.31.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.31.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.31.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.31.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.31.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.93034617\n",
            " -- 2.1987 bpw  accuracy: 0.93504557\n",
            " -- 2.2831 bpw  accuracy: 0.94916219\n",
            " -- 2.6768 bpw  accuracy: 0.96250245\n",
            " -- 3.1689 bpw  accuracy: 0.96644609\n",
            " -- 3.1705 bpw  accuracy: 0.96742413\n",
            " -- 4.0439 bpw  accuracy: 0.97957727\n",
            " -- 4.0471 bpw  accuracy: 0.98028673\n",
            " -- 4.0816 bpw  accuracy: 0.98204840\n",
            " -- 4.1381 bpw  accuracy: 0.98281621\n",
            " -- 4.1705 bpw  accuracy: 0.98282973\n",
            " -- 4.1902 bpw  accuracy: 0.98419214\n",
            " -- 4.2737 bpw  accuracy: 0.98700140\n",
            " -- 4.3295 bpw  accuracy: 0.98792818\n",
            " -- 5.2564 bpw  accuracy: 0.99294487\n",
            " -- 5.3295 bpw  accuracy: 0.99395643\n",
            " -- 6.0439 bpw  accuracy: 0.99470817\n",
            " -- 6.3381 bpw  accuracy: 0.99685667\n",
            " -- 8.0439 bpw  accuracy: 0.99856495\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.31 (Attention)        |\n",
            "| Duration: 11.55 seconds                      |\n",
            "| Completed step: 63/67                        |\n",
            "| Avg time / step (rolling): 22.27 seconds     |\n",
            "| Estimated remaining time: 1min 29sec         |\n",
            "| Last checkpoint layer: model.layers.26 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Saving checkpoint...\n",
            " -- Layer: model.layers.31 (MLP)\n",
            " -- model.layers.31.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.31.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.31.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.31.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.31.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.31.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.31.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.31.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.31.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.31.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.31.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.31.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.31.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.31.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.31.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.31.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.31.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.31.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.31.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.31.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.31.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.31.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.31.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.31.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.31.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.31.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.31.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.31.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.31.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.31.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.31.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.31.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.31.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.31.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.31.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.31.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.31.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.31.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.31.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.31.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.87252791\n",
            " -- 2.3230 bpw  accuracy: 0.87642699\n",
            " -- 2.5958 bpw  accuracy: 0.88644356\n",
            " -- 2.9120 bpw  accuracy: 0.88990588\n",
            " -- 3.2833 bpw  accuracy: 0.93326407\n",
            " -- 3.3655 bpw  accuracy: 0.94093608\n",
            " -- 3.6186 bpw  accuracy: 0.94539630\n",
            " -- 4.1368 bpw  accuracy: 0.96569588\n",
            " -- 4.1977 bpw  accuracy: 0.96840616\n",
            " -- 4.2662 bpw  accuracy: 0.96532914\n",
            " -- 4.3484 bpw  accuracy: 0.97064735\n",
            " -- 5.2491 bpw  accuracy: 0.98207607\n",
            " -- 5.3313 bpw  accuracy: 0.98514478\n",
            " -- 6.0713 bpw  accuracy: 0.98993719\n",
            " -- 6.3032 bpw  accuracy: 0.99067806\n",
            " -- 6.8687 bpw  accuracy: 0.99161076\n",
            " -- 8.0354 bpw  accuracy: 0.99708822\n",
            "------------------------------------------------------\n",
            "| Measured: model.layers.31 (MLP)                    |\n",
            "| Duration: 33.17 seconds                            |\n",
            "| Completed step: 64/67                              |\n",
            "| Avg time / step (rolling): 22.31 seconds           |\n",
            "| Estimated remaining time: 1min 6sec                |\n",
            "| Last checkpoint layer: model.layers.31 (Attention) |\n",
            "------------------------------------------------------\n",
            " -- Layer: model.norm (RMSNorm)\n",
            "------------------------------------------------------\n",
            "| Measured: model.norm (RMSNorm)                     |\n",
            "| Duration: 0.56 seconds                             |\n",
            "| Completed step: 65/67                              |\n",
            "| Avg time / step (rolling): 21.21 seconds           |\n",
            "| Estimated remaining time: 0min 42sec               |\n",
            "| Last checkpoint layer: model.layers.31 (Attention) |\n",
            "------------------------------------------------------\n",
            " -- Layer: lm_head (Linear)\n",
            "------------------------------------------------------\n",
            "| Measured: lm_head (Linear)                         |\n",
            "| Duration: 5.42 seconds                             |\n",
            "| Completed step: 66/67                              |\n",
            "| Avg time / step (rolling): 18.45 seconds           |\n",
            "| Estimated remaining time: 0min 18sec               |\n",
            "| Last checkpoint layer: model.layers.31 (Attention) |\n",
            "------------------------------------------------------\n",
            " -- Saving checkpoint...\n",
            " -- Optimizing...\n",
            " -- Optimizing:    1/ 240\n",
            " -- Optimizing:    8/ 240\n",
            " -- Optimizing:   15/ 240\n",
            " -- Optimizing:   22/ 240\n",
            " -- Optimizing:   29/ 240\n",
            " -- Optimizing:   36/ 240\n",
            " -- Optimizing:   43/ 240\n",
            " -- Optimizing:   50/ 240\n",
            " -- Optimizing:   57/ 240\n",
            " -- Optimizing:   64/ 240\n",
            " -- Optimizing:   71/ 240\n",
            " -- Optimizing:   78/ 240\n",
            " -- Optimizing:   80/ 240\n",
            " -- Optimizing:   87/ 240\n",
            " -- Optimizing:   94/ 240\n",
            " -- Optimizing:  101/ 240\n",
            " -- Optimizing:  108/ 240\n",
            " -- Optimizing:  115/ 240\n",
            " -- Optimizing:  122/ 240\n",
            " -- Optimizing:  129/ 240\n",
            " -- Optimizing:  136/ 240\n",
            " -- Optimizing:  143/ 240\n",
            " -- Optimizing:  150/ 240\n",
            " -- Optimizing:  157/ 240\n",
            " -- Optimizing:  164/ 240\n",
            " -- Optimizing:  171/ 240\n",
            " -- Optimizing:  178/ 240\n",
            " -- Optimizing:  185/ 240\n",
            " -- Optimizing:  192/ 240\n",
            " -- Optimizing:  199/ 240\n",
            " -- Optimizing:  206/ 240\n",
            " -- Optimizing:  213/ 240\n",
            " -- Optimizing:  220/ 240\n",
            " -- Optimizing:  227/ 240\n",
            " -- Optimizing:  234/ 240\n",
            " -- max(err): 0.037706\n",
            " -- error_norm: 1.601266\n",
            " -- Quantization strategy:\n",
            " --   model.layers.0.self_attn                           4.3295 bpw - exp. error: 0.01978883\n",
            " --   model.layers.0.mlp                                 4.1977 bpw - exp. error: 0.03321034\n",
            " --   model.layers.1.self_attn                           6.3381 bpw - exp. error: 0.00763836\n",
            " --   model.layers.1.mlp                                 3.3655 bpw - exp. error: 0.01418861\n",
            " --   model.layers.2.self_attn                           2.2831 bpw - exp. error: 0.01536042\n",
            " --   model.layers.2.mlp                                 2.2449 bpw - exp. error: 0.03277579\n",
            " --   model.layers.3.self_attn                           4.3295 bpw - exp. error: 0.00608055\n",
            " --   model.layers.3.mlp                                 3.2833 bpw - exp. error: 0.02296937\n",
            " --   model.layers.4.self_attn                           4.2737 bpw - exp. error: 0.00847471\n",
            " --   model.layers.4.mlp                                 3.2833 bpw - exp. error: 0.02936308\n",
            " --   model.layers.5.self_attn                           4.1902 bpw - exp. error: 0.01077728\n",
            " --   model.layers.5.mlp                                 3.3655 bpw - exp. error: 0.03211206\n",
            " --   model.layers.6.self_attn                           4.3295 bpw - exp. error: 0.00954468\n",
            " --   model.layers.6.mlp                                 4.1368 bpw - exp. error: 0.02030586\n",
            " --   model.layers.7.self_attn                           4.1381 bpw - exp. error: 0.01364419\n",
            " --   model.layers.7.mlp                                 3.6186 bpw - exp. error: 0.03357182\n",
            " --   model.layers.8.self_attn                           4.3295 bpw - exp. error: 0.01259067\n",
            " --   model.layers.8.mlp                                 4.1977 bpw - exp. error: 0.02084731\n",
            " --   model.layers.9.self_attn                           4.3295 bpw - exp. error: 0.01358000\n",
            " --   model.layers.9.mlp                                 4.1977 bpw - exp. error: 0.02131250\n",
            " --   model.layers.10.self_attn                          4.3295 bpw - exp. error: 0.01360830\n",
            " --   model.layers.10.mlp                                4.1977 bpw - exp. error: 0.02177383\n",
            " --   model.layers.11.self_attn                          4.3295 bpw - exp. error: 0.01342180\n",
            " --   model.layers.11.mlp                                3.6186 bpw - exp. error: 0.03770568\n",
            " --   model.layers.12.self_attn                          4.3295 bpw - exp. error: 0.01425580\n",
            " --   model.layers.12.mlp                                4.1368 bpw - exp. error: 0.02543582\n",
            " --   model.layers.13.self_attn                          4.3295 bpw - exp. error: 0.01547474\n",
            " --   model.layers.13.mlp                                4.1977 bpw - exp. error: 0.02361256\n",
            " --   model.layers.14.self_attn                          4.3295 bpw - exp. error: 0.01618688\n",
            " --   model.layers.14.mlp                                4.1977 bpw - exp. error: 0.02512225\n",
            " --   model.layers.15.self_attn                          4.3295 bpw - exp. error: 0.01579214\n",
            " --   model.layers.15.mlp                                4.1977 bpw - exp. error: 0.02561679\n",
            " --   model.layers.16.self_attn                          4.3295 bpw - exp. error: 0.01357293\n",
            " --   model.layers.16.mlp                                4.1977 bpw - exp. error: 0.02482957\n",
            " --   model.layers.17.self_attn                          4.3295 bpw - exp. error: 0.01240720\n",
            " --   model.layers.17.mlp                                4.1977 bpw - exp. error: 0.02433812\n",
            " --   model.layers.18.self_attn                          4.3295 bpw - exp. error: 0.01002616\n",
            " --   model.layers.18.mlp                                4.1977 bpw - exp. error: 0.02303066\n",
            " --   model.layers.19.self_attn                          4.3295 bpw - exp. error: 0.00977488\n",
            " --   model.layers.19.mlp                                4.1977 bpw - exp. error: 0.02258451\n",
            " --   model.layers.20.self_attn                          4.3295 bpw - exp. error: 0.00934840\n",
            " --   model.layers.20.mlp                                4.1368 bpw - exp. error: 0.02459159\n",
            " --   model.layers.21.self_attn                          4.3295 bpw - exp. error: 0.01002851\n",
            " --   model.layers.21.mlp                                3.6186 bpw - exp. error: 0.03665657\n",
            " --   model.layers.22.self_attn                          4.2737 bpw - exp. error: 0.00999221\n",
            " --   model.layers.22.mlp                                3.6186 bpw - exp. error: 0.03545958\n",
            " --   model.layers.23.self_attn                          4.3295 bpw - exp. error: 0.00830616\n",
            " --   model.layers.23.mlp                                4.1977 bpw - exp. error: 0.02034540\n",
            " --   model.layers.24.self_attn                          4.2737 bpw - exp. error: 0.00955446\n",
            " --   model.layers.24.mlp                                4.1368 bpw - exp. error: 0.02181959\n",
            " --   model.layers.25.self_attn                          4.2737 bpw - exp. error: 0.00908906\n",
            " --   model.layers.25.mlp                                4.1977 bpw - exp. error: 0.02022674\n",
            " --   model.layers.26.self_attn                          4.3295 bpw - exp. error: 0.00868687\n",
            " --   model.layers.26.mlp                                3.6186 bpw - exp. error: 0.03463363\n",
            " --   model.layers.27.self_attn                          4.3295 bpw - exp. error: 0.00980090\n",
            " --   model.layers.27.mlp                                4.1977 bpw - exp. error: 0.02023600\n",
            " --   model.layers.28.self_attn                          4.3295 bpw - exp. error: 0.01079208\n",
            " --   model.layers.28.mlp                                4.1368 bpw - exp. error: 0.02335993\n",
            " --   model.layers.29.self_attn                          4.3295 bpw - exp. error: 0.01053458\n",
            " --   model.layers.29.mlp                                4.1368 bpw - exp. error: 0.02492736\n",
            " --   model.layers.30.self_attn                          4.3295 bpw - exp. error: 0.01127859\n",
            " --   model.layers.30.mlp                                4.1977 bpw - exp. error: 0.02556776\n",
            " --   model.layers.31.self_attn                          4.3295 bpw - exp. error: 0.01207182\n",
            " --   model.layers.31.mlp                                4.1977 bpw - exp. error: 0.03159384\n",
            " -- sum(log(err)): -261.070198\n",
            " -- max(err): 0.037706\n",
            " -- Tokenizing samples...\n",
            " -- First 50 tokens of dataset:\n",
            "    ' = Robert Boulter = \\n Robert Boulter is an English film , television and theatre actor . He had a guest @-@ starring role on the television series The Bill in 2000 . This was followed by a starring role in the'\n",
            " -- Last 50 tokens of dataset:\n",
            "    ' could return , receive reinforcements , and quell the rebellion in 61 . Fearing Paulinus himself would provoke further rebellion , Nero replaced him with the more passive Publius Petronius <unk> . \\n The Pisonian Conspiracy of 65'\n",
            " -- Token embeddings again...\n",
            " -- Quantizing...\n",
            " -- Layer: model.layers.0 (Attention)\n",
            " -- Linear: model.layers.0.self_attn.q_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.0.self_attn.k_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.0.self_attn.v_proj -> 1:5b_32g s4, 5.16 bpw\n",
            " -- Linear: model.layers.0.self_attn.o_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Module quantized, rfn_error: 0.022055\n",
            " -- Layer: model.layers.0 (MLP)\n",
            " -- Linear: model.layers.0.mlp.gate_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.0.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.0.mlp.down_proj -> 0.05:8b_32g/0.95:4b_32g s4, 4.34 bpw\n",
            " -- Module quantized, rfn_error: 0.042142\n",
            " -- Layer: model.layers.1 (Attention)\n",
            " -- Linear: model.layers.1.self_attn.q_proj -> 1:6b_32g s4, 6.13 bpw\n",
            " -- Linear: model.layers.1.self_attn.k_proj -> 1:6b_32g s4, 6.16 bpw\n",
            " -- Linear: model.layers.1.self_attn.v_proj -> 1:8b_32g s4, 8.16 bpw\n",
            " -- Linear: model.layers.1.self_attn.o_proj -> 1:6b_32g s4, 6.13 bpw\n",
            " -- Module quantized, rfn_error: 0.008429\n",
            " -- Layer: model.layers.1 (MLP)\n",
            " -- Linear: model.layers.1.mlp.gate_proj -> 0.1:4b_32g/0.9:3b_32g s4, 3.23 bpw\n",
            " -- Linear: model.layers.1.mlp.up_proj -> 0.25:4b_32g/0.75:3b_32g s4, 3.38 bpw\n",
            " -- Linear: model.layers.1.mlp.down_proj -> 0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4, 3.49 bpw\n",
            " -- Module quantized, rfn_error: 0.015733\n",
            " -- Layer: model.layers.2 (Attention)\n",
            " -- Linear: model.layers.2.self_attn.q_proj -> 0.1:3b_64g/0.9:2b_64g s4, 2.18 bpw\n",
            " -- Linear: model.layers.2.self_attn.k_proj -> 0.1:3b_64g/0.9:2b_64g s4, 2.20 bpw\n",
            " -- Linear: model.layers.2.self_attn.v_proj -> 0.1:4b_128g/0.9:3b_128g s4, 3.19 bpw\n",
            " -- Linear: model.layers.2.self_attn.o_proj -> 0.1:3b_64g/0.9:2b_64g s4, 2.18 bpw\n",
            " -- Module quantized, rfn_error: 0.016169\n",
            " -- Layer: model.layers.2 (MLP)\n",
            " -- Linear: model.layers.2.mlp.gate_proj -> 0.05:3b_64g/0.95:2b_64g s4, 2.13 bpw\n",
            " -- Linear: model.layers.2.mlp.up_proj -> 0.05:3b_64g/0.95:2b_64g s4, 2.13 bpw\n",
            " -- Linear: model.layers.2.mlp.down_proj -> 0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4, 2.48 bpw\n",
            " -- Module quantized, rfn_error: 0.036977\n",
            " -- Layer: model.layers.3 (Attention)\n",
            " -- Linear: model.layers.3.self_attn.q_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.3.self_attn.k_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.3.self_attn.v_proj -> 1:5b_32g s4, 5.16 bpw\n",
            " -- Linear: model.layers.3.self_attn.o_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Module quantized, rfn_error: 0.006122\n",
            " -- Layer: model.layers.3 (MLP)\n",
            " -- Linear: model.layers.3.mlp.gate_proj -> 0.1:4b_128g/0.9:3b_128g s4, 3.16 bpw\n",
            " -- Linear: model.layers.3.mlp.up_proj -> 0.25:4b_128g/0.75:3b_128g s4, 3.28 bpw\n",
            " -- Linear: model.layers.3.mlp.down_proj -> 0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4, 3.41 bpw\n",
            " -- Module quantized, rfn_error: 0.026078\n",
            " -- Layer: model.layers.4 (Attention)\n",
            " -- Linear: model.layers.4.self_attn.q_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.18 bpw\n",
            " -- Linear: model.layers.4.self_attn.k_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.20 bpw\n",
            " -- Linear: model.layers.4.self_attn.v_proj -> 1:5b_64g s4, 5.09 bpw\n",
            " -- Linear: model.layers.4.self_attn.o_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.18 bpw\n",
            " -- Module quantized, rfn_error: 0.008733\n",
            " -- Layer: model.layers.4 (MLP)\n",
            " -- Linear: model.layers.4.mlp.gate_proj -> 0.1:4b_128g/0.9:3b_128g s4, 3.16 bpw\n",
            " -- Linear: model.layers.4.mlp.up_proj -> 0.25:4b_128g/0.75:3b_128g s4, 3.28 bpw\n",
            " -- Linear: model.layers.4.mlp.down_proj -> 0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4, 3.41 bpw\n",
            " -- Module quantized, rfn_error: 0.033504\n",
            " -- Layer: model.layers.5 (Attention)\n",
            " -- Linear: model.layers.5.self_attn.q_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.18 bpw\n",
            " -- Linear: model.layers.5.self_attn.k_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.20 bpw\n",
            " -- Linear: model.layers.5.self_attn.v_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.5.self_attn.o_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.18 bpw\n",
            " -- Module quantized, rfn_error: 0.011402\n",
            " -- Layer: model.layers.5 (MLP)\n",
            " -- Linear: model.layers.5.mlp.gate_proj -> 0.1:4b_32g/0.9:3b_32g s4, 3.23 bpw\n",
            " -- Linear: model.layers.5.mlp.up_proj -> 0.25:4b_32g/0.75:3b_32g s4, 3.38 bpw\n",
            " -- Linear: model.layers.5.mlp.down_proj -> 0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4, 3.49 bpw\n",
            " -- Module quantized, rfn_error: 0.036816\n",
            " -- Layer: model.layers.6 (Attention)\n",
            " -- Linear: model.layers.6.self_attn.q_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.6.self_attn.k_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.6.self_attn.v_proj -> 1:5b_32g s4, 5.16 bpw\n",
            " -- Linear: model.layers.6.self_attn.o_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Module quantized, rfn_error: 0.010506\n",
            " -- Layer: model.layers.6 (MLP)\n",
            " -- Linear: model.layers.6.mlp.gate_proj -> 1:4b_128g s4, 4.03 bpw\n",
            " -- Linear: model.layers.6.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.6.mlp.down_proj -> 0.05:8b_32g/0.95:4b_128g s4, 4.25 bpw\n",
            " -- Module quantized, rfn_error: 0.023565\n",
            " -- Saving checkpoint...\n",
            " -- Layer: model.layers.7 (Attention)\n",
            " -- Linear: model.layers.7.self_attn.q_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.7.self_attn.k_proj -> 1:4b_32g s4, 4.16 bpw\n",
            " -- Linear: model.layers.7.self_attn.v_proj -> 1:4b_32g s4, 4.16 bpw\n",
            " -- Linear: model.layers.7.self_attn.o_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Module quantized, rfn_error: 0.015039\n",
            " -- Layer: model.layers.7 (MLP)\n",
            " -- Linear: model.layers.7.mlp.gate_proj -> 0.1:4b_32g/0.9:3b_32g s4, 3.23 bpw\n",
            " -- Linear: model.layers.7.mlp.up_proj -> 0.25:4b_32g/0.75:3b_32g s4, 3.38 bpw\n",
            " -- Linear: model.layers.7.mlp.down_proj -> 0.05:8b_32g/0.95:4b_128g s4, 4.25 bpw\n",
            " -- Module quantized, rfn_error: 0.037613\n",
            " -- Layer: model.layers.8 (Attention)\n",
            " -- Linear: model.layers.8.self_attn.q_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.8.self_attn.k_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.8.self_attn.v_proj -> 1:5b_32g s4, 5.16 bpw\n",
            " -- Linear: model.layers.8.self_attn.o_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Module quantized, rfn_error: 0.013875\n",
            " -- Layer: model.layers.8 (MLP)\n",
            " -- Linear: model.layers.8.mlp.gate_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.8.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.8.mlp.down_proj -> 0.05:8b_32g/0.95:4b_32g s4, 4.34 bpw\n",
            " -- Module quantized, rfn_error: 0.023998\n",
            " -- Layer: model.layers.9 (Attention)\n",
            " -- Linear: model.layers.9.self_attn.q_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.9.self_attn.k_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.9.self_attn.v_proj -> 1:5b_32g s4, 5.16 bpw\n",
            " -- Linear: model.layers.9.self_attn.o_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Module quantized, rfn_error: 0.015128\n",
            " -- Layer: model.layers.9 (MLP)\n",
            " -- Linear: model.layers.9.mlp.gate_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.9.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.9.mlp.down_proj -> 0.05:8b_32g/0.95:4b_32g s4, 4.34 bpw\n",
            " -- Module quantized, rfn_error: 0.024687\n",
            " -- Layer: model.layers.10 (Attention)\n",
            " -- Linear: model.layers.10.self_attn.q_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.10.self_attn.k_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.10.self_attn.v_proj -> 1:5b_32g s4, 5.16 bpw\n",
            " -- Linear: model.layers.10.self_attn.o_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Module quantized, rfn_error: 0.015005\n",
            " -- Layer: model.layers.10 (MLP)\n",
            " -- Linear: model.layers.10.mlp.gate_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.10.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.10.mlp.down_proj -> 0.05:8b_32g/0.95:4b_32g s4, 4.34 bpw\n",
            " -- Module quantized, rfn_error: 0.025168\n",
            " -- Layer: model.layers.11 (Attention)\n",
            " -- Linear: model.layers.11.self_attn.q_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.11.self_attn.k_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.11.self_attn.v_proj -> 1:5b_32g s4, 5.16 bpw\n",
            " -- Linear: model.layers.11.self_attn.o_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Module quantized, rfn_error: 0.014942\n",
            " -- Layer: model.layers.11 (MLP)\n",
            " -- Linear: model.layers.11.mlp.gate_proj -> 0.1:4b_32g/0.9:3b_32g s4, 3.23 bpw\n",
            " -- Linear: model.layers.11.mlp.up_proj -> 0.25:4b_32g/0.75:3b_32g s4, 3.38 bpw\n",
            " -- Linear: model.layers.11.mlp.down_proj -> 0.05:8b_32g/0.95:4b_128g s4, 4.25 bpw\n",
            " -- Module quantized, rfn_error: 0.042181\n",
            " -- Layer: model.layers.12 (Attention)\n",
            " -- Linear: model.layers.12.self_attn.q_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.12.self_attn.k_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.12.self_attn.v_proj -> 1:5b_32g s4, 5.16 bpw\n",
            " -- Linear: model.layers.12.self_attn.o_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Module quantized, rfn_error: 0.015765\n",
            " -- Layer: model.layers.12 (MLP)\n",
            " -- Linear: model.layers.12.mlp.gate_proj -> 1:4b_128g s4, 4.03 bpw\n",
            " -- Linear: model.layers.12.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.12.mlp.down_proj -> 0.05:8b_32g/0.95:4b_128g s4, 4.25 bpw\n",
            " -- Module quantized, rfn_error: 0.029234\n",
            " -- Layer: model.layers.13 (Attention)\n",
            " -- Linear: model.layers.13.self_attn.q_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.13.self_attn.k_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.13.self_attn.v_proj -> 1:5b_32g s4, 5.16 bpw\n",
            " -- Linear: model.layers.13.self_attn.o_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Module quantized, rfn_error: 0.017291\n",
            " -- Saving checkpoint...\n",
            " -- Layer: model.layers.13 (MLP)\n",
            " -- Linear: model.layers.13.mlp.gate_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.13.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.13.mlp.down_proj -> 0.05:8b_32g/0.95:4b_32g s4, 4.34 bpw\n",
            " -- Module quantized, rfn_error: 0.027239\n",
            " -- Layer: model.layers.14 (Attention)\n",
            " -- Linear: model.layers.14.self_attn.q_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.14.self_attn.k_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.14.self_attn.v_proj -> 1:5b_32g s4, 5.16 bpw\n",
            " -- Linear: model.layers.14.self_attn.o_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Module quantized, rfn_error: 0.017678\n",
            " -- Layer: model.layers.14 (MLP)\n",
            " -- Linear: model.layers.14.mlp.gate_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.14.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.14.mlp.down_proj -> 0.05:8b_32g/0.95:4b_32g s4, 4.34 bpw\n",
            " -- Module quantized, rfn_error: 0.029004\n",
            " -- Layer: model.layers.15 (Attention)\n",
            " -- Linear: model.layers.15.self_attn.q_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.15.self_attn.k_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.15.self_attn.v_proj -> 1:5b_32g s4, 5.16 bpw\n",
            " -- Linear: model.layers.15.self_attn.o_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Module quantized, rfn_error: 0.017675\n",
            " -- Layer: model.layers.15 (MLP)\n",
            " -- Linear: model.layers.15.mlp.gate_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.15.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.15.mlp.down_proj -> 0.05:8b_32g/0.95:4b_32g s4, 4.34 bpw\n",
            " -- Module quantized, rfn_error: 0.029795\n",
            " -- Layer: model.layers.16 (Attention)\n",
            " -- Linear: model.layers.16.self_attn.q_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.16.self_attn.k_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.16.self_attn.v_proj -> 1:5b_32g s4, 5.16 bpw\n",
            " -- Linear: model.layers.16.self_attn.o_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Module quantized, rfn_error: 0.015008\n",
            " -- Layer: model.layers.16 (MLP)\n",
            " -- Linear: model.layers.16.mlp.gate_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.16.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.16.mlp.down_proj -> 0.05:8b_32g/0.95:4b_32g s4, 4.34 bpw\n",
            " -- Module quantized, rfn_error: 0.029075\n",
            " -- Layer: model.layers.17 (Attention)\n",
            " -- Linear: model.layers.17.self_attn.q_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.17.self_attn.k_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.17.self_attn.v_proj -> 1:5b_32g s4, 5.16 bpw\n",
            " -- Linear: model.layers.17.self_attn.o_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Module quantized, rfn_error: 0.014033\n",
            " -- Layer: model.layers.17 (MLP)\n",
            " -- Linear: model.layers.17.mlp.gate_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.17.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.17.mlp.down_proj -> 0.05:8b_32g/0.95:4b_32g s4, 4.34 bpw\n",
            " -- Module quantized, rfn_error: 0.028609\n",
            " -- Layer: model.layers.18 (Attention)\n",
            " -- Linear: model.layers.18.self_attn.q_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.18.self_attn.k_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.18.self_attn.v_proj -> 1:5b_32g s4, 5.16 bpw\n",
            " -- Linear: model.layers.18.self_attn.o_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Module quantized, rfn_error: 0.011492\n",
            " -- Layer: model.layers.18 (MLP)\n",
            " -- Linear: model.layers.18.mlp.gate_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.18.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.18.mlp.down_proj -> 0.05:8b_32g/0.95:4b_32g s4, 4.34 bpw\n",
            " -- Module quantized, rfn_error: 0.027222\n",
            " -- Layer: model.layers.19 (Attention)\n",
            " -- Linear: model.layers.19.self_attn.q_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.19.self_attn.k_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.19.self_attn.v_proj -> 1:5b_32g s4, 5.16 bpw\n",
            " -- Linear: model.layers.19.self_attn.o_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Module quantized, rfn_error: 0.010578\n",
            " -- Layer: model.layers.19 (MLP)\n",
            " -- Linear: model.layers.19.mlp.gate_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.19.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.19.mlp.down_proj -> 0.05:8b_32g/0.95:4b_32g s4, 4.34 bpw\n",
            " -- Module quantized, rfn_error: 0.026704\n",
            " -- Saving checkpoint...\n",
            " -- Layer: model.layers.20 (Attention)\n",
            " -- Linear: model.layers.20.self_attn.q_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.20.self_attn.k_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.20.self_attn.v_proj -> 1:5b_32g s4, 5.16 bpw\n",
            " -- Linear: model.layers.20.self_attn.o_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Module quantized, rfn_error: 0.010312\n",
            " -- Layer: model.layers.20 (MLP)\n",
            " -- Linear: model.layers.20.mlp.gate_proj -> 1:4b_128g s4, 4.03 bpw\n",
            " -- Linear: model.layers.20.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.20.mlp.down_proj -> 0.05:8b_32g/0.95:4b_128g s4, 4.25 bpw\n",
            " -- Module quantized, rfn_error: 0.029147\n",
            " -- Layer: model.layers.21 (Attention)\n",
            " -- Linear: model.layers.21.self_attn.q_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.21.self_attn.k_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.21.self_attn.v_proj -> 1:5b_32g s4, 5.16 bpw\n",
            " -- Linear: model.layers.21.self_attn.o_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Module quantized, rfn_error: 0.011155\n",
            " -- Layer: model.layers.21 (MLP)\n",
            " -- Linear: model.layers.21.mlp.gate_proj -> 0.1:4b_32g/0.9:3b_32g s4, 3.23 bpw\n",
            " -- Linear: model.layers.21.mlp.up_proj -> 0.25:4b_32g/0.75:3b_32g s4, 3.38 bpw\n",
            " -- Linear: model.layers.21.mlp.down_proj -> 0.05:8b_32g/0.95:4b_128g s4, 4.25 bpw\n",
            " -- Module quantized, rfn_error: 0.042078\n",
            " -- Layer: model.layers.22 (Attention)\n",
            " -- Linear: model.layers.22.self_attn.q_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.18 bpw\n",
            " -- Linear: model.layers.22.self_attn.k_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.20 bpw\n",
            " -- Linear: model.layers.22.self_attn.v_proj -> 1:5b_64g s4, 5.09 bpw\n",
            " -- Linear: model.layers.22.self_attn.o_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.18 bpw\n",
            " -- Module quantized, rfn_error: 0.010717\n",
            " -- Layer: model.layers.22 (MLP)\n",
            " -- Linear: model.layers.22.mlp.gate_proj -> 0.1:4b_32g/0.9:3b_32g s4, 3.23 bpw\n",
            " -- Linear: model.layers.22.mlp.up_proj -> 0.25:4b_32g/0.75:3b_32g s4, 3.38 bpw\n",
            " -- Linear: model.layers.22.mlp.down_proj -> 0.05:8b_32g/0.95:4b_128g s4, 4.25 bpw\n",
            " -- Module quantized, rfn_error: 0.040661\n",
            " -- Layer: model.layers.23 (Attention)\n",
            " -- Linear: model.layers.23.self_attn.q_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.23.self_attn.k_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.23.self_attn.v_proj -> 1:5b_32g s4, 5.16 bpw\n",
            " -- Linear: model.layers.23.self_attn.o_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Module quantized, rfn_error: 0.009252\n",
            " -- Layer: model.layers.23 (MLP)\n",
            " -- Linear: model.layers.23.mlp.gate_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.23.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.23.mlp.down_proj -> 0.05:8b_32g/0.95:4b_32g s4, 4.34 bpw\n",
            " -- Module quantized, rfn_error: 0.024093\n",
            " -- Layer: model.layers.24 (Attention)\n",
            " -- Linear: model.layers.24.self_attn.q_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.18 bpw\n",
            " -- Linear: model.layers.24.self_attn.k_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.20 bpw\n",
            " -- Linear: model.layers.24.self_attn.v_proj -> 1:5b_64g s4, 5.09 bpw\n",
            " -- Linear: model.layers.24.self_attn.o_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.18 bpw\n",
            " -- Module quantized, rfn_error: 0.010322\n",
            " -- Layer: model.layers.24 (MLP)\n",
            " -- Linear: model.layers.24.mlp.gate_proj -> 1:4b_128g s4, 4.03 bpw\n",
            " -- Linear: model.layers.24.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.24.mlp.down_proj -> 0.05:8b_32g/0.95:4b_128g s4, 4.25 bpw\n",
            " -- Module quantized, rfn_error: 0.025984\n",
            " -- Layer: model.layers.25 (Attention)\n",
            " -- Linear: model.layers.25.self_attn.q_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.18 bpw\n",
            " -- Linear: model.layers.25.self_attn.k_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.20 bpw\n",
            " -- Linear: model.layers.25.self_attn.v_proj -> 1:5b_64g s4, 5.09 bpw\n",
            " -- Linear: model.layers.25.self_attn.o_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.18 bpw\n",
            " -- Module quantized, rfn_error: 0.010296\n",
            " -- Layer: model.layers.25 (MLP)\n",
            " -- Linear: model.layers.25.mlp.gate_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.25.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.25.mlp.down_proj -> 0.05:8b_32g/0.95:4b_32g s4, 4.34 bpw\n",
            " -- Module quantized, rfn_error: 0.023921\n",
            " -- Layer: model.layers.26 (Attention)\n",
            " -- Linear: model.layers.26.self_attn.q_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.26.self_attn.k_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.26.self_attn.v_proj -> 1:5b_32g s4, 5.16 bpw\n",
            " -- Linear: model.layers.26.self_attn.o_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Module quantized, rfn_error: 0.010014\n",
            " -- Layer: model.layers.26 (MLP)\n",
            " -- Linear: model.layers.26.mlp.gate_proj -> 0.1:4b_32g/0.9:3b_32g s4, 3.23 bpw\n",
            " -- Linear: model.layers.26.mlp.up_proj -> 0.25:4b_32g/0.75:3b_32g s4, 3.38 bpw\n",
            " -- Linear: model.layers.26.mlp.down_proj -> 0.05:8b_32g/0.95:4b_128g s4, 4.25 bpw\n",
            " -- Module quantized, rfn_error: 0.039662\n",
            " -- Saving checkpoint...\n",
            " -- Layer: model.layers.27 (Attention)\n",
            " -- Linear: model.layers.27.self_attn.q_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.27.self_attn.k_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.27.self_attn.v_proj -> 1:5b_32g s4, 5.16 bpw\n",
            " -- Linear: model.layers.27.self_attn.o_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Module quantized, rfn_error: 0.011067\n",
            " -- Layer: model.layers.27 (MLP)\n",
            " -- Linear: model.layers.27.mlp.gate_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.27.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.27.mlp.down_proj -> 0.05:8b_32g/0.95:4b_32g s4, 4.34 bpw\n",
            " -- Module quantized, rfn_error: 0.023752\n",
            " -- Layer: model.layers.28 (Attention)\n",
            " -- Linear: model.layers.28.self_attn.q_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.28.self_attn.k_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.28.self_attn.v_proj -> 1:5b_32g s4, 5.16 bpw\n",
            " -- Linear: model.layers.28.self_attn.o_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Module quantized, rfn_error: 0.011938\n",
            " -- Layer: model.layers.28 (MLP)\n",
            " -- Linear: model.layers.28.mlp.gate_proj -> 1:4b_128g s4, 4.03 bpw\n",
            " -- Linear: model.layers.28.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.28.mlp.down_proj -> 0.05:8b_32g/0.95:4b_128g s4, 4.25 bpw\n",
            " -- Module quantized, rfn_error: 0.027370\n",
            " -- Layer: model.layers.29 (Attention)\n",
            " -- Linear: model.layers.29.self_attn.q_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.29.self_attn.k_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.29.self_attn.v_proj -> 1:5b_32g s4, 5.16 bpw\n",
            " -- Linear: model.layers.29.self_attn.o_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Module quantized, rfn_error: 0.011517\n",
            " -- Layer: model.layers.29 (MLP)\n",
            " -- Linear: model.layers.29.mlp.gate_proj -> 1:4b_128g s4, 4.03 bpw\n",
            " -- Linear: model.layers.29.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.29.mlp.down_proj -> 0.05:8b_32g/0.95:4b_128g s4, 4.25 bpw\n",
            " -- Module quantized, rfn_error: 0.028954\n",
            " -- Layer: model.layers.30 (Attention)\n",
            " -- Linear: model.layers.30.self_attn.q_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.30.self_attn.k_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.30.self_attn.v_proj -> 1:5b_32g s4, 5.16 bpw\n",
            " -- Linear: model.layers.30.self_attn.o_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Module quantized, rfn_error: 0.012599\n",
            " -- Layer: model.layers.30 (MLP)\n",
            " -- Linear: model.layers.30.mlp.gate_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.30.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.30.mlp.down_proj -> 0.05:8b_32g/0.95:4b_32g s4, 4.34 bpw\n",
            " -- Module quantized, rfn_error: 0.029289\n",
            " -- Layer: model.layers.31 (Attention)\n",
            " -- Linear: model.layers.31.self_attn.q_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.31.self_attn.k_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.31.self_attn.v_proj -> 1:5b_32g s4, 5.16 bpw\n",
            " -- Linear: model.layers.31.self_attn.o_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Module quantized, rfn_error: 0.013061\n",
            " -- Layer: model.layers.31 (MLP)\n",
            " -- Linear: model.layers.31.mlp.gate_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.31.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.31.mlp.down_proj -> 0.05:8b_32g/0.95:4b_32g s4, 4.34 bpw\n",
            " -- Module quantized, rfn_error: 0.035792\n",
            " -- Layer: model.norm (RMSNorm)\n",
            " -- Module quantized, rfn_error: 0.000000\n",
            " -- Layer: lm_head (Linear)\n",
            " -- Linear: lm_head -> 0.15:8b_128g/0.85:6b_128g s4, 6.34 bpw\n",
            " -- Module quantized, calibration perplexity (quant): 8.1003\n",
            " -- Saving checkpoint...\n",
            " -- Compiling output file...\n",
            " -- Writing shard 1...\n",
            " --   quant/output.safetensors (4,727 MB)\n",
            " -- Finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy files\n",
        "!rm -rf quant/out_tensor\n",
        "!rsync -av --exclude='*.safetensors' --exclude='.*' ./base_model/ ./quant/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S023p_fMpyMF",
        "outputId": "80bcb2aa-6243-4394-bfbb-0e7acf478089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sending incremental file list\n",
            "./\n",
            "README.md\n",
            "config.json\n",
            "generation_config.json\n",
            "model.safetensors.index.json\n",
            "special_tokens_map.json\n",
            "tokenizer.json\n",
            "tokenizer_config.json\n",
            "\n",
            "sent 9,173,563 bytes  received 152 bytes  18,347,430.00 bytes/sec\n",
            "total size is 9,170,766  speedup is 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-DhHYxRqmf4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run model\n",
        "!python exllamav2/test_inference.py -m quant/ -p \"I am a Filipino and\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cqy2lCTp3j1",
        "outputId": "0ca4a809-052e-41e0-ebbe-ca19fadb2199"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " -- Model: quant/\n",
            " -- Options: []\n",
            " -- Loading model...\n",
            " -- Loaded model in 2.1017 seconds\n",
            " -- Loading tokenizer...\n",
            " -- Warmup...\n",
            " -- Generating...\n",
            "\n",
            "I am a Filipino and my native language is Tagalog.\n",
            "I love to write and share my thoughts and experiences with others. I also enjoy reading books, watching movies, and learning new things.\n",
            "I am a devout Catholic and I strive to live my faith every day.\n",
            "I am a member of the Filipino community here in the United States and I am proud to be part of it.\n",
            "I hope that through this blog, I can share my culture, my faith, and my experiences with others and learn from them as well.\n",
            "I am excited to start this journey and I hope you will join me along the way. Mabuhay! (Long live!)\n",
            "\n",
            " -- Response generated in 2.53 seconds, 128 tokens, 50.59 tokens/second (includes prompt eval.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Download dataset\n",
        "!wget https://huggingface.co/datasets/wikitext/resolve/9a9e482b5987f9d25b3a9b2883fc6cc9fd8071b3/wikitext-103-v1/wikitext-test.parquet\n",
        "\n",
        "# Quantize model\n",
        "save_folder = \"/content/Daredevil-8B\" + \"-EXL2\"\n",
        "!mkdir {save_folder}\n",
        "!python exllamav2/convert.py \\\n",
        "    -i base_model \\\n",
        "    -o {save_folder} \\\n",
        "    -c wikitext-test.parquet \\\n",
        "    -b {BPW}\n",
        "\n",
        "# Copy files\n",
        "!rm -rf quant/out_tensor\n",
        "!rsync -av --exclude='*.safetensors' --exclude='.*' ./base_model/ ./{save_folder}/\n",
        "\n",
        "# Create model card\n",
        "card = ModelCard.load(MODEL_ID)\n",
        "card.data.tags.append(\"autoquant\")\n",
        "card.data.tags.append(\"exl2\")\n",
        "card.save(f'{save_folder}/README.md')\n",
        "\n",
        "# Upload model\n",
        "create_repo(\n",
        "    repo_id = f\"{USERNAME}/{MODEL_NAME}-{BPW:.1f}bpw-exl2\",\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True,\n",
        "    token=hf_token\n",
        ")\n",
        "api.upload_folder(\n",
        "    folder_path=save_folder,\n",
        "    repo_id=f\"{USERNAME}/{MODEL_NAME}-{BPW:.1f}bpw-exl2\",\n",
        "    token=hf_token\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_Y73BS3bf0J",
        "outputId": "a04d15c4-e087-4da4-a047-cd93c50d72e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Daredevil-8B  exllamav2  mlabonne  sample_data\twikitext-test.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lmwhGirEdCkG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f66187b44ddb42a5a1fad3146c3f6255": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d75ac4d7862a408b8f2980cce8e47436",
              "IPY_MODEL_ff5fe59fe9b64175907204407891580f",
              "IPY_MODEL_54ca6f77540d435cb3e7f118c5b185e9"
            ],
            "layout": "IPY_MODEL_5cced1e3e2804a61b041441bfa7e4547"
          }
        },
        "d75ac4d7862a408b8f2980cce8e47436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_892492a70bbf482c910bd3dddf1be4c9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_94d0820a35e242738f47defdf739d07f",
            "value": "tokenizer_config.json:‚Äá100%"
          }
        },
        "ff5fe59fe9b64175907204407891580f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e18818553e74f83b8b1ac9e9e9565c6",
            "max": 50982,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae0c76b28463407b8e707318a8d825e8",
            "value": 50982
          }
        },
        "54ca6f77540d435cb3e7f118c5b185e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b18f8160818640a8b7d360270eaaaa87",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_33f5fcf78a014d2290e89cd07cebe113",
            "value": "‚Äá51.0k/51.0k‚Äá[00:00&lt;00:00,‚Äá3.61MB/s]"
          }
        },
        "5cced1e3e2804a61b041441bfa7e4547": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "892492a70bbf482c910bd3dddf1be4c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94d0820a35e242738f47defdf739d07f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e18818553e74f83b8b1ac9e9e9565c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae0c76b28463407b8e707318a8d825e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b18f8160818640a8b7d360270eaaaa87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33f5fcf78a014d2290e89cd07cebe113": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28efaf5a401c4606b30c437456d5f019": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6894289abfb5403fb0ac83612b2ad499",
              "IPY_MODEL_c831dac9f0a9472584ee379265ddb69d",
              "IPY_MODEL_051ff396d2304091b91627e158c30ca5"
            ],
            "layout": "IPY_MODEL_44567038202a4f8098231b6377ff0f44"
          }
        },
        "6894289abfb5403fb0ac83612b2ad499": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_065a684f0456409195c6acde7bde4363",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ad2cc9af12d84d03b7d82607d8e4bbca",
            "value": "tokenizer.json:‚Äá100%"
          }
        },
        "c831dac9f0a9472584ee379265ddb69d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c836fd2ac4440d9adbc5f1e4ae5c136",
            "max": 9085698,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fd7dabb983aa47cf92d7df00741befa1",
            "value": 9085698
          }
        },
        "051ff396d2304091b91627e158c30ca5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c4b9eeb1e4145b3949608ca3b35db5e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6a14d90e4e8048a580ce74f1aff99654",
            "value": "‚Äá9.09M/9.09M‚Äá[00:01&lt;00:00,‚Äá6.12MB/s]"
          }
        },
        "44567038202a4f8098231b6377ff0f44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "065a684f0456409195c6acde7bde4363": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad2cc9af12d84d03b7d82607d8e4bbca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c836fd2ac4440d9adbc5f1e4ae5c136": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd7dabb983aa47cf92d7df00741befa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9c4b9eeb1e4145b3949608ca3b35db5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a14d90e4e8048a580ce74f1aff99654": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd1c9720d9d1414586a2e5a038385430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9487b9b30e6746fdaa516f343cda3e95",
              "IPY_MODEL_1d1f3611b7a946788e69b7318f6c29c1",
              "IPY_MODEL_5134c16e4d43433f987a13efe7d4e7b9"
            ],
            "layout": "IPY_MODEL_0f2aab6c99ca4696a59995f058b8622b"
          }
        },
        "9487b9b30e6746fdaa516f343cda3e95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66cddb44f9e9481382734e6cb019fa72",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3fca8d896cb5464a8aa96346f290fae3",
            "value": "special_tokens_map.json:‚Äá100%"
          }
        },
        "1d1f3611b7a946788e69b7318f6c29c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9f8941cf88449ab8061dae64143cc20",
            "max": 301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af7bab9696af41c69a50fe9ea7bcff45",
            "value": 301
          }
        },
        "5134c16e4d43433f987a13efe7d4e7b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a42beabe0274544bd580e0ead386c65",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2be49d11d3a94a6faa8d8a662e3fdf1d",
            "value": "‚Äá301/301‚Äá[00:00&lt;00:00,‚Äá18.2kB/s]"
          }
        },
        "0f2aab6c99ca4696a59995f058b8622b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66cddb44f9e9481382734e6cb019fa72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fca8d896cb5464a8aa96346f290fae3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9f8941cf88449ab8061dae64143cc20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af7bab9696af41c69a50fe9ea7bcff45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a42beabe0274544bd580e0ead386c65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2be49d11d3a94a6faa8d8a662e3fdf1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2d6c7e9167044bd84aef59d602fe6ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e86e364e66f24102b18a345f02bb0f2d",
              "IPY_MODEL_30a806bb1129444a8742bd6d69891899",
              "IPY_MODEL_d95ff3f8c428407584188b0c2d63787d"
            ],
            "layout": "IPY_MODEL_9f534b44470549efa1d5de2bc6dd0989"
          }
        },
        "e86e364e66f24102b18a345f02bb0f2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd5a2da6c35f4ac4843b03bcae8afedd",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_38f90f23c3034f5b8595368d3c24359e",
            "value": "config.json:‚Äá100%"
          }
        },
        "30a806bb1129444a8742bd6d69891899": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a38db6145784b659df1115dc78417b2",
            "max": 714,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d6760b2b05594cb49ff5e40ac5982434",
            "value": 714
          }
        },
        "d95ff3f8c428407584188b0c2d63787d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1ccdbbf728e4a1aad7e964124041107",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fb84fb7dac284521bfb36c401bed74af",
            "value": "‚Äá714/714‚Äá[00:00&lt;00:00,‚Äá60.2kB/s]"
          }
        },
        "9f534b44470549efa1d5de2bc6dd0989": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd5a2da6c35f4ac4843b03bcae8afedd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38f90f23c3034f5b8595368d3c24359e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a38db6145784b659df1115dc78417b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6760b2b05594cb49ff5e40ac5982434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1ccdbbf728e4a1aad7e964124041107": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb84fb7dac284521bfb36c401bed74af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52d815c0305f4a108d7013d0d5a3ce9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1a45ad46f4b54a0cb138e125beac37a2",
              "IPY_MODEL_068cb915f608429396dc4f512422643b",
              "IPY_MODEL_4dc4a42e2fd14d299bcf4f2f9cd33c51"
            ],
            "layout": "IPY_MODEL_79e35a6316ba4644b9ea6ef0e4aae200"
          }
        },
        "1a45ad46f4b54a0cb138e125beac37a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad628a6218ec4b63bd22c27bd2d682de",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f7b1267fea1a4c90a4e16346d7fdeee1",
            "value": "model.safetensors.index.json:‚Äá100%"
          }
        },
        "068cb915f608429396dc4f512422643b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b433f4bf3144c41b677167b9f81e7c7",
            "max": 23950,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b67f1e7d1c3468682f9160d83865332",
            "value": 23950
          }
        },
        "4dc4a42e2fd14d299bcf4f2f9cd33c51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de670ef4715b4cbc95fa95c565e1fd22",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6df0afaa41ad4786a5fe6ae84e826b1c",
            "value": "‚Äá23.9k/23.9k‚Äá[00:00&lt;00:00,‚Äá1.73MB/s]"
          }
        },
        "79e35a6316ba4644b9ea6ef0e4aae200": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad628a6218ec4b63bd22c27bd2d682de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7b1267fea1a4c90a4e16346d7fdeee1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b433f4bf3144c41b677167b9f81e7c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b67f1e7d1c3468682f9160d83865332": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "de670ef4715b4cbc95fa95c565e1fd22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6df0afaa41ad4786a5fe6ae84e826b1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "281689f67d11411dbe66f3ca1ee9a411": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eca6455c587a4d16b453ef9892eed6b9",
              "IPY_MODEL_5562114265314e6f9d2c21aa06f20325",
              "IPY_MODEL_e6a2db0fa47943bdae1b0d7af6e48a5a"
            ],
            "layout": "IPY_MODEL_9436957b363c4b7692244f428db294d1"
          }
        },
        "eca6455c587a4d16b453ef9892eed6b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8545be8e225b488e9087ba838b054071",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5cd3872ebdf3471e8bb73c3156c8ceb0",
            "value": "Downloading‚Äáshards:‚Äá100%"
          }
        },
        "5562114265314e6f9d2c21aa06f20325": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8411e3d71dc542959ba65299213f0a41",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e922a75880c477a838273ce05fd8edd",
            "value": 4
          }
        },
        "e6a2db0fa47943bdae1b0d7af6e48a5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e3b86b91caf4a679a0b13bd884390c0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_36fb12c2d1e34fa4893fcdd92c3c9adb",
            "value": "‚Äá4/4‚Äá[21:54&lt;00:00,‚Äá229.84s/it]"
          }
        },
        "9436957b363c4b7692244f428db294d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8545be8e225b488e9087ba838b054071": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cd3872ebdf3471e8bb73c3156c8ceb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8411e3d71dc542959ba65299213f0a41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e922a75880c477a838273ce05fd8edd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e3b86b91caf4a679a0b13bd884390c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36fb12c2d1e34fa4893fcdd92c3c9adb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1471994fea64dda8f4f6475b92bfb2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9408f71c66f24c7dab48ab9a5054e97f",
              "IPY_MODEL_9280806eddb041d1ab823c1821415d54",
              "IPY_MODEL_36a71ef54de44256bee9e555b19eceb2"
            ],
            "layout": "IPY_MODEL_ee6049828d07469aa22ebbe10e5dffce"
          }
        },
        "9408f71c66f24c7dab48ab9a5054e97f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f07365a5c5d247f1966c371c30bf8fcc",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_eb7b908df66a4196ad551a82aab21768",
            "value": "model-00001-of-00004.safetensors:‚Äá100%"
          }
        },
        "9280806eddb041d1ab823c1821415d54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3208efaf8db94c33ab25780d3fae9561",
            "max": 4976698672,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e947d99469c4d08907314983b8f0f5d",
            "value": 4976698672
          }
        },
        "36a71ef54de44256bee9e555b19eceb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6c45755c06d457fb3c661cf7d3560d3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4b0a0aebe7db496d862b60a6cb122811",
            "value": "‚Äá4.98G/4.98G‚Äá[16:23&lt;00:00,‚Äá4.78MB/s]"
          }
        },
        "ee6049828d07469aa22ebbe10e5dffce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f07365a5c5d247f1966c371c30bf8fcc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb7b908df66a4196ad551a82aab21768": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3208efaf8db94c33ab25780d3fae9561": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e947d99469c4d08907314983b8f0f5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d6c45755c06d457fb3c661cf7d3560d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b0a0aebe7db496d862b60a6cb122811": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ba6c83db9f84c359c83830843802512": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5afc52df76d94b3db38c56d63d20edf7",
              "IPY_MODEL_4a566bddd4464b5385963a1c3b41f133",
              "IPY_MODEL_8cee6e7994034fb393eba3ba413b1f2e"
            ],
            "layout": "IPY_MODEL_1201bdd42e4a4200bf46c7ca7407b1cf"
          }
        },
        "5afc52df76d94b3db38c56d63d20edf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dca0d065f53e41a390897e276c695829",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c51c74a39f8a4764b3849afebf8bb37f",
            "value": "model-00002-of-00004.safetensors:‚Äá100%"
          }
        },
        "4a566bddd4464b5385963a1c3b41f133": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82ccc54d925e4ccc9ee464a30fd7dcba",
            "max": 4999802720,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d57fd40796694db6a851bbbed08e12d1",
            "value": 4999802720
          }
        },
        "8cee6e7994034fb393eba3ba413b1f2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fde1bc3aca94244babd8e2441b2bc78",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c5d0dd9daea947bcbbbf7076ee9ffc75",
            "value": "‚Äá5.00G/5.00G‚Äá[00:14&lt;00:00,‚Äá474MB/s]"
          }
        },
        "1201bdd42e4a4200bf46c7ca7407b1cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dca0d065f53e41a390897e276c695829": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c51c74a39f8a4764b3849afebf8bb37f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82ccc54d925e4ccc9ee464a30fd7dcba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d57fd40796694db6a851bbbed08e12d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6fde1bc3aca94244babd8e2441b2bc78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5d0dd9daea947bcbbbf7076ee9ffc75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ee3b7fbd3824cc3b96f90f0dab927fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb6285b059ec41449b2c77a174e8a2a3",
              "IPY_MODEL_7451f1a611764f0f9863cc9bc60d033a",
              "IPY_MODEL_d6e0c415d31545d0be4fb1b37868d088"
            ],
            "layout": "IPY_MODEL_7f816fd2b5ac4f15b430b025a798fdab"
          }
        },
        "cb6285b059ec41449b2c77a174e8a2a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b09905a1f9a94ad3b1f396d31f18c8d4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7cc764f2659741ad931c5e1b3a51f48f",
            "value": "model-00003-of-00004.safetensors:‚Äá100%"
          }
        },
        "7451f1a611764f0f9863cc9bc60d033a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03d559863eaa44bf9af0b251e84202e8",
            "max": 4915916176,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b9069bf679d492fb7bdbb55e6cc6b72",
            "value": 4915916176
          }
        },
        "d6e0c415d31545d0be4fb1b37868d088": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80df318602ba4dfc92e399f6d4111b36",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fbb489a4c599459cb8a9e12cb8e60756",
            "value": "‚Äá4.92G/4.92G‚Äá[04:19&lt;00:00,‚Äá20.7MB/s]"
          }
        },
        "7f816fd2b5ac4f15b430b025a798fdab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b09905a1f9a94ad3b1f396d31f18c8d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cc764f2659741ad931c5e1b3a51f48f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03d559863eaa44bf9af0b251e84202e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b9069bf679d492fb7bdbb55e6cc6b72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "80df318602ba4dfc92e399f6d4111b36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbb489a4c599459cb8a9e12cb8e60756": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "794ab4f8c64d462682d921304c0a2d44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c51d07609c0f45919c67d61ae7470a1d",
              "IPY_MODEL_e2589f1cbefc433695e1360047bd1d81",
              "IPY_MODEL_c033d88262b8419792b28499d9c45900"
            ],
            "layout": "IPY_MODEL_da718d4018554a739007f82f428b09aa"
          }
        },
        "c51d07609c0f45919c67d61ae7470a1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1d6deee2c4545f596429f5d8bc8a3e6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_686cde5ce9644ef796206e9930a4971b",
            "value": "model-00004-of-00004.safetensors:‚Äá100%"
          }
        },
        "e2589f1cbefc433695e1360047bd1d81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_264cee4589c742c98e00088cf741edc6",
            "max": 1168138808,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_76d1f5dee9c24c4d899d299c0ac5d88a",
            "value": 1168138808
          }
        },
        "c033d88262b8419792b28499d9c45900": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f313e2cd13324234bd9523607250c2b7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2943794bf29e4a1eb269a38e5cabfa76",
            "value": "‚Äá1.17G/1.17G‚Äá[00:54&lt;00:00,‚Äá19.0MB/s]"
          }
        },
        "da718d4018554a739007f82f428b09aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1d6deee2c4545f596429f5d8bc8a3e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "686cde5ce9644ef796206e9930a4971b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "264cee4589c742c98e00088cf741edc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76d1f5dee9c24c4d899d299c0ac5d88a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f313e2cd13324234bd9523607250c2b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2943794bf29e4a1eb269a38e5cabfa76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6833ff2efccf4cfab7f9d88a22ca78c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59735e02b0be4bc6a3acaabce337e701",
              "IPY_MODEL_a30a9d3cad934e0d9bc4ec5c507a6dd3",
              "IPY_MODEL_e6090dfb51ef4416ac893d1c17c25c87"
            ],
            "layout": "IPY_MODEL_28686b7f86184038a1fdf7f1b9ea5d8a"
          }
        },
        "59735e02b0be4bc6a3acaabce337e701": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d20fd51b42774cd1ad83ac756ff001c5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_77f33fcff9934991b5e5a77ecb0b8370",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "a30a9d3cad934e0d9bc4ec5c507a6dd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf1a78a726f34d32ad96520c3ad7e9f8",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3602bca21bfb4bdcb06e654bdf6df861",
            "value": 4
          }
        },
        "e6090dfb51ef4416ac893d1c17c25c87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e86dbe0039e94bb88b2f81437084c127",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ad187da78eeb437fabb1334a0bdcd7ec",
            "value": "‚Äá4/4‚Äá[00:06&lt;00:00,‚Äá‚Äá1.39s/it]"
          }
        },
        "28686b7f86184038a1fdf7f1b9ea5d8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d20fd51b42774cd1ad83ac756ff001c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77f33fcff9934991b5e5a77ecb0b8370": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf1a78a726f34d32ad96520c3ad7e9f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3602bca21bfb4bdcb06e654bdf6df861": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e86dbe0039e94bb88b2f81437084c127": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad187da78eeb437fabb1334a0bdcd7ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8613e6d4502e48959e0fec7010c30635": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f971475c6d12498abf0fe8527bca107c",
              "IPY_MODEL_536851cb1f2145c4b4b0c2d114273bb9",
              "IPY_MODEL_28216418edd6434ba6387163ab7506c9"
            ],
            "layout": "IPY_MODEL_6279b894605e41358ed0a3d7614ee146"
          }
        },
        "f971475c6d12498abf0fe8527bca107c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_276a3b6078894315861b4a94d64bafb8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2a05ff4f56524b2db810bf0dceda1654",
            "value": "generation_config.json:‚Äá100%"
          }
        },
        "536851cb1f2145c4b4b0c2d114273bb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33a89b2f28c442b8963e23724eda9dac",
            "max": 186,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d5c50c90a7294d619cc3fa25e626b079",
            "value": 186
          }
        },
        "28216418edd6434ba6387163ab7506c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_445a58de87c545309385b8776bb40d36",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_61f987539055462c933ba21dcc08b677",
            "value": "‚Äá186/186‚Äá[00:00&lt;00:00,‚Äá15.5kB/s]"
          }
        },
        "6279b894605e41358ed0a3d7614ee146": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "276a3b6078894315861b4a94d64bafb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a05ff4f56524b2db810bf0dceda1654": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33a89b2f28c442b8963e23724eda9dac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5c50c90a7294d619cc3fa25e626b079": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "445a58de87c545309385b8776bb40d36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61f987539055462c933ba21dcc08b677": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea4db376965b483183b2b3a3c97ccb6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0be21e876be14ab88a8dd5b789e413e1",
              "IPY_MODEL_c3487547c2ba4d1986ecfa86144989d6",
              "IPY_MODEL_665220344bd047ff83b60aa8e04534d0"
            ],
            "layout": "IPY_MODEL_66423fc52ebf40758f7948d2abef3a49"
          }
        },
        "0be21e876be14ab88a8dd5b789e413e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_537309b50ca341828919a3587d8e6fb0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_49df527ab0464941b0df2129a181d549",
            "value": "README.md:‚Äá100%"
          }
        },
        "c3487547c2ba4d1986ecfa86144989d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b05c1666c74f4e61b0e2aceb8c53fc52",
            "max": 8935,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9546446318e54236ba0034f7386a8a00",
            "value": 8935
          }
        },
        "665220344bd047ff83b60aa8e04534d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be45ac263c594b7c91fa358336165a59",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_08fc66ed78ed47efb38601b26bf61ff6",
            "value": "‚Äá8.94k/8.94k‚Äá[00:00&lt;00:00,‚Äá662kB/s]"
          }
        },
        "66423fc52ebf40758f7948d2abef3a49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "537309b50ca341828919a3587d8e6fb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49df527ab0464941b0df2129a181d549": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b05c1666c74f4e61b0e2aceb8c53fc52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9546446318e54236ba0034f7386a8a00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "be45ac263c594b7c91fa358336165a59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08fc66ed78ed47efb38601b26bf61ff6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b10dbca46f684e419005350d279d4755": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_82cbdb187d0f4428af843a8a1ec0b1b6",
              "IPY_MODEL_540c9770091b43c0a4f942c3cf20a251",
              "IPY_MODEL_41765c0e89de46e5b760fdfeaa6fbfb3"
            ],
            "layout": "IPY_MODEL_68ba4046ac2b4a0da45f1a3dcd337571"
          }
        },
        "82cbdb187d0f4428af843a8a1ec0b1b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77f37f1d2a12400d84729af455b76517",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6e9914283d6d4add8e1b1ce89ed696c0",
            "value": "daredevil-8b.Q4_K_M.gguf:‚Äá100%"
          }
        },
        "540c9770091b43c0a4f942c3cf20a251": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3163d333044479d939a2a0d7c289714",
            "max": 4920733984,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b796398cc71b4a4fb64a3a2ef1f4c057",
            "value": 4920733984
          }
        },
        "41765c0e89de46e5b760fdfeaa6fbfb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17d984410676455fa3cb58a9bdab48a1",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d5e837a9d6d648c683f1c50613258ad4",
            "value": "‚Äá4.92G/4.92G‚Äá[02:24&lt;00:00,‚Äá34.1MB/s]"
          }
        },
        "68ba4046ac2b4a0da45f1a3dcd337571": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77f37f1d2a12400d84729af455b76517": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e9914283d6d4add8e1b1ce89ed696c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3163d333044479d939a2a0d7c289714": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b796398cc71b4a4fb64a3a2ef1f4c057": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "17d984410676455fa3cb58a9bdab48a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5e837a9d6d648c683f1c50613258ad4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21fdba3514104fa6b76cf83f1c205a75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47b4b8ea49ac48b9b50b696f60129260",
              "IPY_MODEL_448a2eaa96f04d43a9dc98d6f5bfc5a6",
              "IPY_MODEL_6a5db9e257694d288d29fbe42697869b"
            ],
            "layout": "IPY_MODEL_cc906292ba7046d9a6ac9098097e4ecd"
          }
        },
        "47b4b8ea49ac48b9b50b696f60129260": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66710857160a408588970a8c8f50a075",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c10e4f3ece5b4424b955f722bc5eceae",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "448a2eaa96f04d43a9dc98d6f5bfc5a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec22984f67c54d31818a7941f2a61d24",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1c8f11269b341f48ad78550698a440e",
            "value": 4
          }
        },
        "6a5db9e257694d288d29fbe42697869b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c51ce038d34443d293c070fecded3a4e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f92c20f0e24c472cab7d93ec50fdf21a",
            "value": "‚Äá4/4‚Äá[01:15&lt;00:00,‚Äá16.14s/it]"
          }
        },
        "cc906292ba7046d9a6ac9098097e4ecd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66710857160a408588970a8c8f50a075": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c10e4f3ece5b4424b955f722bc5eceae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec22984f67c54d31818a7941f2a61d24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1c8f11269b341f48ad78550698a440e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c51ce038d34443d293c070fecded3a4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f92c20f0e24c472cab7d93ec50fdf21a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9434636914074d43a43331c65af84648": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8bc2a40745d43b8a5592658153106df",
              "IPY_MODEL_eb6dff63847246479fc5bf30f4de48c1",
              "IPY_MODEL_bb5575e8670148319ea76862c0cd64aa"
            ],
            "layout": "IPY_MODEL_396460d9c43041ac8774ffff141989f7"
          }
        },
        "e8bc2a40745d43b8a5592658153106df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f46060e7f0a4114a287fe188718492b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4f2c9f648bf142128a4032496a72d1f8",
            "value": "gptq_model-4bit-128g.safetensors:‚Äá100%"
          }
        },
        "eb6dff63847246479fc5bf30f4de48c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4280dbcde904c6caef00bda720bbbdd",
            "max": 5735720552,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f1fb92f5013b4e1fa2cd1f60700312ce",
            "value": 5735720552
          }
        },
        "bb5575e8670148319ea76862c0cd64aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1586fef5dcb45efa902878b326e22dc",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7a10c955985e4ce0b4e8ef08e29858b8",
            "value": "‚Äá5.74G/5.74G‚Äá[02:42&lt;00:00,‚Äá30.7MB/s]"
          }
        },
        "396460d9c43041ac8774ffff141989f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f46060e7f0a4114a287fe188718492b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f2c9f648bf142128a4032496a72d1f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4280dbcde904c6caef00bda720bbbdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1fb92f5013b4e1fa2cd1f60700312ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d1586fef5dcb45efa902878b326e22dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a10c955985e4ce0b4e8ef08e29858b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}